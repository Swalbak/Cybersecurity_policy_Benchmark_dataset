{
    "number": 2,
    "label": 2,
    "source": "NIST",
    "subject": "High-Performance Computing (HPC) Security: Architecture, Threat Analysis, and Security Posture",
    "document(english)": "NIST SP 800-223 initial public draft, High-Performance Computing (HPC) Security: Architecture, Threat Analysis, and Security Posture  NIST Special Publication  NIST SP 800-223 ipd  High-Performance Computing  (HPC) Security:  Architecture, Threat Analysis, and Security Posture  Initial Public Draft  Yang Guo  Ramaswamy Chandramouli  Lowell Wofford  Rickey Gregg  Gary Key  Antwan Clark  Catherine Hinton  Andrew Prout  Albert Reuther  Ryan Adamson  Aron Warren  Purushotham Bangalore  Erik Deumens  Csilla Farkas  This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.800-223.ipd  https://crossmark.crossref.org/dialog/?doi=10.6028/NIST.SP.800-223.ipd  NIST Special Publication  NIST SP 800-223 ipd  High-Performance Computing  (HPC) Security:  Architecture, Threat Analysis, and Security Posture  Initial Public Draft  Yang Guo  Ramaswamy Chandramouli  Computer Security Division  Information Technology  Laboratory  Lowell Wofford  Amazon.com, Inc.  Rickey Gregg  Gary Key  HPCMP  Antwan Clark  Laboratory for Physical Sciences  Catherine Hinton  Los Alamos National Laboratory  Andrew Prout  Albert Reuther  MIT Lincoln Laboratory  Ryan Adamson  Oak Ridge National Laboratory  Aron Warren  Sandia National Laboratories  Purushotham Bangalore  University of Alabama  Erik Deumens  University of Florida  Csilla Farkas  University of South Carolina  This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.800-223.ipd  February 2023  U.S. Department of Commerce  Gina M. Raimondo, Secretary  National Institute of Standards and Technology  Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  Certain commercial entities, equipment, or materials may be identified in this document in order to describe an  experimental procedure or concept adequately. Such identification is not intended to imply recommendation or  endorsement by the National Institute of Standards and Technology (NIST), nor is it intended to imply that the  entities, materials, or equipment are necessarily the best available for the purpose.  There may be references in this publication to other publications currently under development by NIST in  accordance with its assigned statutory responsibilities. The information in this publication, including concepts and  methodologies, may be used by federal agencies even before the completion of such companion publications. Thus,  until each publication is completed, current requirements, guidelines, and procedures, where they exist, remain  operative. For planning and transition purposes, federal agencies may wish to closely follow the development of  these new publications by NIST.  Organizations are encouraged to review all draft publications during public comment periods and provide feedback  to NIST. Many NIST cybersecurity publications, other than the ones noted above, are available at  https://csrc.nist.gov/publications.  Authority  This publication has been developed by NIST in accordance with its statutory responsibilities under the Federal  Information Security Modernization Act (FISMA) of 2014, 44 U.S.C. § 3551 et seq., Public Law (P.L.) 113-283.  NIST is responsible for developing information security standards and guidelines, including minimum requirements  for federal information systems, but such standards and guidelines shall not apply to national security systems  without the express approval of appropriate federal officials exercising policy authority over such systems. This  guideline is consistent with the requirements of the Office of Management and Budget (OMB) Circular A-130.  Nothing in this publication should be taken to contradict the standards and guidelines made mandatory and binding  on federal agencies by the Secretary of Commerce under statutory authority. Nor should these guidelines be  interpreted as altering or superseding the existing authorities of the Secretary of Commerce, Director of the OMB, or  any other federal official.  This publication may be used by nongovernmental organizations on a voluntary basis and  is not subject to copyright in the United States. Attribution would, however, be appreciated by NIST.  NIST Technical Series Policies  Copyright, Use, and Licensing Statements  NIST Technical Series Publication Identifier Syntax  Publication History  Approved by the NIST Editorial Review Board on YYYY-MM-DD [will be add upon final publication]  How to Cite this NIST Technical Series Publication:  Guo Y, Chandramouli R, Wofford L, Gregg R, Key G, Clark A, Hinton C, Prout A, Reuther A, Adamson R, Warren  A, Bangalore P, Deumens E, Farkas C (2023) High-Performance Computing (HPC) Security: Architecture, Threat  Analysis, and Security Posture. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Special  Publication (SP) NIST SP 800-223 ipd. https://doi.org/10.6028/NIST.SP.800-223.ipd  Author ORCID iDs  Yang Guo: 0000-0002-3245-3069  Ramaswamy Chandramouli: 0000-0002-7387-5858  Lowell Wofford: 0000-0003-1003-5090  Albert Reuther: 0000-0002-3168-3663  Aron Warren: 0000-0002-5090-2198  Purushotham Bangalore: 0000-0002-1098-9997  https://csrc.nist.gov/publications https://doi.org/10.6028/NIST-TECHPUBS.CROSSMARK-POLICY https://www.nist.gov/document/publication-identifier-syntax-nist-technical-series-publications  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  Erik Deumens: 0000-0002-7398-3090  Public Comment Period  February 6, 2023 – April 7, 2023  Submit Comments  sp800-223-comments@list.nist.gov  National Institute of Standards and Technology  Attn: Computer Security Division, Information Technology Laboratory  100 Bureau Drive (Mail Stop 8930) Gaithersburg, MD 20899-8930  All comments are subject to release under the Freedom of Information Act (FOIA).  mailto:sp800-223-comments@list.nist.gov  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  i  Abstract 1  Security is an essential component of high-performance computing (HPC). HPC systems often 2  differ based on the evolution of their system designs, the applications they run, and the missions 3  they support. An HPC system may also have its own unique security requirements, follow 4  different security guidance, and require tailored security solutions. Their complexity and 5  uniqueness impede the sharing of security solutions and knowledge. This NIST Special 6  Publication aims to standardize and facilitate the information and knowledge-sharing of HPC 7  security using an HPC system reference model and key components as the basics of an HPC 8  system lexicon. This publication also analyzes HPC threats, considers current HPC security 9  postures and challenges, and makes best-practice recommendations. 10  Keywords 11  high-performance computing (HPC); HPC security; HPC reference model; security guidance; 12  HPC threat analysis; HPC security posture. 13  Reports on Computer Systems Technology 14  The Information Technology Laboratory (ITL) at the National Institute of Standards and 15  Technology (NIST) promotes the U.S. economy and public welfare by providing technical 16  leadership for the Nation’s measurement and standards infrastructure. ITL develops tests, test 17  methods, reference data, proof of concept implementations, and technical analyses to advance 18  the development and productive use of information technology. ITL’s responsibilities include the 19  development of management, administrative, technical, and physical standards and guidelines for 20  the cost-effective security and privacy of other than national security-related information in 21  federal information systems. The Special Publication 800-series reports on ITL’s research, 22  guidelines, and outreach efforts in information system security, and its collaborative activities 23  with industry, government, and academic organizations.  24  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  ii  Call for Patent Claims 25  This public review includes a call for information on essential patent claims (claims whose use 26  would be required for compliance with the guidance or requirements in this Information 27  Technology Laboratory (ITL) draft publication). Such guidance and/or requirements may be 28  directly stated in this ITL Publication or by reference to another publication. This call also 29  includes disclosure, where known, of the existence of pending U.S. or foreign patent applications 30  relating to this ITL draft publication and of any relevant unexpired U.S. or foreign patents. 31  ITL may require from the patent holder, or a party authorized to make assurances on its behalf, 32  in written or electronic form, either: 33  a) assurance in the form of a general disclaimer to the effect that such party does not hold 34  and does not currently intend holding any essential patent claim(s); or 35  b) assurance that a license to such essential patent claim(s) will be made available to 36  applicants desiring to utilize the license for the purpose of complying with the guidance 37  or requirements in this ITL draft publication either: 38  i. under reasonable terms and conditions that are demonstrably free of any unfair 39  discrimination; or 40  ii. without compensation and under reasonable terms and conditions that are 41  demonstrably free of any unfair discrimination. 42  Such assurance shall indicate that the patent holder (or third party authorized to make assurances 43  on its behalf) will include in any documents transferring ownership of patents subject to the 44  assurance, provisions sufficient to ensure that the commitments in the assurance are binding on 45  the transferee, and that the transferee will similarly include appropriate provisions in the event of 46  future transfers with the goal of binding each successor-in-interest. 47  The assurance shall also indicate that it is intended to be binding on successors-in-interest 48  regardless of whether such provisions are included in the relevant transfer documents. 49  Such statements should be addressed to: sp800-223-comments@list.nist.gov  50  mailto:sp800-223-comments@list.nist.gov  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  iii  Table of Contents 51  Introduction ......................................................................................................................... 1 52  HPC System Reference Model and Main Components .................................................... 2 53  Main Components ......................................................................................................... 3 54  2.1.1. Components of the High-Performance Computing Zone ............................................ 3 55  2.1.2. Components of the Data Storage Zone ...................................................................... 4 56  2.1.3. Components of the Access Zone ................................................................................ 5 57  2.1.4. Components of the Management Zone ...................................................................... 6 58  2.1.5. HPC Software ............................................................................................................. 7 59  2.1.6. Container Usage in HPC ............................................................................................. 8 60  HPC Architecture Variants ............................................................................................ 9 61  2.2.1. Diskless Booting HPC ................................................................................................. 9 62  2.2.2. Virtual and Hybrid HPC Environments ........................................................................ 9 63  HPC Threat Analysis ......................................................................................................... 11 64  Key HPC Security Characteristics and Use Requirements ......................................... 11 65  Threats to HPC Function Zones .................................................................................. 11 66  3.2.1. Access Zone Threats ................................................................................................ 11 67  3.2.2. Management Zone Threats ....................................................................................... 12 68  3.2.3. High-Performance Computing Zone Threats ............................................................ 12 69  3.2.4. Data Storage Zone Threats ...................................................................................... 13 70  Other Threats .............................................................................................................. 13 71  HPC Security Posture, Challenges, and Recommendations ........................................ 15 72  HPC Access Control via Multiple Physical Networks .................................................. 15 73  Compute Node Sanitization ......................................................................................... 16 74  Data Integrity Protection .............................................................................................. 16 75  Securing Containers .................................................................................................... 17 76  Achieving Security While Maintaining HPC Performance ........................................... 17 77  Challenges to HPC Security Tools .............................................................................. 17 78  Conclusions ....................................................................................................................... 19 79  References ................................................................................................................................. 20 80  List of Figures 81  Fig. 1. HPC System Reference Model .......................................................................................... 3 82  83  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  1  Introduction 84  In 2015, Executive Order 13702 established the National Strategic Computing Initiative (NSCI) 85  to maximize the benefits of high-performance computing (HPC) for economic competitiveness 86  and scientific discovery. The ability to process large volumes of data and perform complex 87  calculations at high speeds is a key part of the Nation’s vision for maintaining its global 88  competitive edge.  89  Security is an essential to achieving the anticipated benefits of HPC. HPC systems bear some 90  resemblance to ordinary IT computing, which allows for the effective application of traditional 91  IT security solutions. However, they also have significant differences. An HPC is designed to 92  maximize performance so its architecture, hardware components, software stacks, and working 93  environment are very different from ordinary IT. As such, security solutions must be tailored to 94  the HPC system’s requirements. Furthermore, HPC systems are often different from one another 95  due to the evolution of their system designs, the applications they run, and the missions they 96  support. An HPC system frequently has its own unique security requirements and follows 97  different security guidance, which can impede the sharing of security solutions and knowledge.  98  This NIST Special Publication aims to standardize and facilitate the sharing of HPC security 99  information and knowledge through the development of an HPC system reference model and key 100  components, which are introduced as the basics of the HPC system lexicon. The reference model 101  divides an HPC system into four function zones. A zone based on the HPC reference model 102  captures the most common features across the majority of HPC systems and segues into HPC 103  system threat analysis. Key HPC security characteristics and use requirements are laid out 104  alongside the major threats faced by the system and individual function zones. HPC security 105  postures, challenges, and recommendations are also included.  106  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  2  HPC System Reference Model and Main Components 107  The HPC system is complex and evolving so a common lexicon can help describe and identify 108  an HPC system’s architecture, critical elements, security threats, and potential risks. An HPC 109  system is divided into four function zones:  110  1. The high-performance computing zone consists of a pool of compute nodes connected by 111  one or more high-speed networks. The high-performance computing zone provides key 112  services specifically designed to run parallel jobs at scale.  113  2. The data storage zone comprises one or multiple high-speed parallel file systems that 114  provide data storage service for user data. The high-speed parallel file systems are 115  designed to store very large data sets and provide fast access to data for reading and 116  writing. 117  3. The access zone has one or more nodes that are connected to external networks, such as 118  the broader organizational network or the internet. This zone provides the means for 119  authenticating and authorizing the access and connections of users and administrators. 120  The access zone provides various services, including interactive shells, web-based 121  portals, data transfer, data visualization, and others.  122  4. The management zone comprises multiple management nodes and/or cloud service 123  clusters through which HPC management services are provided. The management zone 124  allows HPC system administrators to configure and manage the HPC system, including 125  the configuration of compute nodes, storage, and networks, provisioning, identity 126  management, auditing, system monitoring, and vulnerability assessment. It also offers, 127  through a portal in the access zone, users an interface to acquire high-performance 128  computing services and to configure access to data storage services. Various management 129  software modules – from job scheduler, workflow management, and the Domain Name 130  System (DNS) – run in the management zone. 131  The HPC system reference model is depicted in Figure 1. 132  133  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  3  134  Fig. 1. HPC System Reference Model 135  Main Components 136  2.1.1. Components of the High-Performance Computing Zone 137  An HPC cluster consists of a collection of independent computing systems, called compute 138  nodes, which are interconnected via high-speed networks. Compute nodes have the same 139  components as a laptop or desktop, including central processing unit (CPU) cores, memory, disk 140  space, and networking interface cards. However, they are equipped with a much larger number 141  of CPU cores and more memory than a laptop or desktop. In some HPC architectures, a compute 142  node may not have local disks and uses the data storage services of remote storage servers 143  instead. In addition, there may be different types of nodes for different types of tasks, and some 144  compute nodes are equipped with hardware accelerators to speed up specific applications. For 145  instance, compute nodes often utilize graphics processing units (GPUs) [1] to accelerate 146  modeling and simulation or AI and machine learning (ML) model training.  147  An HPC compute node installs its own software stack (e.g., operating system [OS], compilers, 148  software libraries, etc.) to support applications. The installation and configuration of the software 149  stacks are cluster-wide, centrally managed, and controlled by the management zone. The number 150  of compute nodes in an HPC ranges from a few nodes to hundreds and even thousands of nodes.  151  A critical requirement of HPC networking that interconnects computer nodes is to have massive 152  amounts of scalable bandwidth (throughput) while keeping the latency ultra-low so that the 153  compute nodes and parallel file system (PFS) in the data storage zone can work as one 154  supercomputer. HPC networking often employs specifically designed protocols, networking 155  cards, processor nodes, and switches to optimize network performance. The popular HPC 156  interconnect networking includes InfiniBand [2], Omni-Path [3], Slingshot [4], and others.  157  A high-performance computing zone typically utilizes non-high-performance communication 158  networks, like ethernet, as cluster internal networks that connect the high-performance 159  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  4  computing zone with the management zone and access zone for traffic associated with 160  maintenance activities as opposed to HPC traffic. 161  2.1.2. Components of the Data Storage Zone 162  Several different classes of storage systems may be present inside of the data storage zone. In 163  general, storage systems within this zone cannot be effectively separated from the HPC resources 164  that they support from an administrative privilege perspective. Typical classes of storage found 165  within this zone include parallel file systems (PFS), node-local storage for low-latency 166  workloads, and archival file systems that support campaign storage and protect against data loss. 167  HPC systems may have other file systems that store non-user data. For instance, the management 168  zone often has its own file system that stores the OS images and configuration files. In that case, 169  the file system is included in the corresponding function zone. 170  HPC applications’ initial data, intermediate results, and final results are stored in the data storage 171  zone and can be accessed during the application runtime and after the application’s completion. 172  External HPC users can also access user data through the login nodes and/or data transfer nodes 173  in the access zone. 174  The storage capacity of these file systems is often measured in petabytes and can reach up to 175  exabytes. File systems within the data storage zone will generally use a transport mechanism 176  appropriate to the tier. For example, high-bandwidth file systems may be attached to the HPC 177  resource’s high-performance network, while lower bandwidth file systems may use 10 Gbps or 178  100 Gbps ethernet. Access control for most HPC file systems is enforced by the operating system 179  software of nodes on which these file systems are mounted. As such, file systems should not be 180  mounted outside of their security boundaries. A rogue system that can mount a file system will 181  have complete control of all file system data, can spoof packets on the high-speed network, and 182  can possibly gain privileges elsewhere within other zones of the HPC security enclave. 183  2.1.2.1. Parallel File System 184  Since HPC workloads can vary significantly, a parallel file system (PFS) is often required to 185  support read-intensive and write-intensive applications with sequential and random-access 186  patterns at speeds of up to terabytes per second. Commonly seen file systems include Lustre [5], 187  GPFS [6], and IBM Spectrum Scale [7]. During procurement, a PFS will typically be designed to 188  hit a particular aggregate bandwidth target rather than a capacity requirement. These PFS will 189  typically consist of a cluster of systems to maintain metadata about files and locations as well as 190  servers that act as storage targets. Clients that mount the file system will typically load the file 191  system client software via a kernel module. Storage target servers will have backing storage 192  arrays configured with dozens of disks in a redundant array of inexpensive disk (RAID) strategy. 193  Both GPFS and Lustre-based PFS are prone to performance degradation when a certain capacity 194  threshold is reached. These file systems may be regularly pruned of unwanted files with a 195  strategy decided by the file system administrators. Some deployments will sweep files older than 196  a certain age, which forces HPC users to transfer job output to a longer-term file system, such as 197  campaign storage. PFS tends to be somewhat unreliable depending on the types of activities 198  being performed by running jobs and users. Because these are distributed file systems, file-199  system software must solve distributed locking of files to ensure deterministic file updates when 200  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  5  multiple clients are writing to the same file at once. Additionally, PFS are susceptible to denial-201  of-service conditions even during legitimate user operations, such as listing a directory with 202  millions of files or applications that perform poor file locking semantics. 203  2.1.2.2. Archival and Campaign Storage 204  Archival and campaign storage systems represent a class of storage that is more resilient to 205  failure conditions than PFS and is often less expensive per GigaByte. These advantages come at 206  the cost of bandwidth and an increased latency of data transfer. While PFS acts as a temporary 207  short-term scratch file system, campaign storage supports the longer-term storage needs of a 208  project over its life cycle.  Finished data products that support scientific publications or other 209  high-value datasets may also be stored in an archival file system. The retention time for data on a 210  campaign storage file system is measured in years, while the retention of data within an archival 211  storage system is measured in decades. Both campaign and archival storage systems might 212  employ low-latency disks – such as solid state drives (SSD) or non-volatile memory express 213  (NVMe) drives within a small tier of storage that acts as a cache – and are backed with cheaper, 214  higher capacity media, such as spinning disk and/or tape media. 215  2.1.2.3. Burst Buffer 216  For applications that require extremely low latency or high-bandwidth memory-to-disk data 217  transfer during runtime, intermediate storage layers that contain “burst buffers” have been 218  incorporated as brokers to primarily mitigate the effects of input/output (I/O) contention and the 219  bandwidth burden on parallel file system (PFS). These burst buffers can pre-fetch data from the 220  parallel file system (PFS) before a computing job begins and stage data out to a parallel file 221  system after a computing job has completed. This saves job runtime that would normally be 222  spent performing bulk I/O to the PFS and allows it to be spent on computation instead. Typical 223  HPC infrastructures contain the following intermediate storage architectures: 224  • Node-local burst buffer architectures: Each burst buffer is collocated with a 225  corresponding HPC compute node [8]. This is advantageous for its scalability and also 226  improves the checkpoint bandwidth because the aggregate bandwidth increases in 227  proportion with the number of compute nodes.  228  • Remote-shared burst buffer architectures: Burst buffers are shared between multiple 229  HPC compute nodes that are hosted on an I/O node [8]. This is advantageous for 230  facilitating the development, deployment, and maintenance of these architectures.  231  There are also HPCs that can contain mixed burst buffer intermediate storage architectures, 232  which are a mixture of the strengths of node-local and remote-shared burst buffer architectures.  233  2.1.3. Components of the Access Zone 234  A typical HPC system provides one or more nodes through which users and administrators 235  access the system. At least one of these nodes is a login node where users have access to shells to 236  launch interactive or batch jobs. Some of these login nodes may also have specialized 237  visualization hardware and software with which users can conduct interactive and/or post-238  execution visualization of their datasets. There may also be one or more data transfer nodes that 239  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  6  provide services to transfer data into and out of the HPC system and may even provide storage-240  mounting services like Network File System (NFS) [9], Common Internet File System (CIFS) 241  [10], Server Message Block (SMB) [11], and Filesystem in Userspace (FUSE) [12] based SSH 242  Filesystem (SSHFS) [13]. Many HPC systems now provide web portals via web portal nodes 243  that enable a variety of web interfaces to HPC system services.  244  2.1.4. Components of the Management Zone 245  The complexity of HPC systems requires a significant infrastructure to operate and manage it, 246  which is collectively referred to as the management zone. The management zone may consist of 247  servers and network switches that enable various functions for operating the system with 248  efficiency, effectiveness, and stability. 249  2.1.4.1. General Architecture and Characteristics 250  One important characteristic of the management zone is that it has a separate security posture 251  because non-privileged users do not need to access the management servers or services in a 252  direct way. Privileged users responsible for configuring, maintaining, and operating the HPC 253  system access the management zone servers and switches through extra security controls. For 254  example, from the public-facing login nodes, they may go through a bastion host that is typically 255  located in the management zone, or they may establish a private virtual network (VPN) with 256  separate authentication and authorization or other appropriate security controls to reach the 257  management zone. All systems are configured on networks that are not routed beyond the 258  perimeter of the HPC system so that only nodes like compute nodes and storage nodes can access 259  the services. 260  The services provided by the management zone have clearly defined protocols and can be 261  implemented as running on assigned hardware platforms or run as virtual machines on a 262  dedicated set of hardware resources. The fact that the management zone has a clear and separate 263  security posture helps with risk assessment and the selection of controls to secure the 264  management zone and manage the risk. 265  2.1.4.2. Basic Services 266  The HPC resources inside of the computing and data storage zones need various services to 267  operate. Examples include Domain Name Services (DNS) [14]; the Dynamic Host Configuration 268  Protocol (DHCP) [15]; configuration definitions, authentication, and authorization services, such 269  as those provided by an LDAP server [16]; and the Network Time Protocol (NTP) [17] for 270  synchronization, log management, version-controlled repositories. 271  The management zone includes storage systems to store configuration data and node images, 272  current versions, development and test versions, and historical versions. Storing logs from the 273  entire HPC system is also part of the management zone as well as the servers to process the logs 274  and alert administrators of events, problems, and incidents. Many of these services will be 275  implemented with high availability and failover capabilities to avoid failure of the HPC 276  resources. The network switches for the management network (ethernet) and the fast 277  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  7  interconnects (e.g., InfiniBand, Omni-Path, Slingshot) are often managed as part of the 278  management zone because non-privileged users do not need direct access to these resources. 279  2.1.4.3. Configuration Management 280  Automated configuration management is crucial to ensure the stable operation of complex 281  systems like HPC. The systems that hold the configuration database and run the server to place 282  configurations on compute nodes, storage servers, and network switches are part of the 283  management zone. The nodes are subject to a regularly scheduled process to verify configuration 284  and enforce consistency with what the configuration management nodes and databases specify. 285  Often, the configuration management systems in the management zone have an even more 286  restricted security posture than the management zone as a whole, with a smaller number of 287  privileged users having access.  288  2.1.4.4. HPC Scheduler and Workflow Management 289  Because of the distributed nature of HPC systems, requesting resources for given workloads is 290  coordinated by a scheduler or workload manager, such as Slurm [18] and Kubernetes [19]. These 291  services are run on servers in the management zone alongside the configurations and job logs. 292  Non-privileged users access the scheduler through specific commands or an application 293  programming interface (API). Access to the service is restricted to nodes within the HPC system 294  perimeter. There may also be a web interface that provides a separate authenticated and 295  authorized path for scheduling workloads, often within the strict constraints of certain 296  application domains. 297  2.1.5. HPC Software 298  In addition to the management software that installs, boots, configures, and manages HPC-299  related systems, HPC application codes rely on several layers of scientific and performance-300  enhancing libraries. The layers of software that are available to users is referred to as the 301  software stack. The lower layers of this stack are typically focused on performance and include 302  compilers, communication libraries, and user-space interfaces to HPC hardware components. 303  The middle layer includes performance tools, math libraries, and data or computation abstraction 304  layers. The top of the stack consists of end-user science or production applications. Each 305  software product within this stack may require certain versions or variants of other products and 306  have many dependencies. For example, Hierarchical Data Format version 5 (HDF5) [20] is a 307  scientific data formatting library with only seven dependencies, while Data Mining Classification 308  and Regression Methods (rminer) [21] – an R-based data mining application – has 150 software 309  dependencies. The full software stack can be split into three general categories that differ based 310  on the maintainer: user software, facility software, and vendor software.  311  2.1.5.1. User Software 312  Often, the end users themselves best understand how to tune their software to the bespoke 313  hardware of an HPC system to ensure sufficient performance for their workload. Users regularly 314  modify and recompile their software to enhance performance, fix bugs, and adapt to changes in 315  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  8  the underlying dependencies or kernel interfaces over time. The sharing of user-built software 316  between teams may be common. User software that is widely used is often open source and, 317  therefore, subject to open-source software supply chain concerns. 318  Continuous integration (CI) pipelines [22] and tests of scientific code on HPC platforms have 319  recently become commonplace. Industry-standard identification of software weaknesses and the 320  publication of Common Vulnerabilities and Exposures (CVEs) [23] is not routine, but the 321  identification and remediation of performance regressions is generally a higher priority within 322  the user community. There is a value-per-cycle trade-off for CI tests since cheaper cycles on 323  commodity hardware may not expose bugs on much more expensive HPC resources. Complex 324  test suites will eat into user allocations, and users and staff prefer that only a cardinal set of 325  smoke tests run within user-developed testing pipelines on HPC systems. 326  2.1.5.2. Site-Provided Software and Vendor Software 327  Site staff and administrators generally build applications and libraries that are most likely to be 328  used. Tools such as Conda [24], EasyBuild [25], and Spack [26] are often used to manage the 329  complexity of software dependency resolution. Staff may also wrap compiler and job submission 330  utilities with custom scripts to collect usage information about software libraries, I/O read and 331  write patterns, or other system telemetry that is useful for decision making. 332  Vendor software includes low-level system tools to facilitate the running of other software. For 333  instance, remote direct memory access, inter-node memory sharing, performance counters, 334  temperature and power telemetry, and debugging are all vendor-provided software.  335  Users can choose specific versions of installed vendor and site-provided software libraries by 336  manipulating environment variables. Tools such as wrapper scripts or module files are usually 337  provided to help users find and choose which versions of installed software to use. 338  2.1.6. Container Usage in HPC 339  A container is a software package that contains an application’s entire runtime environment. It 340  consists of an application program and all of the dependencies, libraries, other binaries, and 341  configuration files needed to run it. Containers provide self-contained, portable, and reproducible 342  environments that abstract away the differences in OS distributions and underlying 343  infrastructure. Containers make applications more portable and the deployment easier. For 344  instance, containers allow users to use a package manager (e.g., apt [27] or yum [28]) to install 345  software without changing anything on the host system or to run the latest software built for 346  newer Linux OS versions. 347  Security is a major concern in deploying containers in HPC environments. Containers possess 348  large attack surfaces due to the different underlying images, each of which can have 349  vulnerabilities. In addition, securing the host is not enough to ensure protection. Container 350  permissions and proper isolation are also necessary. Finally, monitoring containers can be 351  difficult due to its dynamic nature.  352  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  9  HPC Architecture Variants 353  2.2.1. Diskless Booting HPC 354  A variant of HPC cluster design is diskless booting clusters in which the OS is not loaded from 355  storage located on the node itself. Diskless nodes typically boot up, obtain an IP address via 356  DHCP, and receive a kernel and partial OS over the network through a protocol, such as the 357  Preboot eXecution Environment (PXE) [29] . To finish the boot process, the remaining OS is 358  typically loaded over NFS or Internet Small Computer System Interface (ISCSI) [30] .  359  For scalability, some HPC clusters may implement a hierarchy design in which intermediate 360  servers boot diskless but have local storage that is used to serve the OS to leaf nodes.  361  2.2.2. Virtual and Hybrid HPC Environments 362  Virtualization technologies can be used to recreate and maintain traditional HPC architectures 363  that allow them to leverage the virtual infrastructure effectively while also providing additional 364  scalability. For example, virtual machines (VMs) can be used to redistribute scientific 365  applications across heterogeneous supercomputing architectures that can be bundled more easily 366  without the need for certain special privileges. Virtual HPC (vHPC) architectures typically 367  contain the following main components [31]:  368  • Hypervisors: These elements create and run VMs in vHPC architectures, which allows 369  them to use multiple resources, such as memory and processing. The benefits to these 370  elements include their speed, efficiency, flexibility, convenience, and portability.  371  • vCenter Server Appliance (VCSA): This element provides centralized management of 372  all virtualized infrastructure with a single management interface that interacts, manages, 373  and monitors the virtualization configuration, settings, and services.  374  • Network virtualization and security platform: This platform provides software-375  defined networking (SDN) management and protection capabilities to the vHPC 376  infrastructure.  377  Some optional technologies include Kubernetes-based platforms and technologies that accelerate 378  the deployment and management of applications and services, optimize systems and application 379  environments, and securely connect and operate remotely. vHPC architectures also contain 380  access, management, compute, and data storage zones, as highlighted in Figure 1, in which the 381  management zones contain VMs that manage the vHPC environment. The compute zones 382  contain virtual compute clusters that are responsible for HPC workloads, while the storage zones 383  consist of virtualized PFS capabilities that store user data. Virtual compute clusters can contain 384  low-latency burst buffer storage that is local to each node or shared across nodes. 385  Cloud technologies can also be employed to enhance traditional HPC infrastructures, which 386  result in the creation of hybrid HPC environments that contain a combination of traditional and 387  cloud HPC architectures [32]. These types of schemas typically use private cloud capabilities 388  because they are more flexible, customizable, and manageable than using public cloud 389  technologies. Recently, high-performance computing as a service (HPCaaS) has been adopted as 390  a flexible solution that offers similar supercomputing capabilities on the cloud. Organizations 391  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  10  can work with vendors or service providers to customize these architectures to their specific 392  needs and maintain control by changing their capacities and architectures. HPCaaS is also cost-393  effective because these solutions can typically be purchased via pay-as-you-go subscriptions that 394  are managed by their service providers.  395  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  11  HPC Threat Analysis 396  HPC poses unique security and privacy challenges, and collaboration and resource-sharing are 397  integral. HPC workloads are often different from their traditional counterparts. For instance, 398  scientific experiments frequently employ unique hardware, software, and configurations that may 399  not be maintained or well-vetted or that present entirely new classes of vulnerabilities absent in 400  more traditional environments. HPC can store large amounts of sensitive research data, 401  personally identifiable information (PII), and intellectual property (IP) that need to be 402  safeguarded. Finally, HPC data and computation are encumbered with a variety of different 403  security and policy constraints. The solutions to protecting data, computation, and workflows 404  must balance these trade-offs. 405  Key HPC Security Characteristics and Use Requirements  406  HPC systems possess some unique security characteristics and distinctive use requirements that 407  differentiate themselves from average IT system: 408  409  • Tussles between performance and security: HPC users may consider security valuable 410  only to the extent that it does not significantly slow down the HPC system and impede 411  research. Ensuring the usability of security mechanisms with a tolerable performance 412  penalty is therefore critical to adoption by the scientific HPC community. 413  • Varying security requirements for different HPC applications: Individual platforms, 414  projects, and data may have significantly different security sensitivities and need to 415  follow different security policies. An HPC may need to enforce multiple security policies 416  simultaneously. 417  • Limited resources for security tools: Most HPC systems are designed to devote their 418  resources to maximizing performance rather than acquiring and operating security tools. 419  • Open-source software and self-developed research software: Open-source software 420  and self-developed research software are widely used in HPC. Open-source software is 421  vulnerable to open-source software supply chain threats, while HPC software input data 422  may also be vulnerable to data supply chain threats. Self-developed software is 423  susceptible to low software quality. 424  • Granular access control on databases: Since different research groups may have a need 425  to know for different portions of data, granular access control capabilities are necessary. 426  Access control requirements may need to be dynamically adjusted as some scientific 427  experiments may increase data needs based on the outcome of experiments. 428  Threats to HPC Function Zones 429  3.2.1. Access Zone Threats  430  The access zone provides an interface for external users to access the HPC system and oversees 431  the authentication and authorization of users. Among the four function zones, the access zone is 432  the only one that is directly connected to the external networks. Hence, the nodes and their 433  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  12  software stacks in this zone are susceptible to external attacks, such as denial of service (DoS) 434  attacks, perimeter network scanning and sniffing, authentication attacks (e.g., brute force login 435  attempts and password guessing), user session hijacking, and machine-in-the-middle attacks. In 436  addition, some nodes are subject to extra attacks due to their specific software stacks. For 437  instance, a web server may be subject to website defacement, phishing, misconfiguration, and 438  code injection attacks. The access zone also provides access to the file systems hosted in the data 439  storage zone. It is important that permissions to directories and files are only given to authorized 440  accesses. 441  Authenticated users sometimes use external networks to download data or code for use inside of 442  the HPC system, which introduces the risk of unintentionally downloading malicious content. 443  The nodes in the access zone are usually configured to support limited computation (e.g., modest 444  debugging). Access zone nodes are susceptible to computational resource abuse. 445  The access zone is also shared by multiple users. One user’s activities, such as commands issued 446  and jobs submitted, can be viewed by other users. A port opened by one user can potentially be 447  used by others. Fortunately, the nodes in the access zone work similarly to enterprise servers, and 448  general IT security tools and measures are available to harden the zone. 449  3.2.2. Management Zone Threats 450  The management zone is responsible for managing the entire HPC system. It is connected to 451  clustered internal networks through which other zones can be reached. It runs a plethora of 452  system management, out-of-band hardware management, job scheduling, and workflow 453  management software, all of which are susceptible to unique threats. 454  Processes running in the management zone, such as schedulers and data tiering/orchestration 455  processes, act on behalf of users. These are privileged processes, and if they are spoofed, it can 456  lead to privilege escalation, which is a distributed system-to-system trust problem. Due to the 457  implied delegation of authority within distributed HPC and file systems, ‘root’ on a compute 458  node may be, depending on configuration, equivalent to ‘root’ on all systems within the HPC 459  zones. Only administrators with privileged access authorization are allowed to log into the 460  management zone, where a privileged administrator logs into the access zone first and then logs 461  into the management zone. A malicious user may attempt to log into the management zone. 462  The management zone may also be implemented as a service running on a cloud via 463  virtualization technologies. In such cases, the risks associated with the cloud also apply to the 464  management zone. 465  3.2.3. High-Performance Computing Zone Threats  466  The high-performance computing zone offers core computational functions in an HPC system. 467  The computer nodes are shared by multiple users or tenants. The exploitation of multi-tenancy 468  environments is a major threat (e.g., side-channel attacks, user data/program leakage, etc.). Other 469  threat sources that often cause extreme resource consumption, performance degradation, or the 470  outage of the HPC system entirely include accidental misconfiguration, software bugs introduced 471  by user-developed software, and system abuse by running applications that are not aligned with 472  the HPC mission. Container escape, side-channel attacks, and DoS can also be threats if 473  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  13  virtualization technologies – such as containers – are used in the high-performance computing 474  zone.  475  As a security mitigating technology, the applications in HPC are mostly run in the user space, 476  except for system calls that must run in the kernel with elevated privilege. Accelerators, high-477  performance interconnects, special protocols, and direct memory access between nodes are 478  commonly used in the high-performance computing zone. Some of these technologies may not 479  be thoroughly tested, and their speed, novelty, and complexity can make monitoring and 480  detecting suspicious activity difficult. Direct memory access and communication between nodes 481  may bypass the kernel and the protections provided by the kernel (e.g., Security-Enhanced Linux 482  [SELinux] [33]) is lost. 483  3.2.4. Data Storage Zone Threats 484  Protecting the confidentiality and integrity of user data is essential for the data storage zone. Data 485  integrity can be compromised by malicious data deletion, corruption, pollution, or false data 486  injection so gaining unauthorized privileged access is a major threat. Legitimate users may also 487  mishandle sensitive data, leading to confidentiality breakdown. File metadata (e.g., file name, 488  author, size, creation date) can also leak sensitive information about the files. 489  HPC file systems in the data storage zone provide superior data access speed and much larger 490  storage size compared to average enterprise file systems. Hard disk failure is a threat due to the 491  large number of disks deployed in the data storage zone. Incident response and contingency 492  planning controls may not be easy to implement, and file backup, recovery, and forensic imaging 493  may become infeasible. The security measures that can be implemented on the enterprise file 494  systems may take an unacceptably long time and degrade HPC file system performance in an 495  unacceptable way. 496  Providing data backup services is another challenge in HPC due to its large volume. By default, 497  user data is often not backed up, and users are responsible for maintaining their own data copies. 498  Inadvertent operations (e.g., accidently deleting a file subdirectory) can cause the permanent loss 499  of data, though making data READ ONLY is one way to combat such a risk. Some organizations 500  offer backup services using their own HPC systems, but these systems may be in the same 501  geographic locations and subject to the same environmental threats.  502  Other Threats 503  In addition to the threats unique to individual function zones, the general HPC systems face the 504  following threats: 505  • Environmental and physical threats: Physical or cyber attacks against facilities (e.g., 506  power, cooling, water), unauthorized physical access, and natural disaster (e.g., fire, 507  flood, earthquake, hurricane, etc.) are all potential threats to an HPC system. 508  • Vulnerabilities introduced by prioritizing performance in HPC design and 509  operation: HPC is designed to process large volumes of data and perform complex 510  computations at very high speeds. Achieving the highest performance possible is a 511  priority in HPC design and operation. Such prioritization, however, has its security 512  implications. For instance, designers often make conscious decisions to build a less 513  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  14  redundant system to achieve high performance, making the system less robust and 514  potentially more vulnerable to attacks, such as DoS attacks. As another example, using a 515  backup system to improve system robustness and high availability is a proven 516  technology. Building a backup HPC system, however, is often infeasible since it is too 517  costly. HPC often lacks storage backup due to the vast size of the data stored. Similarly, 518  most HPC missions do not have a service-level agreement to justify the need for a full 519  backup system at a backup location. All resources are poured into building the single best 520  HPC system possible.  521  • Supply chain threats: The HPC supply chain faces a variety of threats, from the theft of 522  proprietary information to attacks on critical hardware components and software 523  manipulation to gain unauthorized access. Some HPC software (e.g., OS, BIOS, 524  applications), firmware, and hardware components have limited manufacturers, suppliers, 525  and integrators, which make diversification difficult. Limited suppliers also lead to 526  shortages in the qualified workforce who can perform required technical support.  527  • Insider threats: Insider threats come from people within the organization who have 528  internal information and may have the privileges needed to access the HPC system. 529  Insider threats can be classified into accidental/unintentional threats and 530  malicious/intentional threats. Unintentional threats come from the unintended side effects 531  of normal actions and activity. In contrast, a malicious insider may intentionally upload 532  malicious code into the HPC system.  533  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  15  HPC Security Posture, Challenges, and Recommendations 534  HPC Access Control via Multiple Physical Networks 535  Access control is a security technique that regulates who can access and/or use resources in a 536  computing environment. In HPC, multiple physical networks are constructed as an effective 537  means of access control: 538  • Management network: The management network is a dedicated network that allows 539  system administrators to remotely control, monitor, and configure computer nodes in an 540  HPC system. Modern computers are often equipped with the Intelligent Platform 541  Management Interface (IPMI) [34], which provides management and monitoring 542  capabilities that are independent of the host system’s CPU, firmware (e.g., BIOS [35]or 543  Unified Extensible Firmware Interface [UEFI] [36]), and operating system. For example, 544  IPMI allows system administrators to remotely power on/off unresponsive machines and 545  install custom operating systems. IPMIs are connected to the management network, 546  which can only be accessed by authorized system administrators. 547  • High-performance networks: High-performance networks offer high bandwidth and 548  low latency to connect computer nodes inside of the high-performance zone and data 549  storage zone. They also support features that are unique to HPC, such as remote memory 550  access over the network and the message passing interface (MPI) [37]. High-performance 551  networks often use special communications standards and architectures to achieve the 552  high performance (e.g., InfiniBand, Slingshot, Omni-Path, etc.). 553  • Auxiliary networks: Additional auxiliary networks can be added to support usability and 554  system manageability. For instance, a user network is constructed to allow users to 555  manipulate or share data or remotely log into and access the compute nodes. Depending 556  on the purpose of the networks, a subset of nodes from different zones are selected to be 557  party to the networks. As an example, a user network contains the nodes in the access 558  zone and the computer nodes in the high-performance computing zone. 559  There are many benefits to having multiple networks in an HPC system. First, all of these 560  networks are private and use different IP address ranges. Network traffic will remain on one 561  network, which facilitates monitoring and measurement. Second, individual networks often serve 562  specific purposes so only the relevant nodes are connected to the network. The networks 563  effectively segment the HPC system into smaller segments, which improves security. Finally, 564  multiple physical networks also provide a degree of fault tolerance. When one network goes 565  down, the system administrator can use the other network to diagnose and troubleshoot. 566  The compute nodes in the access zone are connected to the external network and assigned public 567  IP addresses, which allow users to remotely access the HPC systems. The user data can be shared 568  through the login nodes or the data transfer nodes. Some systems allow storage to be exported 569  using CIFS [10] or SMB [11] (e.g. via a SAMBA [38] server). If necessary, a network address 570  translation (NAT) [39] or a Squid proxy [40] can be installed to allow users on a private network 571  to access the internet and download new versions of software or share software data. However, a 572  NAT and Squid proxy can also be security weaknesses that demand extra caution and mitigation 573  considerations. 574  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  16  Employing multiple physical networks is a common and effective means for access control and 575  fault tolerance, and it is highly recommended. 576  Compute Node Sanitization 577  High-performance compute nodes are used by multiple tenants and projects. At the end of a task 578  run, the previous project may leave behind a residual “footprint,” such as the data in memory and 579  GPUs. It is important to sanitize the compute node so that data from previous jobs are not 580  accidently leaked and the new job can start with a clean slate. 581  The common practice of compute node sanitization includes:  582  • Conducting a node health check at the end of a job 583  • Removing a node or forcing a reboot if a node is deemed “unhealthy” 584  • Working with hardware and software vendors to provide management hooks to sanitize 585  the GPU 586  • Resetting GPUs to remove residual data between jobs 587  • Validating and checking firmware 588  • Rebooting nodes after the completion of a job at the OS level to remove accumulated 589  residuals and ensure a consistent node state 590  • Checking critical files to ensure that they have not been changed 591  Compute node sanitization is highly recommended as the compute nodes are equipped with 592  sophisticated hardware accelerators.  593  Data Integrity Protection  594  Data encryption is an effective means of providing data integrity. HPC data storage systems 595  typically support uniform encryption at the file level or block level. Such data encryption at the 596  file system level protects data from unauthorized access. However, it does not provide granular 597  access (i.e., segmenting one user from others). Granular access can be achieved using user-level 598  or group-level encryption. Even a system administrator cannot access a user’s data with granular 599  access. Additionally, file systems do not authorize users. Rather, users access the file system via 600  the HPC access zone, which is responsible for authenticating the users and their access rights.  601  Hashing is another technique for protecting data integrity. Data files can be hashed at the 602  beginning to acquire hashing keys. A file is not modified if its hashing key remains the same. 603  Parallel file systems maintain many types of metadata (e.g., user ID, group ID, modification 604  time, sh54 of file, etc.). Hashing metadata is also another way to check whether a file has been 605  modified. 606  Periodically scanning file systems for malware is a proven technique for ensuring data integrity. 607  However, scanning an HPC system for malware is challenging. HPC data storage can easily 608  contain a petabyte or more of data. Existing malware scanning tools are only designed for a 609  single machine or laptop using one thread. They are not efficient or fast enough to scan large 610  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  17  HPC data storage systems. Furthermore, the scanning operation can adversely affect the 611  performance of running jobs. 612  Protecting data integrity is vital to the HPC security. Granular data access provides the best 613  protection and is highly recommended when possible.  614  Securing Containers 615  Containers bundle an application’s code, related libraries, configuration files, and required 616  dependencies to allow the applications to run seamlessly across environments. Containers 617  provide the benefits of portability, reproducibility, and productivity, but they can hide software. 618  In a well-managed HPC system, a lot of software has already been installed as a baseline system 619  environment. The applications developed using native libraries often run faster than a container. 620  Hence, training users to develop programs in the HPC programming environment is one way to 621  reduce exposure to container vulnerabilities. 622  Using containers in an HPC is a balancing act. When containers are supported, restrictions are 623  recommended to mitigate vulnerabilities. This may include prohibiting independent network 624  stacks or user namespaces (i.e., container namespace is always mapped to the host user account). 625  These measures can ensure that the risks posed by containers are no different from other 626  workloads. The threats of a container only affect the applications run by the same user. Other 627  security measures include selecting a container runtime that does not require root access to the 628  system and prohibiting container development in the HPC. 629  Achieving Security While Maintaining HPC Performance  630  HPC security measures often come with an undesirable performance penalty. The following are 631  several effective ways to balance performance and security:  632  • Conduct tests to measure the performance penalty of security tools, which can be 633  benchmarked to determine whether they are acceptable. Testing and measurement would 634  also encourage more performance-aware tool design. 635  • Incorporate security requirements in the initial HPC design rather than as an afterthought. 636  For instance, independent add-on security tools tend to have more impact on performance 637  than native security measures that come with the HPC software stack. 638  • Avoid “one size fits all” security. Differentiate the types of nodes in the HPC system and 639  apply appropriate security rules and controls to different node types. For instance, 640  classify the nodes in HPC into three categories: external nodes, internal nodes, and 641  backend nodes. Apply individual security controls to each node category. Such a 642  differentiation also mitigates performance impacts. 643  Challenges to HPC Security Tools 644  Many industrial security tools are designed with stand-alone devices in mind (e.g., laptops, 645  desktops, or mobile devices). HPC is a large-scale, complex system with strict performance 646  requirements. Security tools that are effective for individual devices may not work well in an 647  HPC environment. For example, a forensic tool that aids the recovery and preservation of a hard 648  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  18  drive and memory for a single server works well in practice. It is unreasonable, however, to 649  install forensic tools on all compute and storage nodes in an HPC system. As another example, 650  HPC nodes may use remotely mounted storage, which may disable some security tools.  651  Moreover, different HPC applications may require different tools. Security tool vendors are often 652  not accustomed to HPC use cases and requirements, which forces HPC security teams to develop 653  analogous tools that may introduce new security vulnerabilities and are sometimes not accepted 654  by organizations. The HPC community needs to work closely with security tool vendors to 655  address these challenges. 656  A Security Technical Implementation Guide (STIG) [41] is a configuration standard and offers a 657  security baseline that reflects security guidance requirements. The security checking tool can 658  measure how well the STIG is satisfied. However, available STIGs are typically written for 659  servers or desktops rather than for HPC. In addition, security baseline checking tools developed 660  for commodity operating systems and applications require customizations to run on HPC. The 661  Lawrence Livermore National Laboratory [42], Sandia National Laboratories [43], and the Los 662  Alamos National Lab [44] have collaborated with the Defense Information Systems Agency 663  (DISA) [45] to develop TOSS 4 STIG [46], which is geared toward HPC systems. Still, a more 664  general STIG library and corresponding security checking tools are desirable to handle diverse 665  subsystems and components inside an HPC.  666  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  19  Conclusions 667  Securing HPC systems is challenging due to their size; performance requirements; diverse and 668  complex hardware, software, and applications; varying security requirements; and the nature of 669  shared resources. The security tools suitable for HPC are inadequate, and current standards and 670  guidelines on HPC security best practices are lacking. The continuous evolution of HPC systems 671  makes the task of securing them even more difficult.  672  This Special Publication aims to standardize and facilitate the information and knowledge-673  sharing of HPC security. A zone based HPC system reference model is introduced to serve as a 674  foundation for a system lexicon and captures common features across the majority of HPC 675  systems. HPC system threat analysis is discussed, security postures and challenges are 676  considered, and recommendations are made.  677  678  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  20  References 679  680  [1]  \"What Is a GPU?,\" [Online]. Available:  https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html.  [2]  R. Sheldon, \"InfiniBand,\" [Online]. Available:  https://www.techtarget.com/searchstorage/definition/InfiniBand.  [3]  \"Omni-Path,\" [Online]. Available: https://en.wikipedia.org/wiki/Omni-  Path#:~:text=Omni%2DPath%20Architecture%20(OPA),this%20architecture%20for%20e xascale%20computing..  [4]  \"HPE Slingshot interconnect,\" [Online]. Available:  https://www.hpe.com/us/en/compute/hpc/slingshot-interconnect.html.  [5]  \"Lustre file system,\" [Online]. Available: https://www.lustre.org/.  [6]  \"Introducing General Parallel File System,\" [Online]. Available:  https://www.ibm.com/docs/en/gpfs/4.1.0.4?topic=guide-introducing-general-parallel-file- system.  [7]  \"IBM Spectrum Scale,\" [Online]. Available: https://www.ibm.com/products/spectrum- scale.  [8]  B. W. S. a. J. B. Lei Cao, \"To share or not to share: comparing burst buffer architectures,\"  in Proceedings of the 25th High Performance Computing Symposium , Virginia Beach,  VA, USA, April 23 - 26, 2017.  [9]  \"Network File System,\" [Online]. Available:  https://en.wikipedia.org/wiki/Network_File_System.  [10]  \"CIFS,\" [Online]. Available: https://cifs.com/.  [11]  \"Server Message Block Overview,\" [Online]. Available: https://learn.microsoft.com/en-  us/previous-versions/windows/it-pro/windows-server-2012-r2-and- 2012/hh831795(v=ws.11).  [12]  \"FUSE,\" [Online]. Available:  https://www.kernel.org/doc/html/latest/filesystems/fuse.html.  [13]  \"SSHFS Homepage,\" [Online]. Available: https://github.com/libfuse/sshfs.  [14]  \"Domain Name Services,\" [Online]. Available: https://www.iana.org/domains.  [15]  \"DHCP,\" [Online]. Available:  https://www.techtarget.com/searchnetworking/definition/DHCP.  [16]  \"LDAP (Lightweight Directory Access Protocol),\" [Online]. Available:  https://www.techtarget.com/searchmobilecomputing/definition/LDAP.  [17]  \"What is NTP?,\" [Online]. Available: http://www.ntp.org/ntpfaq/NTP-s-def.htm.  [18]  \"SLURM Workload Manager,\" [Online]. Available:  https://slurm.schedmd.com/documentation.html.  [19]  \"Kubernetes,\" [Online]. Available: https://kubernetes.io/.  [20]  \"HDF5,\" [Online]. Available: https://www.hdfgroup.org/solutions/hdf5/.  [21]  \"rminer: Data Mining Classification and Regression Methods,\" [Online]. Available:  https://cran.r-project.org/web/packages/rminer/index.html.  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  21  [22]  \"What is a CI/CD pipeline?,\" [Online]. Available:  https://www.redhat.com/en/topics/devops/what-cicd- pipeline#:~:text=A%20continuous%20integration%20and%20continuous,development%2 0life%20cycle%20via%20automation..  [23]  \"CVE,\" [Online]. Available: https://cve.mitre.org/.  [24]  \"Conda,\" [Online]. Available: https://docs.conda.io/en/latest/.  [25]  \"EasyBuild: building software with ease,\" [Online]. Available: https://easybuild.io/.  [26]  \"Spack,\" [Online]. Available: https://spack.io/.  [27]  \"Package Management,\" [Online]. Available: https://ubuntu.com/server/docs/package-  management#:~:text=Apt,upgrading%20the%20entire%20Ubuntu%20system..  [28]  \"Yum,\" [Online]. Available: https://en.wikipedia.org/wiki/Yum_(software).  [29]  \"PXE,\" [Online]. Available:  https://www.techtarget.com/searchnetworking/definition/Preboot-Execution-Environment.  [30]  \"iSCSI (Internet Small Computer System Interface),\" [Online]. Available:  https://www.techtarget.com/searchstorage/definition/iSCSI.  [31]  \"Virtualizing High Performance Computing (HPC) Environments: Dell EMC Reference  Architecture,\" May 2020. [Online]. Available:  https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/solutions/VMwar e-HPC-Virtualized-DTC.pdf.  [32]  HPCWire, \"Hybrid HPC: The time to embrace the cloud is now,\" 2020. [Online].  Available: https://www.hpcwire.com/2020/08/31/hybrid-hpc-the-time-to-embrace-the- cloud-is-now/.  [33]  \"What is SELinux?,\" [Online]. Available: https://www.redhat.com/en/topics/linux/what-is- selinux.  [34]  \"Intelligent Platform Management Interface,\" [Online]. Available:  https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface.  [35]  \"BIOS,\" [Online]. Available: https://en.wikipedia.org/wiki/BIOS.  [36]  \"UEFI,\" [Online]. Available: https://uefi.org/.  [37]  \"MPI Forum,\" [Online]. Available: https://www.mpi-forum.org/.  [38]  \"How to share files with Samba,\" [Online]. Available:  https://www.redhat.com/sysadmin/samba-file-sharing.  [39]  \"Network address translation,\" [Online]. Available:  https://en.wikipedia.org/wiki/Network_address_translation.  [40]  \"Squid-cache.org,\" [Online]. Available: http://www.squid-cache.org/.  [41]  \"Security Technical Implementation Guides (STIGs),\" [Online]. Available:  https://public.cyber.mil/stigs/.  [42]  \"Lawrence Livermore National Laboratory,\" [Online]. Available: https://www.llnl.gov/.  [43]  \"Sandia National Laboratories,\" [Online]. Available: https://www.sandia.gov/.  [44]  \"Los Alamos National Lab,\" [Online]. Available: https://www.lanl.gov/.  [45]  \"Defense Information Systems Agency,\" [Online]. Available: https://disa.mil/.  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  22  [46]  \"DISA releases the TOSS 4 Security Technical Implementation Guide,\" [Online].  Available: https://public.cyber.mil/announcement/disa-releases-the-toss-4-security- technical-implementation-guide/.  681  1. Introduction  2. HPC System Reference Model and Main Components  2.1. Main Components  2.1.1. Components of the High-Performance Computing Zone  2.1.2. Components of the Data Storage Zone  2.1.2.1. Parallel File System  2.1.2.2. Archival and Campaign Storage  2.1.2.3. Burst Buffer  2.1.3. Components of the Access Zone  2.1.4. Components of the Management Zone  2.1.4.1. General Architecture and Characteristics  2.1.4.2. Basic Services  2.1.4.3. Configuration Management  2.1.4.4. HPC Scheduler and Workflow Management  2.1.5. HPC Software  2.1.5.1. User Software  2.1.5.2. Site-Provided Software and Vendor Software  2.1.6. Container Usage in HPC  2.2. HPC Architecture Variants  2.2.1. Diskless Booting HPC  2.2.2. Virtual and Hybrid HPC Environments  3. HPC Threat Analysis  3.1. Key HPC Security Characteristics and Use Requirements  3.2. Threats to HPC Function Zones  3.2.1. Access Zone Threats  3.2.2. Management Zone Threats  3.2.3. High-Performance Computing Zone Threats  3.2.4. Data Storage Zone Threats  3.3. Other Threats  4. HPC Security Posture, Challenges, and Recommendations  4.1. HPC Access Control via Multiple Physical Networks  4.2. Compute Node Sanitization  4.3. Data Integrity Protection  4.4. Securing Containers  4.5. Achieving Security While Maintaining HPC Performance  4.6. Challenges to HPC Security Tools  5. Conclusions  References",
    "original document": "NIST SP 800-223 initial public draft, High-Performance Computing (HPC) Security: Architecture, Threat Analysis, and Security Posture  NIST Special Publication  NIST SP 800-223 ipd  High-Performance Computing  (HPC) Security:  Architecture, Threat Analysis, and Security Posture  Initial Public Draft  Yang Guo  Ramaswamy Chandramouli  Lowell Wofford  Rickey Gregg  Gary Key  Antwan Clark  Catherine Hinton  Andrew Prout  Albert Reuther  Ryan Adamson  Aron Warren  Purushotham Bangalore  Erik Deumens  Csilla Farkas  This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.800-223.ipd  https://crossmark.crossref.org/dialog/?doi=10.6028/NIST.SP.800-223.ipd  NIST Special Publication  NIST SP 800-223 ipd  High-Performance Computing  (HPC) Security:  Architecture, Threat Analysis, and Security Posture  Initial Public Draft  Yang Guo  Ramaswamy Chandramouli  Computer Security Division  Information Technology  Laboratory  Lowell Wofford  Amazon.com, Inc.  Rickey Gregg  Gary Key  HPCMP  Antwan Clark  Laboratory for Physical Sciences  Catherine Hinton  Los Alamos National Laboratory  Andrew Prout  Albert Reuther  MIT Lincoln Laboratory  Ryan Adamson  Oak Ridge National Laboratory  Aron Warren  Sandia National Laboratories  Purushotham Bangalore  University of Alabama  Erik Deumens  University of Florida  Csilla Farkas  University of South Carolina  This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.800-223.ipd  February 2023  U.S. Department of Commerce  Gina M. Raimondo, Secretary  National Institute of Standards and Technology  Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  Certain commercial entities, equipment, or materials may be identified in this document in order to describe an  experimental procedure or concept adequately. Such identification is not intended to imply recommendation or  endorsement by the National Institute of Standards and Technology (NIST), nor is it intended to imply that the  entities, materials, or equipment are necessarily the best available for the purpose.  There may be references in this publication to other publications currently under development by NIST in  accordance with its assigned statutory responsibilities. The information in this publication, including concepts and  methodologies, may be used by federal agencies even before the completion of such companion publications. Thus,  until each publication is completed, current requirements, guidelines, and procedures, where they exist, remain  operative. For planning and transition purposes, federal agencies may wish to closely follow the development of  these new publications by NIST.  Organizations are encouraged to review all draft publications during public comment periods and provide feedback  to NIST. Many NIST cybersecurity publications, other than the ones noted above, are available at  https://csrc.nist.gov/publications.  Authority  This publication has been developed by NIST in accordance with its statutory responsibilities under the Federal  Information Security Modernization Act (FISMA) of 2014, 44 U.S.C. § 3551 et seq., Public Law (P.L.) 113-283.  NIST is responsible for developing information security standards and guidelines, including minimum requirements  for federal information systems, but such standards and guidelines shall not apply to national security systems  without the express approval of appropriate federal officials exercising policy authority over such systems. This  guideline is consistent with the requirements of the Office of Management and Budget (OMB) Circular A-130.  Nothing in this publication should be taken to contradict the standards and guidelines made mandatory and binding  on federal agencies by the Secretary of Commerce under statutory authority. Nor should these guidelines be  interpreted as altering or superseding the existing authorities of the Secretary of Commerce, Director of the OMB, or  any other federal official.  This publication may be used by nongovernmental organizations on a voluntary basis and  is not subject to copyright in the United States. Attribution would, however, be appreciated by NIST.  NIST Technical Series Policies  Copyright, Use, and Licensing Statements  NIST Technical Series Publication Identifier Syntax  Publication History  Approved by the NIST Editorial Review Board on YYYY-MM-DD [will be add upon final publication]  How to Cite this NIST Technical Series Publication:  Guo Y, Chandramouli R, Wofford L, Gregg R, Key G, Clark A, Hinton C, Prout A, Reuther A, Adamson R, Warren  A, Bangalore P, Deumens E, Farkas C (2023) High-Performance Computing (HPC) Security: Architecture, Threat  Analysis, and Security Posture. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Special  Publication (SP) NIST SP 800-223 ipd. https://doi.org/10.6028/NIST.SP.800-223.ipd  Author ORCID iDs  Yang Guo: 0000-0002-3245-3069  Ramaswamy Chandramouli: 0000-0002-7387-5858  Lowell Wofford: 0000-0003-1003-5090  Albert Reuther: 0000-0002-3168-3663  Aron Warren: 0000-0002-5090-2198  Purushotham Bangalore: 0000-0002-1098-9997  https://csrc.nist.gov/publications https://doi.org/10.6028/NIST-TECHPUBS.CROSSMARK-POLICY https://www.nist.gov/document/publication-identifier-syntax-nist-technical-series-publications  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  Erik Deumens: 0000-0002-7398-3090  Public Comment Period  February 6, 2023 – April 7, 2023  Submit Comments  sp800-223-comments@list.nist.gov  National Institute of Standards and Technology  Attn: Computer Security Division, Information Technology Laboratory  100 Bureau Drive (Mail Stop 8930) Gaithersburg, MD 20899-8930  All comments are subject to release under the Freedom of Information Act (FOIA).  mailto:sp800-223-comments@list.nist.gov  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  i  Abstract 1  Security is an essential component of high-performance computing (HPC). HPC systems often 2  differ based on the evolution of their system designs, the applications they run, and the missions 3  they support. An HPC system may also have its own unique security requirements, follow 4  different security guidance, and require tailored security solutions. Their complexity and 5  uniqueness impede the sharing of security solutions and knowledge. This NIST Special 6  Publication aims to standardize and facilitate the information and knowledge-sharing of HPC 7  security using an HPC system reference model and key components as the basics of an HPC 8  system lexicon. This publication also analyzes HPC threats, considers current HPC security 9  postures and challenges, and makes best-practice recommendations. 10  Keywords 11  high-performance computing (HPC); HPC security; HPC reference model; security guidance; 12  HPC threat analysis; HPC security posture. 13  Reports on Computer Systems Technology 14  The Information Technology Laboratory (ITL) at the National Institute of Standards and 15  Technology (NIST) promotes the U.S. economy and public welfare by providing technical 16  leadership for the Nation’s measurement and standards infrastructure. ITL develops tests, test 17  methods, reference data, proof of concept implementations, and technical analyses to advance 18  the development and productive use of information technology. ITL’s responsibilities include the 19  development of management, administrative, technical, and physical standards and guidelines for 20  the cost-effective security and privacy of other than national security-related information in 21  federal information systems. The Special Publication 800-series reports on ITL’s research, 22  guidelines, and outreach efforts in information system security, and its collaborative activities 23  with industry, government, and academic organizations.  24  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  ii  Call for Patent Claims 25  This public review includes a call for information on essential patent claims (claims whose use 26  would be required for compliance with the guidance or requirements in this Information 27  Technology Laboratory (ITL) draft publication). Such guidance and/or requirements may be 28  directly stated in this ITL Publication or by reference to another publication. This call also 29  includes disclosure, where known, of the existence of pending U.S. or foreign patent applications 30  relating to this ITL draft publication and of any relevant unexpired U.S. or foreign patents. 31  ITL may require from the patent holder, or a party authorized to make assurances on its behalf, 32  in written or electronic form, either: 33  a) assurance in the form of a general disclaimer to the effect that such party does not hold 34  and does not currently intend holding any essential patent claim(s); or 35  b) assurance that a license to such essential patent claim(s) will be made available to 36  applicants desiring to utilize the license for the purpose of complying with the guidance 37  or requirements in this ITL draft publication either: 38  i. under reasonable terms and conditions that are demonstrably free of any unfair 39  discrimination; or 40  ii. without compensation and under reasonable terms and conditions that are 41  demonstrably free of any unfair discrimination. 42  Such assurance shall indicate that the patent holder (or third party authorized to make assurances 43  on its behalf) will include in any documents transferring ownership of patents subject to the 44  assurance, provisions sufficient to ensure that the commitments in the assurance are binding on 45  the transferee, and that the transferee will similarly include appropriate provisions in the event of 46  future transfers with the goal of binding each successor-in-interest. 47  The assurance shall also indicate that it is intended to be binding on successors-in-interest 48  regardless of whether such provisions are included in the relevant transfer documents. 49  Such statements should be addressed to: sp800-223-comments@list.nist.gov  50  mailto:sp800-223-comments@list.nist.gov  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  iii  Table of Contents 51  Introduction ......................................................................................................................... 1 52  HPC System Reference Model and Main Components .................................................... 2 53  Main Components ......................................................................................................... 3 54  2.1.1. Components of the High-Performance Computing Zone ............................................ 3 55  2.1.2. Components of the Data Storage Zone ...................................................................... 4 56  2.1.3. Components of the Access Zone ................................................................................ 5 57  2.1.4. Components of the Management Zone ...................................................................... 6 58  2.1.5. HPC Software ............................................................................................................. 7 59  2.1.6. Container Usage in HPC ............................................................................................. 8 60  HPC Architecture Variants ............................................................................................ 9 61  2.2.1. Diskless Booting HPC ................................................................................................. 9 62  2.2.2. Virtual and Hybrid HPC Environments ........................................................................ 9 63  HPC Threat Analysis ......................................................................................................... 11 64  Key HPC Security Characteristics and Use Requirements ......................................... 11 65  Threats to HPC Function Zones .................................................................................. 11 66  3.2.1. Access Zone Threats ................................................................................................ 11 67  3.2.2. Management Zone Threats ....................................................................................... 12 68  3.2.3. High-Performance Computing Zone Threats ............................................................ 12 69  3.2.4. Data Storage Zone Threats ...................................................................................... 13 70  Other Threats .............................................................................................................. 13 71  HPC Security Posture, Challenges, and Recommendations ........................................ 15 72  HPC Access Control via Multiple Physical Networks .................................................. 15 73  Compute Node Sanitization ......................................................................................... 16 74  Data Integrity Protection .............................................................................................. 16 75  Securing Containers .................................................................................................... 17 76  Achieving Security While Maintaining HPC Performance ........................................... 17 77  Challenges to HPC Security Tools .............................................................................. 17 78  Conclusions ....................................................................................................................... 19 79  References ................................................................................................................................. 20 80  List of Figures 81  Fig. 1. HPC System Reference Model .......................................................................................... 3 82  83  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  1  Introduction 84  In 2015, Executive Order 13702 established the National Strategic Computing Initiative (NSCI) 85  to maximize the benefits of high-performance computing (HPC) for economic competitiveness 86  and scientific discovery. The ability to process large volumes of data and perform complex 87  calculations at high speeds is a key part of the Nation’s vision for maintaining its global 88  competitive edge.  89  Security is an essential to achieving the anticipated benefits of HPC. HPC systems bear some 90  resemblance to ordinary IT computing, which allows for the effective application of traditional 91  IT security solutions. However, they also have significant differences. An HPC is designed to 92  maximize performance so its architecture, hardware components, software stacks, and working 93  environment are very different from ordinary IT. As such, security solutions must be tailored to 94  the HPC system’s requirements. Furthermore, HPC systems are often different from one another 95  due to the evolution of their system designs, the applications they run, and the missions they 96  support. An HPC system frequently has its own unique security requirements and follows 97  different security guidance, which can impede the sharing of security solutions and knowledge.  98  This NIST Special Publication aims to standardize and facilitate the sharing of HPC security 99  information and knowledge through the development of an HPC system reference model and key 100  components, which are introduced as the basics of the HPC system lexicon. The reference model 101  divides an HPC system into four function zones. A zone based on the HPC reference model 102  captures the most common features across the majority of HPC systems and segues into HPC 103  system threat analysis. Key HPC security characteristics and use requirements are laid out 104  alongside the major threats faced by the system and individual function zones. HPC security 105  postures, challenges, and recommendations are also included.  106  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  2  HPC System Reference Model and Main Components 107  The HPC system is complex and evolving so a common lexicon can help describe and identify 108  an HPC system’s architecture, critical elements, security threats, and potential risks. An HPC 109  system is divided into four function zones:  110  1. The high-performance computing zone consists of a pool of compute nodes connected by 111  one or more high-speed networks. The high-performance computing zone provides key 112  services specifically designed to run parallel jobs at scale.  113  2. The data storage zone comprises one or multiple high-speed parallel file systems that 114  provide data storage service for user data. The high-speed parallel file systems are 115  designed to store very large data sets and provide fast access to data for reading and 116  writing. 117  3. The access zone has one or more nodes that are connected to external networks, such as 118  the broader organizational network or the internet. This zone provides the means for 119  authenticating and authorizing the access and connections of users and administrators. 120  The access zone provides various services, including interactive shells, web-based 121  portals, data transfer, data visualization, and others.  122  4. The management zone comprises multiple management nodes and/or cloud service 123  clusters through which HPC management services are provided. The management zone 124  allows HPC system administrators to configure and manage the HPC system, including 125  the configuration of compute nodes, storage, and networks, provisioning, identity 126  management, auditing, system monitoring, and vulnerability assessment. It also offers, 127  through a portal in the access zone, users an interface to acquire high-performance 128  computing services and to configure access to data storage services. Various management 129  software modules – from job scheduler, workflow management, and the Domain Name 130  System (DNS) – run in the management zone. 131  The HPC system reference model is depicted in Figure 1. 132  133  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  3  134  Fig. 1. HPC System Reference Model 135  Main Components 136  2.1.1. Components of the High-Performance Computing Zone 137  An HPC cluster consists of a collection of independent computing systems, called compute 138  nodes, which are interconnected via high-speed networks. Compute nodes have the same 139  components as a laptop or desktop, including central processing unit (CPU) cores, memory, disk 140  space, and networking interface cards. However, they are equipped with a much larger number 141  of CPU cores and more memory than a laptop or desktop. In some HPC architectures, a compute 142  node may not have local disks and uses the data storage services of remote storage servers 143  instead. In addition, there may be different types of nodes for different types of tasks, and some 144  compute nodes are equipped with hardware accelerators to speed up specific applications. For 145  instance, compute nodes often utilize graphics processing units (GPUs) [1] to accelerate 146  modeling and simulation or AI and machine learning (ML) model training.  147  An HPC compute node installs its own software stack (e.g., operating system [OS], compilers, 148  software libraries, etc.) to support applications. The installation and configuration of the software 149  stacks are cluster-wide, centrally managed, and controlled by the management zone. The number 150  of compute nodes in an HPC ranges from a few nodes to hundreds and even thousands of nodes.  151  A critical requirement of HPC networking that interconnects computer nodes is to have massive 152  amounts of scalable bandwidth (throughput) while keeping the latency ultra-low so that the 153  compute nodes and parallel file system (PFS) in the data storage zone can work as one 154  supercomputer. HPC networking often employs specifically designed protocols, networking 155  cards, processor nodes, and switches to optimize network performance. The popular HPC 156  interconnect networking includes InfiniBand [2], Omni-Path [3], Slingshot [4], and others.  157  A high-performance computing zone typically utilizes non-high-performance communication 158  networks, like ethernet, as cluster internal networks that connect the high-performance 159  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  4  computing zone with the management zone and access zone for traffic associated with 160  maintenance activities as opposed to HPC traffic. 161  2.1.2. Components of the Data Storage Zone 162  Several different classes of storage systems may be present inside of the data storage zone. In 163  general, storage systems within this zone cannot be effectively separated from the HPC resources 164  that they support from an administrative privilege perspective. Typical classes of storage found 165  within this zone include parallel file systems (PFS), node-local storage for low-latency 166  workloads, and archival file systems that support campaign storage and protect against data loss. 167  HPC systems may have other file systems that store non-user data. For instance, the management 168  zone often has its own file system that stores the OS images and configuration files. In that case, 169  the file system is included in the corresponding function zone. 170  HPC applications’ initial data, intermediate results, and final results are stored in the data storage 171  zone and can be accessed during the application runtime and after the application’s completion. 172  External HPC users can also access user data through the login nodes and/or data transfer nodes 173  in the access zone. 174  The storage capacity of these file systems is often measured in petabytes and can reach up to 175  exabytes. File systems within the data storage zone will generally use a transport mechanism 176  appropriate to the tier. For example, high-bandwidth file systems may be attached to the HPC 177  resource’s high-performance network, while lower bandwidth file systems may use 10 Gbps or 178  100 Gbps ethernet. Access control for most HPC file systems is enforced by the operating system 179  software of nodes on which these file systems are mounted. As such, file systems should not be 180  mounted outside of their security boundaries. A rogue system that can mount a file system will 181  have complete control of all file system data, can spoof packets on the high-speed network, and 182  can possibly gain privileges elsewhere within other zones of the HPC security enclave. 183  2.1.2.1. Parallel File System 184  Since HPC workloads can vary significantly, a parallel file system (PFS) is often required to 185  support read-intensive and write-intensive applications with sequential and random-access 186  patterns at speeds of up to terabytes per second. Commonly seen file systems include Lustre [5], 187  GPFS [6], and IBM Spectrum Scale [7]. During procurement, a PFS will typically be designed to 188  hit a particular aggregate bandwidth target rather than a capacity requirement. These PFS will 189  typically consist of a cluster of systems to maintain metadata about files and locations as well as 190  servers that act as storage targets. Clients that mount the file system will typically load the file 191  system client software via a kernel module. Storage target servers will have backing storage 192  arrays configured with dozens of disks in a redundant array of inexpensive disk (RAID) strategy. 193  Both GPFS and Lustre-based PFS are prone to performance degradation when a certain capacity 194  threshold is reached. These file systems may be regularly pruned of unwanted files with a 195  strategy decided by the file system administrators. Some deployments will sweep files older than 196  a certain age, which forces HPC users to transfer job output to a longer-term file system, such as 197  campaign storage. PFS tends to be somewhat unreliable depending on the types of activities 198  being performed by running jobs and users. Because these are distributed file systems, file-199  system software must solve distributed locking of files to ensure deterministic file updates when 200  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  5  multiple clients are writing to the same file at once. Additionally, PFS are susceptible to denial-201  of-service conditions even during legitimate user operations, such as listing a directory with 202  millions of files or applications that perform poor file locking semantics. 203  2.1.2.2. Archival and Campaign Storage 204  Archival and campaign storage systems represent a class of storage that is more resilient to 205  failure conditions than PFS and is often less expensive per GigaByte. These advantages come at 206  the cost of bandwidth and an increased latency of data transfer. While PFS acts as a temporary 207  short-term scratch file system, campaign storage supports the longer-term storage needs of a 208  project over its life cycle.  Finished data products that support scientific publications or other 209  high-value datasets may also be stored in an archival file system. The retention time for data on a 210  campaign storage file system is measured in years, while the retention of data within an archival 211  storage system is measured in decades. Both campaign and archival storage systems might 212  employ low-latency disks – such as solid state drives (SSD) or non-volatile memory express 213  (NVMe) drives within a small tier of storage that acts as a cache – and are backed with cheaper, 214  higher capacity media, such as spinning disk and/or tape media. 215  2.1.2.3. Burst Buffer 216  For applications that require extremely low latency or high-bandwidth memory-to-disk data 217  transfer during runtime, intermediate storage layers that contain “burst buffers” have been 218  incorporated as brokers to primarily mitigate the effects of input/output (I/O) contention and the 219  bandwidth burden on parallel file system (PFS). These burst buffers can pre-fetch data from the 220  parallel file system (PFS) before a computing job begins and stage data out to a parallel file 221  system after a computing job has completed. This saves job runtime that would normally be 222  spent performing bulk I/O to the PFS and allows it to be spent on computation instead. Typical 223  HPC infrastructures contain the following intermediate storage architectures: 224  • Node-local burst buffer architectures: Each burst buffer is collocated with a 225  corresponding HPC compute node [8]. This is advantageous for its scalability and also 226  improves the checkpoint bandwidth because the aggregate bandwidth increases in 227  proportion with the number of compute nodes.  228  • Remote-shared burst buffer architectures: Burst buffers are shared between multiple 229  HPC compute nodes that are hosted on an I/O node [8]. This is advantageous for 230  facilitating the development, deployment, and maintenance of these architectures.  231  There are also HPCs that can contain mixed burst buffer intermediate storage architectures, 232  which are a mixture of the strengths of node-local and remote-shared burst buffer architectures.  233  2.1.3. Components of the Access Zone 234  A typical HPC system provides one or more nodes through which users and administrators 235  access the system. At least one of these nodes is a login node where users have access to shells to 236  launch interactive or batch jobs. Some of these login nodes may also have specialized 237  visualization hardware and software with which users can conduct interactive and/or post-238  execution visualization of their datasets. There may also be one or more data transfer nodes that 239  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  6  provide services to transfer data into and out of the HPC system and may even provide storage-240  mounting services like Network File System (NFS) [9], Common Internet File System (CIFS) 241  [10], Server Message Block (SMB) [11], and Filesystem in Userspace (FUSE) [12] based SSH 242  Filesystem (SSHFS) [13]. Many HPC systems now provide web portals via web portal nodes 243  that enable a variety of web interfaces to HPC system services.  244  2.1.4. Components of the Management Zone 245  The complexity of HPC systems requires a significant infrastructure to operate and manage it, 246  which is collectively referred to as the management zone. The management zone may consist of 247  servers and network switches that enable various functions for operating the system with 248  efficiency, effectiveness, and stability. 249  2.1.4.1. General Architecture and Characteristics 250  One important characteristic of the management zone is that it has a separate security posture 251  because non-privileged users do not need to access the management servers or services in a 252  direct way. Privileged users responsible for configuring, maintaining, and operating the HPC 253  system access the management zone servers and switches through extra security controls. For 254  example, from the public-facing login nodes, they may go through a bastion host that is typically 255  located in the management zone, or they may establish a private virtual network (VPN) with 256  separate authentication and authorization or other appropriate security controls to reach the 257  management zone. All systems are configured on networks that are not routed beyond the 258  perimeter of the HPC system so that only nodes like compute nodes and storage nodes can access 259  the services. 260  The services provided by the management zone have clearly defined protocols and can be 261  implemented as running on assigned hardware platforms or run as virtual machines on a 262  dedicated set of hardware resources. The fact that the management zone has a clear and separate 263  security posture helps with risk assessment and the selection of controls to secure the 264  management zone and manage the risk. 265  2.1.4.2. Basic Services 266  The HPC resources inside of the computing and data storage zones need various services to 267  operate. Examples include Domain Name Services (DNS) [14]; the Dynamic Host Configuration 268  Protocol (DHCP) [15]; configuration definitions, authentication, and authorization services, such 269  as those provided by an LDAP server [16]; and the Network Time Protocol (NTP) [17] for 270  synchronization, log management, version-controlled repositories. 271  The management zone includes storage systems to store configuration data and node images, 272  current versions, development and test versions, and historical versions. Storing logs from the 273  entire HPC system is also part of the management zone as well as the servers to process the logs 274  and alert administrators of events, problems, and incidents. Many of these services will be 275  implemented with high availability and failover capabilities to avoid failure of the HPC 276  resources. The network switches for the management network (ethernet) and the fast 277  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  7  interconnects (e.g., InfiniBand, Omni-Path, Slingshot) are often managed as part of the 278  management zone because non-privileged users do not need direct access to these resources. 279  2.1.4.3. Configuration Management 280  Automated configuration management is crucial to ensure the stable operation of complex 281  systems like HPC. The systems that hold the configuration database and run the server to place 282  configurations on compute nodes, storage servers, and network switches are part of the 283  management zone. The nodes are subject to a regularly scheduled process to verify configuration 284  and enforce consistency with what the configuration management nodes and databases specify. 285  Often, the configuration management systems in the management zone have an even more 286  restricted security posture than the management zone as a whole, with a smaller number of 287  privileged users having access.  288  2.1.4.4. HPC Scheduler and Workflow Management 289  Because of the distributed nature of HPC systems, requesting resources for given workloads is 290  coordinated by a scheduler or workload manager, such as Slurm [18] and Kubernetes [19]. These 291  services are run on servers in the management zone alongside the configurations and job logs. 292  Non-privileged users access the scheduler through specific commands or an application 293  programming interface (API). Access to the service is restricted to nodes within the HPC system 294  perimeter. There may also be a web interface that provides a separate authenticated and 295  authorized path for scheduling workloads, often within the strict constraints of certain 296  application domains. 297  2.1.5. HPC Software 298  In addition to the management software that installs, boots, configures, and manages HPC-299  related systems, HPC application codes rely on several layers of scientific and performance-300  enhancing libraries. The layers of software that are available to users is referred to as the 301  software stack. The lower layers of this stack are typically focused on performance and include 302  compilers, communication libraries, and user-space interfaces to HPC hardware components. 303  The middle layer includes performance tools, math libraries, and data or computation abstraction 304  layers. The top of the stack consists of end-user science or production applications. Each 305  software product within this stack may require certain versions or variants of other products and 306  have many dependencies. For example, Hierarchical Data Format version 5 (HDF5) [20] is a 307  scientific data formatting library with only seven dependencies, while Data Mining Classification 308  and Regression Methods (rminer) [21] – an R-based data mining application – has 150 software 309  dependencies. The full software stack can be split into three general categories that differ based 310  on the maintainer: user software, facility software, and vendor software.  311  2.1.5.1. User Software 312  Often, the end users themselves best understand how to tune their software to the bespoke 313  hardware of an HPC system to ensure sufficient performance for their workload. Users regularly 314  modify and recompile their software to enhance performance, fix bugs, and adapt to changes in 315  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  8  the underlying dependencies or kernel interfaces over time. The sharing of user-built software 316  between teams may be common. User software that is widely used is often open source and, 317  therefore, subject to open-source software supply chain concerns. 318  Continuous integration (CI) pipelines [22] and tests of scientific code on HPC platforms have 319  recently become commonplace. Industry-standard identification of software weaknesses and the 320  publication of Common Vulnerabilities and Exposures (CVEs) [23] is not routine, but the 321  identification and remediation of performance regressions is generally a higher priority within 322  the user community. There is a value-per-cycle trade-off for CI tests since cheaper cycles on 323  commodity hardware may not expose bugs on much more expensive HPC resources. Complex 324  test suites will eat into user allocations, and users and staff prefer that only a cardinal set of 325  smoke tests run within user-developed testing pipelines on HPC systems. 326  2.1.5.2. Site-Provided Software and Vendor Software 327  Site staff and administrators generally build applications and libraries that are most likely to be 328  used. Tools such as Conda [24], EasyBuild [25], and Spack [26] are often used to manage the 329  complexity of software dependency resolution. Staff may also wrap compiler and job submission 330  utilities with custom scripts to collect usage information about software libraries, I/O read and 331  write patterns, or other system telemetry that is useful for decision making. 332  Vendor software includes low-level system tools to facilitate the running of other software. For 333  instance, remote direct memory access, inter-node memory sharing, performance counters, 334  temperature and power telemetry, and debugging are all vendor-provided software.  335  Users can choose specific versions of installed vendor and site-provided software libraries by 336  manipulating environment variables. Tools such as wrapper scripts or module files are usually 337  provided to help users find and choose which versions of installed software to use. 338  2.1.6. Container Usage in HPC 339  A container is a software package that contains an application’s entire runtime environment. It 340  consists of an application program and all of the dependencies, libraries, other binaries, and 341  configuration files needed to run it. Containers provide self-contained, portable, and reproducible 342  environments that abstract away the differences in OS distributions and underlying 343  infrastructure. Containers make applications more portable and the deployment easier. For 344  instance, containers allow users to use a package manager (e.g., apt [27] or yum [28]) to install 345  software without changing anything on the host system or to run the latest software built for 346  newer Linux OS versions. 347  Security is a major concern in deploying containers in HPC environments. Containers possess 348  large attack surfaces due to the different underlying images, each of which can have 349  vulnerabilities. In addition, securing the host is not enough to ensure protection. Container 350  permissions and proper isolation are also necessary. Finally, monitoring containers can be 351  difficult due to its dynamic nature.  352  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  9  HPC Architecture Variants 353  2.2.1. Diskless Booting HPC 354  A variant of HPC cluster design is diskless booting clusters in which the OS is not loaded from 355  storage located on the node itself. Diskless nodes typically boot up, obtain an IP address via 356  DHCP, and receive a kernel and partial OS over the network through a protocol, such as the 357  Preboot eXecution Environment (PXE) [29] . To finish the boot process, the remaining OS is 358  typically loaded over NFS or Internet Small Computer System Interface (ISCSI) [30] .  359  For scalability, some HPC clusters may implement a hierarchy design in which intermediate 360  servers boot diskless but have local storage that is used to serve the OS to leaf nodes.  361  2.2.2. Virtual and Hybrid HPC Environments 362  Virtualization technologies can be used to recreate and maintain traditional HPC architectures 363  that allow them to leverage the virtual infrastructure effectively while also providing additional 364  scalability. For example, virtual machines (VMs) can be used to redistribute scientific 365  applications across heterogeneous supercomputing architectures that can be bundled more easily 366  without the need for certain special privileges. Virtual HPC (vHPC) architectures typically 367  contain the following main components [31]:  368  • Hypervisors: These elements create and run VMs in vHPC architectures, which allows 369  them to use multiple resources, such as memory and processing. The benefits to these 370  elements include their speed, efficiency, flexibility, convenience, and portability.  371  • vCenter Server Appliance (VCSA): This element provides centralized management of 372  all virtualized infrastructure with a single management interface that interacts, manages, 373  and monitors the virtualization configuration, settings, and services.  374  • Network virtualization and security platform: This platform provides software-375  defined networking (SDN) management and protection capabilities to the vHPC 376  infrastructure.  377  Some optional technologies include Kubernetes-based platforms and technologies that accelerate 378  the deployment and management of applications and services, optimize systems and application 379  environments, and securely connect and operate remotely. vHPC architectures also contain 380  access, management, compute, and data storage zones, as highlighted in Figure 1, in which the 381  management zones contain VMs that manage the vHPC environment. The compute zones 382  contain virtual compute clusters that are responsible for HPC workloads, while the storage zones 383  consist of virtualized PFS capabilities that store user data. Virtual compute clusters can contain 384  low-latency burst buffer storage that is local to each node or shared across nodes. 385  Cloud technologies can also be employed to enhance traditional HPC infrastructures, which 386  result in the creation of hybrid HPC environments that contain a combination of traditional and 387  cloud HPC architectures [32]. These types of schemas typically use private cloud capabilities 388  because they are more flexible, customizable, and manageable than using public cloud 389  technologies. Recently, high-performance computing as a service (HPCaaS) has been adopted as 390  a flexible solution that offers similar supercomputing capabilities on the cloud. Organizations 391  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  10  can work with vendors or service providers to customize these architectures to their specific 392  needs and maintain control by changing their capacities and architectures. HPCaaS is also cost-393  effective because these solutions can typically be purchased via pay-as-you-go subscriptions that 394  are managed by their service providers.  395  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  11  HPC Threat Analysis 396  HPC poses unique security and privacy challenges, and collaboration and resource-sharing are 397  integral. HPC workloads are often different from their traditional counterparts. For instance, 398  scientific experiments frequently employ unique hardware, software, and configurations that may 399  not be maintained or well-vetted or that present entirely new classes of vulnerabilities absent in 400  more traditional environments. HPC can store large amounts of sensitive research data, 401  personally identifiable information (PII), and intellectual property (IP) that need to be 402  safeguarded. Finally, HPC data and computation are encumbered with a variety of different 403  security and policy constraints. The solutions to protecting data, computation, and workflows 404  must balance these trade-offs. 405  Key HPC Security Characteristics and Use Requirements  406  HPC systems possess some unique security characteristics and distinctive use requirements that 407  differentiate themselves from average IT system: 408  409  • Tussles between performance and security: HPC users may consider security valuable 410  only to the extent that it does not significantly slow down the HPC system and impede 411  research. Ensuring the usability of security mechanisms with a tolerable performance 412  penalty is therefore critical to adoption by the scientific HPC community. 413  • Varying security requirements for different HPC applications: Individual platforms, 414  projects, and data may have significantly different security sensitivities and need to 415  follow different security policies. An HPC may need to enforce multiple security policies 416  simultaneously. 417  • Limited resources for security tools: Most HPC systems are designed to devote their 418  resources to maximizing performance rather than acquiring and operating security tools. 419  • Open-source software and self-developed research software: Open-source software 420  and self-developed research software are widely used in HPC. Open-source software is 421  vulnerable to open-source software supply chain threats, while HPC software input data 422  may also be vulnerable to data supply chain threats. Self-developed software is 423  susceptible to low software quality. 424  • Granular access control on databases: Since different research groups may have a need 425  to know for different portions of data, granular access control capabilities are necessary. 426  Access control requirements may need to be dynamically adjusted as some scientific 427  experiments may increase data needs based on the outcome of experiments. 428  Threats to HPC Function Zones 429  3.2.1. Access Zone Threats  430  The access zone provides an interface for external users to access the HPC system and oversees 431  the authentication and authorization of users. Among the four function zones, the access zone is 432  the only one that is directly connected to the external networks. Hence, the nodes and their 433  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  12  software stacks in this zone are susceptible to external attacks, such as denial of service (DoS) 434  attacks, perimeter network scanning and sniffing, authentication attacks (e.g., brute force login 435  attempts and password guessing), user session hijacking, and machine-in-the-middle attacks. In 436  addition, some nodes are subject to extra attacks due to their specific software stacks. For 437  instance, a web server may be subject to website defacement, phishing, misconfiguration, and 438  code injection attacks. The access zone also provides access to the file systems hosted in the data 439  storage zone. It is important that permissions to directories and files are only given to authorized 440  accesses. 441  Authenticated users sometimes use external networks to download data or code for use inside of 442  the HPC system, which introduces the risk of unintentionally downloading malicious content. 443  The nodes in the access zone are usually configured to support limited computation (e.g., modest 444  debugging). Access zone nodes are susceptible to computational resource abuse. 445  The access zone is also shared by multiple users. One user’s activities, such as commands issued 446  and jobs submitted, can be viewed by other users. A port opened by one user can potentially be 447  used by others. Fortunately, the nodes in the access zone work similarly to enterprise servers, and 448  general IT security tools and measures are available to harden the zone. 449  3.2.2. Management Zone Threats 450  The management zone is responsible for managing the entire HPC system. It is connected to 451  clustered internal networks through which other zones can be reached. It runs a plethora of 452  system management, out-of-band hardware management, job scheduling, and workflow 453  management software, all of which are susceptible to unique threats. 454  Processes running in the management zone, such as schedulers and data tiering/orchestration 455  processes, act on behalf of users. These are privileged processes, and if they are spoofed, it can 456  lead to privilege escalation, which is a distributed system-to-system trust problem. Due to the 457  implied delegation of authority within distributed HPC and file systems, ‘root’ on a compute 458  node may be, depending on configuration, equivalent to ‘root’ on all systems within the HPC 459  zones. Only administrators with privileged access authorization are allowed to log into the 460  management zone, where a privileged administrator logs into the access zone first and then logs 461  into the management zone. A malicious user may attempt to log into the management zone. 462  The management zone may also be implemented as a service running on a cloud via 463  virtualization technologies. In such cases, the risks associated with the cloud also apply to the 464  management zone. 465  3.2.3. High-Performance Computing Zone Threats  466  The high-performance computing zone offers core computational functions in an HPC system. 467  The computer nodes are shared by multiple users or tenants. The exploitation of multi-tenancy 468  environments is a major threat (e.g., side-channel attacks, user data/program leakage, etc.). Other 469  threat sources that often cause extreme resource consumption, performance degradation, or the 470  outage of the HPC system entirely include accidental misconfiguration, software bugs introduced 471  by user-developed software, and system abuse by running applications that are not aligned with 472  the HPC mission. Container escape, side-channel attacks, and DoS can also be threats if 473  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  13  virtualization technologies – such as containers – are used in the high-performance computing 474  zone.  475  As a security mitigating technology, the applications in HPC are mostly run in the user space, 476  except for system calls that must run in the kernel with elevated privilege. Accelerators, high-477  performance interconnects, special protocols, and direct memory access between nodes are 478  commonly used in the high-performance computing zone. Some of these technologies may not 479  be thoroughly tested, and their speed, novelty, and complexity can make monitoring and 480  detecting suspicious activity difficult. Direct memory access and communication between nodes 481  may bypass the kernel and the protections provided by the kernel (e.g., Security-Enhanced Linux 482  [SELinux] [33]) is lost. 483  3.2.4. Data Storage Zone Threats 484  Protecting the confidentiality and integrity of user data is essential for the data storage zone. Data 485  integrity can be compromised by malicious data deletion, corruption, pollution, or false data 486  injection so gaining unauthorized privileged access is a major threat. Legitimate users may also 487  mishandle sensitive data, leading to confidentiality breakdown. File metadata (e.g., file name, 488  author, size, creation date) can also leak sensitive information about the files. 489  HPC file systems in the data storage zone provide superior data access speed and much larger 490  storage size compared to average enterprise file systems. Hard disk failure is a threat due to the 491  large number of disks deployed in the data storage zone. Incident response and contingency 492  planning controls may not be easy to implement, and file backup, recovery, and forensic imaging 493  may become infeasible. The security measures that can be implemented on the enterprise file 494  systems may take an unacceptably long time and degrade HPC file system performance in an 495  unacceptable way. 496  Providing data backup services is another challenge in HPC due to its large volume. By default, 497  user data is often not backed up, and users are responsible for maintaining their own data copies. 498  Inadvertent operations (e.g., accidently deleting a file subdirectory) can cause the permanent loss 499  of data, though making data READ ONLY is one way to combat such a risk. Some organizations 500  offer backup services using their own HPC systems, but these systems may be in the same 501  geographic locations and subject to the same environmental threats.  502  Other Threats 503  In addition to the threats unique to individual function zones, the general HPC systems face the 504  following threats: 505  • Environmental and physical threats: Physical or cyber attacks against facilities (e.g., 506  power, cooling, water), unauthorized physical access, and natural disaster (e.g., fire, 507  flood, earthquake, hurricane, etc.) are all potential threats to an HPC system. 508  • Vulnerabilities introduced by prioritizing performance in HPC design and 509  operation: HPC is designed to process large volumes of data and perform complex 510  computations at very high speeds. Achieving the highest performance possible is a 511  priority in HPC design and operation. Such prioritization, however, has its security 512  implications. For instance, designers often make conscious decisions to build a less 513  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  14  redundant system to achieve high performance, making the system less robust and 514  potentially more vulnerable to attacks, such as DoS attacks. As another example, using a 515  backup system to improve system robustness and high availability is a proven 516  technology. Building a backup HPC system, however, is often infeasible since it is too 517  costly. HPC often lacks storage backup due to the vast size of the data stored. Similarly, 518  most HPC missions do not have a service-level agreement to justify the need for a full 519  backup system at a backup location. All resources are poured into building the single best 520  HPC system possible.  521  • Supply chain threats: The HPC supply chain faces a variety of threats, from the theft of 522  proprietary information to attacks on critical hardware components and software 523  manipulation to gain unauthorized access. Some HPC software (e.g., OS, BIOS, 524  applications), firmware, and hardware components have limited manufacturers, suppliers, 525  and integrators, which make diversification difficult. Limited suppliers also lead to 526  shortages in the qualified workforce who can perform required technical support.  527  • Insider threats: Insider threats come from people within the organization who have 528  internal information and may have the privileges needed to access the HPC system. 529  Insider threats can be classified into accidental/unintentional threats and 530  malicious/intentional threats. Unintentional threats come from the unintended side effects 531  of normal actions and activity. In contrast, a malicious insider may intentionally upload 532  malicious code into the HPC system.  533  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  15  HPC Security Posture, Challenges, and Recommendations 534  HPC Access Control via Multiple Physical Networks 535  Access control is a security technique that regulates who can access and/or use resources in a 536  computing environment. In HPC, multiple physical networks are constructed as an effective 537  means of access control: 538  • Management network: The management network is a dedicated network that allows 539  system administrators to remotely control, monitor, and configure computer nodes in an 540  HPC system. Modern computers are often equipped with the Intelligent Platform 541  Management Interface (IPMI) [34], which provides management and monitoring 542  capabilities that are independent of the host system’s CPU, firmware (e.g., BIOS [35]or 543  Unified Extensible Firmware Interface [UEFI] [36]), and operating system. For example, 544  IPMI allows system administrators to remotely power on/off unresponsive machines and 545  install custom operating systems. IPMIs are connected to the management network, 546  which can only be accessed by authorized system administrators. 547  • High-performance networks: High-performance networks offer high bandwidth and 548  low latency to connect computer nodes inside of the high-performance zone and data 549  storage zone. They also support features that are unique to HPC, such as remote memory 550  access over the network and the message passing interface (MPI) [37]. High-performance 551  networks often use special communications standards and architectures to achieve the 552  high performance (e.g., InfiniBand, Slingshot, Omni-Path, etc.). 553  • Auxiliary networks: Additional auxiliary networks can be added to support usability and 554  system manageability. For instance, a user network is constructed to allow users to 555  manipulate or share data or remotely log into and access the compute nodes. Depending 556  on the purpose of the networks, a subset of nodes from different zones are selected to be 557  party to the networks. As an example, a user network contains the nodes in the access 558  zone and the computer nodes in the high-performance computing zone. 559  There are many benefits to having multiple networks in an HPC system. First, all of these 560  networks are private and use different IP address ranges. Network traffic will remain on one 561  network, which facilitates monitoring and measurement. Second, individual networks often serve 562  specific purposes so only the relevant nodes are connected to the network. The networks 563  effectively segment the HPC system into smaller segments, which improves security. Finally, 564  multiple physical networks also provide a degree of fault tolerance. When one network goes 565  down, the system administrator can use the other network to diagnose and troubleshoot. 566  The compute nodes in the access zone are connected to the external network and assigned public 567  IP addresses, which allow users to remotely access the HPC systems. The user data can be shared 568  through the login nodes or the data transfer nodes. Some systems allow storage to be exported 569  using CIFS [10] or SMB [11] (e.g. via a SAMBA [38] server). If necessary, a network address 570  translation (NAT) [39] or a Squid proxy [40] can be installed to allow users on a private network 571  to access the internet and download new versions of software or share software data. However, a 572  NAT and Squid proxy can also be security weaknesses that demand extra caution and mitigation 573  considerations. 574  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  16  Employing multiple physical networks is a common and effective means for access control and 575  fault tolerance, and it is highly recommended. 576  Compute Node Sanitization 577  High-performance compute nodes are used by multiple tenants and projects. At the end of a task 578  run, the previous project may leave behind a residual “footprint,” such as the data in memory and 579  GPUs. It is important to sanitize the compute node so that data from previous jobs are not 580  accidently leaked and the new job can start with a clean slate. 581  The common practice of compute node sanitization includes:  582  • Conducting a node health check at the end of a job 583  • Removing a node or forcing a reboot if a node is deemed “unhealthy” 584  • Working with hardware and software vendors to provide management hooks to sanitize 585  the GPU 586  • Resetting GPUs to remove residual data between jobs 587  • Validating and checking firmware 588  • Rebooting nodes after the completion of a job at the OS level to remove accumulated 589  residuals and ensure a consistent node state 590  • Checking critical files to ensure that they have not been changed 591  Compute node sanitization is highly recommended as the compute nodes are equipped with 592  sophisticated hardware accelerators.  593  Data Integrity Protection  594  Data encryption is an effective means of providing data integrity. HPC data storage systems 595  typically support uniform encryption at the file level or block level. Such data encryption at the 596  file system level protects data from unauthorized access. However, it does not provide granular 597  access (i.e., segmenting one user from others). Granular access can be achieved using user-level 598  or group-level encryption. Even a system administrator cannot access a user’s data with granular 599  access. Additionally, file systems do not authorize users. Rather, users access the file system via 600  the HPC access zone, which is responsible for authenticating the users and their access rights.  601  Hashing is another technique for protecting data integrity. Data files can be hashed at the 602  beginning to acquire hashing keys. A file is not modified if its hashing key remains the same. 603  Parallel file systems maintain many types of metadata (e.g., user ID, group ID, modification 604  time, sh54 of file, etc.). Hashing metadata is also another way to check whether a file has been 605  modified. 606  Periodically scanning file systems for malware is a proven technique for ensuring data integrity. 607  However, scanning an HPC system for malware is challenging. HPC data storage can easily 608  contain a petabyte or more of data. Existing malware scanning tools are only designed for a 609  single machine or laptop using one thread. They are not efficient or fast enough to scan large 610  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  17  HPC data storage systems. Furthermore, the scanning operation can adversely affect the 611  performance of running jobs. 612  Protecting data integrity is vital to the HPC security. Granular data access provides the best 613  protection and is highly recommended when possible.  614  Securing Containers 615  Containers bundle an application’s code, related libraries, configuration files, and required 616  dependencies to allow the applications to run seamlessly across environments. Containers 617  provide the benefits of portability, reproducibility, and productivity, but they can hide software. 618  In a well-managed HPC system, a lot of software has already been installed as a baseline system 619  environment. The applications developed using native libraries often run faster than a container. 620  Hence, training users to develop programs in the HPC programming environment is one way to 621  reduce exposure to container vulnerabilities. 622  Using containers in an HPC is a balancing act. When containers are supported, restrictions are 623  recommended to mitigate vulnerabilities. This may include prohibiting independent network 624  stacks or user namespaces (i.e., container namespace is always mapped to the host user account). 625  These measures can ensure that the risks posed by containers are no different from other 626  workloads. The threats of a container only affect the applications run by the same user. Other 627  security measures include selecting a container runtime that does not require root access to the 628  system and prohibiting container development in the HPC. 629  Achieving Security While Maintaining HPC Performance  630  HPC security measures often come with an undesirable performance penalty. The following are 631  several effective ways to balance performance and security:  632  • Conduct tests to measure the performance penalty of security tools, which can be 633  benchmarked to determine whether they are acceptable. Testing and measurement would 634  also encourage more performance-aware tool design. 635  • Incorporate security requirements in the initial HPC design rather than as an afterthought. 636  For instance, independent add-on security tools tend to have more impact on performance 637  than native security measures that come with the HPC software stack. 638  • Avoid “one size fits all” security. Differentiate the types of nodes in the HPC system and 639  apply appropriate security rules and controls to different node types. For instance, 640  classify the nodes in HPC into three categories: external nodes, internal nodes, and 641  backend nodes. Apply individual security controls to each node category. Such a 642  differentiation also mitigates performance impacts. 643  Challenges to HPC Security Tools 644  Many industrial security tools are designed with stand-alone devices in mind (e.g., laptops, 645  desktops, or mobile devices). HPC is a large-scale, complex system with strict performance 646  requirements. Security tools that are effective for individual devices may not work well in an 647  HPC environment. For example, a forensic tool that aids the recovery and preservation of a hard 648  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  18  drive and memory for a single server works well in practice. It is unreasonable, however, to 649  install forensic tools on all compute and storage nodes in an HPC system. As another example, 650  HPC nodes may use remotely mounted storage, which may disable some security tools.  651  Moreover, different HPC applications may require different tools. Security tool vendors are often 652  not accustomed to HPC use cases and requirements, which forces HPC security teams to develop 653  analogous tools that may introduce new security vulnerabilities and are sometimes not accepted 654  by organizations. The HPC community needs to work closely with security tool vendors to 655  address these challenges. 656  A Security Technical Implementation Guide (STIG) [41] is a configuration standard and offers a 657  security baseline that reflects security guidance requirements. The security checking tool can 658  measure how well the STIG is satisfied. However, available STIGs are typically written for 659  servers or desktops rather than for HPC. In addition, security baseline checking tools developed 660  for commodity operating systems and applications require customizations to run on HPC. The 661  Lawrence Livermore National Laboratory [42], Sandia National Laboratories [43], and the Los 662  Alamos National Lab [44] have collaborated with the Defense Information Systems Agency 663  (DISA) [45] to develop TOSS 4 STIG [46], which is geared toward HPC systems. Still, a more 664  general STIG library and corresponding security checking tools are desirable to handle diverse 665  subsystems and components inside an HPC.  666  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  19  Conclusions 667  Securing HPC systems is challenging due to their size; performance requirements; diverse and 668  complex hardware, software, and applications; varying security requirements; and the nature of 669  shared resources. The security tools suitable for HPC are inadequate, and current standards and 670  guidelines on HPC security best practices are lacking. The continuous evolution of HPC systems 671  makes the task of securing them even more difficult.  672  This Special Publication aims to standardize and facilitate the information and knowledge-673  sharing of HPC security. A zone based HPC system reference model is introduced to serve as a 674  foundation for a system lexicon and captures common features across the majority of HPC 675  systems. HPC system threat analysis is discussed, security postures and challenges are 676  considered, and recommendations are made.  677  678  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  20  References 679  680  [1]  \"What Is a GPU?,\" [Online]. Available:  https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html.  [2]  R. Sheldon, \"InfiniBand,\" [Online]. Available:  https://www.techtarget.com/searchstorage/definition/InfiniBand.  [3]  \"Omni-Path,\" [Online]. Available: https://en.wikipedia.org/wiki/Omni-  Path#:~:text=Omni%2DPath%20Architecture%20(OPA),this%20architecture%20for%20e xascale%20computing..  [4]  \"HPE Slingshot interconnect,\" [Online]. Available:  https://www.hpe.com/us/en/compute/hpc/slingshot-interconnect.html.  [5]  \"Lustre file system,\" [Online]. Available: https://www.lustre.org/.  [6]  \"Introducing General Parallel File System,\" [Online]. Available:  https://www.ibm.com/docs/en/gpfs/4.1.0.4?topic=guide-introducing-general-parallel-file- system.  [7]  \"IBM Spectrum Scale,\" [Online]. Available: https://www.ibm.com/products/spectrum- scale.  [8]  B. W. S. a. J. B. Lei Cao, \"To share or not to share: comparing burst buffer architectures,\"  in Proceedings of the 25th High Performance Computing Symposium , Virginia Beach,  VA, USA, April 23 - 26, 2017.  [9]  \"Network File System,\" [Online]. Available:  https://en.wikipedia.org/wiki/Network_File_System.  [10]  \"CIFS,\" [Online]. Available: https://cifs.com/.  [11]  \"Server Message Block Overview,\" [Online]. Available: https://learn.microsoft.com/en-  us/previous-versions/windows/it-pro/windows-server-2012-r2-and- 2012/hh831795(v=ws.11).  [12]  \"FUSE,\" [Online]. Available:  https://www.kernel.org/doc/html/latest/filesystems/fuse.html.  [13]  \"SSHFS Homepage,\" [Online]. Available: https://github.com/libfuse/sshfs.  [14]  \"Domain Name Services,\" [Online]. Available: https://www.iana.org/domains.  [15]  \"DHCP,\" [Online]. Available:  https://www.techtarget.com/searchnetworking/definition/DHCP.  [16]  \"LDAP (Lightweight Directory Access Protocol),\" [Online]. Available:  https://www.techtarget.com/searchmobilecomputing/definition/LDAP.  [17]  \"What is NTP?,\" [Online]. Available: http://www.ntp.org/ntpfaq/NTP-s-def.htm.  [18]  \"SLURM Workload Manager,\" [Online]. Available:  https://slurm.schedmd.com/documentation.html.  [19]  \"Kubernetes,\" [Online]. Available: https://kubernetes.io/.  [20]  \"HDF5,\" [Online]. Available: https://www.hdfgroup.org/solutions/hdf5/.  [21]  \"rminer: Data Mining Classification and Regression Methods,\" [Online]. Available:  https://cran.r-project.org/web/packages/rminer/index.html.  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  21  [22]  \"What is a CI/CD pipeline?,\" [Online]. Available:  https://www.redhat.com/en/topics/devops/what-cicd- pipeline#:~:text=A%20continuous%20integration%20and%20continuous,development%2 0life%20cycle%20via%20automation..  [23]  \"CVE,\" [Online]. Available: https://cve.mitre.org/.  [24]  \"Conda,\" [Online]. Available: https://docs.conda.io/en/latest/.  [25]  \"EasyBuild: building software with ease,\" [Online]. Available: https://easybuild.io/.  [26]  \"Spack,\" [Online]. Available: https://spack.io/.  [27]  \"Package Management,\" [Online]. Available: https://ubuntu.com/server/docs/package-  management#:~:text=Apt,upgrading%20the%20entire%20Ubuntu%20system..  [28]  \"Yum,\" [Online]. Available: https://en.wikipedia.org/wiki/Yum_(software).  [29]  \"PXE,\" [Online]. Available:  https://www.techtarget.com/searchnetworking/definition/Preboot-Execution-Environment.  [30]  \"iSCSI (Internet Small Computer System Interface),\" [Online]. Available:  https://www.techtarget.com/searchstorage/definition/iSCSI.  [31]  \"Virtualizing High Performance Computing (HPC) Environments: Dell EMC Reference  Architecture,\" May 2020. [Online]. Available:  https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/solutions/VMwar e-HPC-Virtualized-DTC.pdf.  [32]  HPCWire, \"Hybrid HPC: The time to embrace the cloud is now,\" 2020. [Online].  Available: https://www.hpcwire.com/2020/08/31/hybrid-hpc-the-time-to-embrace-the- cloud-is-now/.  [33]  \"What is SELinux?,\" [Online]. Available: https://www.redhat.com/en/topics/linux/what-is- selinux.  [34]  \"Intelligent Platform Management Interface,\" [Online]. Available:  https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface.  [35]  \"BIOS,\" [Online]. Available: https://en.wikipedia.org/wiki/BIOS.  [36]  \"UEFI,\" [Online]. Available: https://uefi.org/.  [37]  \"MPI Forum,\" [Online]. Available: https://www.mpi-forum.org/.  [38]  \"How to share files with Samba,\" [Online]. Available:  https://www.redhat.com/sysadmin/samba-file-sharing.  [39]  \"Network address translation,\" [Online]. Available:  https://en.wikipedia.org/wiki/Network_address_translation.  [40]  \"Squid-cache.org,\" [Online]. Available: http://www.squid-cache.org/.  [41]  \"Security Technical Implementation Guides (STIGs),\" [Online]. Available:  https://public.cyber.mil/stigs/.  [42]  \"Lawrence Livermore National Laboratory,\" [Online]. Available: https://www.llnl.gov/.  [43]  \"Sandia National Laboratories,\" [Online]. Available: https://www.sandia.gov/.  [44]  \"Los Alamos National Lab,\" [Online]. Available: https://www.lanl.gov/.  [45]  \"Defense Information Systems Agency,\" [Online]. Available: https://disa.mil/.  NIST SP 800-223 ipd (Initial Public Draft)  High-Performance Computing Security  February 2023  22  [46]  \"DISA releases the TOSS 4 Security Technical Implementation Guide,\" [Online].  Available: https://public.cyber.mil/announcement/disa-releases-the-toss-4-security- technical-implementation-guide/.  681  1. Introduction  2. HPC System Reference Model and Main Components  2.1. Main Components  2.1.1. Components of the High-Performance Computing Zone  2.1.2. Components of the Data Storage Zone  2.1.2.1. Parallel File System  2.1.2.2. Archival and Campaign Storage  2.1.2.3. Burst Buffer  2.1.3. Components of the Access Zone  2.1.4. Components of the Management Zone  2.1.4.1. General Architecture and Characteristics  2.1.4.2. Basic Services  2.1.4.3. Configuration Management  2.1.4.4. HPC Scheduler and Workflow Management  2.1.5. HPC Software  2.1.5.1. User Software  2.1.5.2. Site-Provided Software and Vendor Software  2.1.6. Container Usage in HPC  2.2. HPC Architecture Variants  2.2.1. Diskless Booting HPC  2.2.2. Virtual and Hybrid HPC Environments  3. HPC Threat Analysis  3.1. Key HPC Security Characteristics and Use Requirements  3.2. Threats to HPC Function Zones  3.2.1. Access Zone Threats  3.2.2. Management Zone Threats  3.2.3. High-Performance Computing Zone Threats  3.2.4. Data Storage Zone Threats  3.3. Other Threats  4. HPC Security Posture, Challenges, and Recommendations  4.1. HPC Access Control via Multiple Physical Networks  4.2. Compute Node Sanitization  4.3. Data Integrity Protection  4.4. Securing Containers  4.5. Achieving Security While Maintaining HPC Performance  4.6. Challenges to HPC Security Tools  5. Conclusions  References",
    "abstract": ""
}