{
    "number": 35,
    "label": 2,
    "source": "NIST",
    "subject": "De-Identifying Government Data Sets (3rd Draft)",
    "document(english)": "NIST SP 800-188 3pd (third public draft), De-Identifying Government Data Sets  1  2  3  4  5  6  7  8  9  10  11  12  13  NIST Special Publication  NIST SP 800-188 3pd  De-Identifying Government Data Sets  Third Public Draft  Simson Garfnkel  Phyllis Singer  Joseph Near  Aref N. Dajani  Barbara Guttman  This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.800-188.3pd  https://doi.org/10.6028/NIST.SP.800-188.3pd https://crossmark.crossref.org/dialog/?doi=10.6028/NIST.SP.800-188.3pd  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  NIST Special Publication  NIST SP 800-188 3pd  De-Identifying Government Data Sets  Third Public Draft  Simson Garfnkel  Barbara Guttman  Software Quality Group  Software and Systems Division  Joseph Near  Department of Computer Science  University of Vermont  Aref N. Dajani  Phyllis Singer  Center for Enterprise Dissemination  US Census Bureau  This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.800-188.3pd  November 2022  US Department of Commerce  Gina M. Raimondo, Secretary  National Institute of Standards and Technology  Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology  https://doi.org/10.6028/NIST.SP.800-188.3pd  40  45  50  55  60  65  70  75  80  38  39  41  42  43  44  46  47  48  49  51  52  53  54  56  57  58  59  61  62  63  64  66  67  68  69  71  72  73  74  76  77  78  79  81  82  Certain commercial entities, equipment, or materials may be identifed in this document in order to describe  an experimental procedure or concept adequately. Such identifcation is not intended to imply  recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to  imply that the entities, materials, or equipment are necessarily the best available for the purpose.  There may be references in this publication to other publications currently under development by NIST in  accordance with its assigned statutory responsibilities. The information in this publication, including  concepts and methodologies, may be used by federal agencies even before the completion of such  companion publications. Thus, until each publication is completed, current requirements, guidelines, and  procedures, where they exist, remain operative. For planning and transition purposes, federal agencies may  wish to closely follow the development of these new publications by NIST.  Organizations are encouraged to review all draft publications during public comment periods and provide  feedback to NIST. Many NIST cybersecurity publications, other than the ones noted above, are available at  https://csrc.nist.gov/publications.  This document is presented with the hope that its content may be of interest to the general privacy  community. The views in this document are those of the authors, and do not represent those of the US  Census Bureau.  Authority  This publication has been developed by NIST in accordance with its statutory responsibilities under the  Federal Information Security Modernization Act (FISMA) of 2014, 44 U.S.C. § 3551 et seq., Public Law  (P.L.) 113-283. NIST is responsible for developing information security standards and guidelines, including  minimum requirements for federal information systems, but such standards and guidelines shall not apply to  national security systems without the express approval of appropriate federal offcials exercising policy  authority over such systems. This guideline is consistent with the requirements of the Offce of Management  and Budget (OMB) Circular A-130.  Nothing in this publication should be taken to contradict the standards and guidelines made mandatory and  binding on federal agencies by the Secretary of Commerce under statutory authority. Nor should these  guidelines be interpreted as altering or superseding the existing authorities of the Secretary of Commerce,  Director of the OMB, or any other federal offcial. This publication may be used by nongovernmental  organizations on a voluntary basis and is not subject to copyright in the United States. Attribution would,  however, be appreciated by NIST.  NIST Technical Series Policies  Copyright, Fair Use, and Licensing Statements  NIST Technical Series Publication Identifer Syntax  Publication History  Approved by the NIST Editorial Review Board on YYYY-MM-DD [will be added upon fnal publication]  How to cite this NIST Technical Series Publication:  Garfnkel S, Guttman B, Near J, Dajani AN, Singer P (2022) De-Identifying Government Data  Sets. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Special Publication (SP)  NIST SP 800-188 3pd. https://doi.org/10.6028/NIST.SP.800-188.3pd  Author ORCID iDs  Simson Garfnkel: 0000-0003-1294-2831  Joseph Near: 0000-0002-3203-3742  Aref N. Dajani: 0000-0003-0361-5409  Phyllis Singer: 0000-0002-8885-7273  Public Comment Period  https://doi.org/10.6028/NIST.SP.800-188.3pd  83  84  85  86  87  88  89  November 15, 2022 – January 15, 2023  Submit Comments  sp800-188-draft@nist.gov  National Institute of Standards and Technology  Attn: Software and Systems Division, Information Technology Laboratory  100 Bureau Drive (Mail Stop 8970) Gaithersburg, MD 20899-8970  All comments are subject to release under the Freedom of Information Act (FOIA).  NIST SP 800-188 3pd  November 2022  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  Abstract  De-identifcation is a process that is applied to a dataset with the goal of preventing or  limiting informational risks to individuals, protected groups, and establishments while still  allowing for meaningful statistical analysis. Government agencies can use de-identifcation  to reduce the privacy risk associated with collecting, processing, archiving, distributing,  or publishing government data. Previously, NISTIR 8053, De-Identifcation of Personal  Information [51], provided a survey of de-identifcation and re-identifcation techniques.  This document provides specifc guidance to government agencies that wish to use de- identifcation. Before using de-identifcation, agencies should evaluate their goals for us- ing de-identifcation and the potential risks that de-identifcation might create. Agencies  should decide upon a de-identifcation release model, such as publishing de-identifed data,  publishing synthetic data based on identifed data, or providing a query interface that incor- porates de-identifcation. Agencies can create a Disclosure Review Board to oversee the  process of de-identifcation. They can also adopt a de-identifcation standard with measur- able performance levels and perform re-identifcation studies to gauge the risk associated  with de-identifcation. Several specifc techniques for de-identifcation are available, in- cluding de-identifcation by removing identifers and transforming quasi-identifers and the  use of formal privacy models. People performing de-identifcation generally use special- purpose software tools to perform the data manipulation and calculate the likely risk of  re-identifcation. However, not all tools that merely mask personal information provide  suffcient functionality for performing de-identifcation. This document also includes an  extensive list of references, a glossary, and a list of specifc de-identifcation tools, which is  only included to convey the range of tools currently available and is not intended to imply  a recommendation or endorsement by NIST.  Keywords  data life cycle; de-identifcation; differential privacy; direct identifers; Disclosure Re- view Board; the fve safes; k-anonymity; privacy; pseudonymization; quasi-identifers;  re-identifcation; synthetic data.  Reports on Computer Systems Technology  The Information Technology Laboratory (ITL) at the National Institute of Standards and  Technology (NIST) promotes the U.S. economy and public welfare by providing technical  leadership for the Nation’s measurement and standards infrastructure. ITL develops tests,  test methods, reference data, proof of concept implementations, and technical analyses to  advance the development and productive use of information technology. ITL’s responsi- bilities include the development of management, administrative, technical, and physical  standards and guidelines for the cost-effective security and privacy of other than national  security-related information in federal information systems. The Special Publication 800-  i  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  series reports on ITL’s research, guidelines, and outreach efforts in information system  security, and its collaborative activities with industry, government, and academic organiza- tions.  Call for Patent Claims  This public review includes a call for information on essential patent claims (claims whose  use would be required for compliance with the guidance or requirements in this Information  Technology Laboratory (ITL) draft publication). Such guidance and/or requirements may  be directly stated in this ITL Publication or by reference to another publication. This call  also includes disclosure, where known, of the existence of pending U.S. or foreign patent  applications relating to this ITL draft publication and of any relevant unexpired U.S. or  foreign patents.  ITL may require from the patent holder, or a party authorized to make assurances on its  behalf, in written or electronic form, either:  1. assurance in the form of a general disclaimer to the effect that such party does not  hold and does not currently intend holding any essential patent claim(s); or  2. assurance that a license to such essential patent claim(s) will be made available to ap- plicants desiring to utilize the license for the purpose of complying with the guidance  or requirements in this ITL draft publication either:  (a) under reasonable terms and conditions that are demonstrably free of any unfair  discrimination; or  (b) without compensation and under reasonable terms and conditions that are demon- strably free of any unfair discrimination.  Such assurance shall indicate that the patent holder (or third party authorized to make assur- ances on its behalf) will include in any documents transferring ownership of patents subject  to the assurance, provisions suffcient to ensure that the commitments in the assurance are  binding on the transferee, and that the transferee will similarly include appropriate provi- sions in the event of future transfers with the goal of binding each successor-in-interest.  The assurance shall also indicate that it is intended to be binding on successors-in-interest  regardless of whether such provisions are included in the relevant transfer documents.  Such statements should be addressed to: sp800-188-draft@nist.gov  ii  157  158  NIST SP 800-188 3pd  November 2022  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  Table of Contents  Executive Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1  1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3  1.1. Document Purpose and Scope . . . . . . . . . . . . . . . . . . . . . . . . . . 7  1.2. Intended Audience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7  1.3. Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7  2. Introducing De-Identifcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8  2.1. Historical Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8  2.2. Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10  3. Governance and Management of Data De-Identifcation . . . . . . . . . . . . . . 17  3.1. Identifying Goals and Intended Uses of De-Identifcation . . . . . . . . . . . 17  3.2. Evaluating Risks that Arise from De-Identifed Data Releases . . . . . . . . 18  3.2.1. Probability of Re-Identifcation . . . . . . . . . . . . . . . . . . . . . 19  3.2.2. Adverse Impacts of Re-Identifcation . . . . . . . . . . . . . . . . . . 22  3.2.3. Impacts Other Than Re-Identifcation . . . . . . . . . . . . . . . . . 23  3.2.4. Remediation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24  3.3. Data Life Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24  3.4. Data-Sharing Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29  3.5. The Five Safes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30  3.6. Disclosure Review Boards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31  3.7. De-Identifcation Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36  3.7.1. Benefts of Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . 36  3.7.2. Prescriptive De-Identifcation Standards . . . . . . . . . . . . . . . . 36  3.7.3. Performance-Based De-Identifcation Standards . . . . . . . . . . . 37  3.8. Education, Training, and Research . . . . . . . . . . . . . . . . . . . . . . . . 38  3.9. Defense in Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38  3.9.1. Encryption and Access Control . . . . . . . . . . . . . . . . . . . . . 38  3.9.2. Secure Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . 38  3.9.3. Trusted Execution Environments . . . . . . . . . . . . . . . . . . . . 39  3.9.4. Physical Enclaves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39  4. Technical Steps for Data De-Identifcation . . . . . . . . . . . . . . . . . . . . . . 40  iii  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  4.1. Determine the Privacy, Data Usability, and Access Objectives . . . . . . . . 40  4.2. Conducting a Data Survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41  4.3. De-Identifcation by Removing Identifers and Transforming Quasi-Identifers 43  4.3.1.  4.3.2.  4.3.3.  4.3.4.  4.3.5.  4.3.6.  4.3.7.  4.3.8.  4.3.9.  Removing or Transforming of Direct Identifers . . . . . . . . . . . . 44  Special Security Note Regarding the Encryption or Hashing of Di- rect Identifers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46  De-Identifying Numeric Quasi-Identifers . . . . . . . . . . . . . . . . 46  De-Identifying Dates . . . . . . . . . . . . . . . . . . . . . . . . . . . 48  De-Identifying Geographical Locations and Geolocation Data . . . 49  De-Identifying Genomic Information . . . . . . . . . . . . . . . . . . 49  De-Identifying Text Narratives and Qualitative Information . . . . . 51  Challenges Posed by Aggregation Techniques . . . . . . . . . . . . . 51  Challenges Posed by High-Dimensional Data . . . . . . . . . . . . . 52  4.3.10. Challenges Posed by Linked Data . . . . . . . . . . . . . . . . . . . . 52  4.3.11. Challenges Posed by Composition . . . . . . . . . . . . . . . . . . . 52  4.3.12. Potential Failures of De-Identifcation . . . . . . . . . . . . . . . . . 53  4.3.13. Post-Release Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . 54  4.4. Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54  4.4.1.  4.4.2.  4.4.3.  4.4.4.  4.4.5.  4.4.6.  Partially Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . 55  Test Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56  Fully Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 56  Synthetic Data with Validation . . . . . . . . . . . . . . . . . . . . . 58  Synthetic Data and Open Data Policy . . . . . . . . . . . . . . . . 58  Creating a Synthetic Dataset with Diferential Privacy . . . . . . . 58  4.5. De-Identifying with an Interactive Query Interface . . . . . . . . . . . . . . 59  4.6. Validating a De-Identifed Dataset . . . . . . . . . . . . . . . . . . . . . . . . 60  4.6.1. Validating Data Usefulness . . . . . . . . . . . . . . . . . . . . . . . 60  4.6.2. Validating Privacy Protection . . . . . . . . . . . . . . . . . . . . . . 60  4.6.3. Re-Identifcation Studies . . . . . . . . . . . . . . . . . . . . . . . . . 61  5. Software Requirements, Evaluation, and Validation . . . . . . . . . . . . . . . . . 63  5.1. Evaluating Privacy-Preserving Techniques . . . . . . . . . . . . . . . . . . . 63  5.2. De-Identifcation Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64  iv  222  223  224  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  5.2.1. De-Identifcation Tool Features . . . . . . . . . . . . . . . . . . . . . 64  5.2.2. Data Provenance and File Formats . . . . . . . . . . . . . . . . . . . 64  5.2.3. Data Masking Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . 64  5.3. Evaluating De-Identifcation Software . . . . . . . . . . . . . . . . . . . . . . 65  5.4. Evaluating Data Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65  6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66  References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80  Appendix A. Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81  A.1. NIST Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81  A.2. Other U.S. Government Publications . . . . . . . . . . . . . . . . . . . . . . 82  Selected Publications by Other Governments . . . . . . . . . . . . . . . . . . . . . 83  Reports and Books . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83  How-To Articles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84  Appendix B. List of Symbols, Abbreviations, and Acronyms . . . . . . . . . . . . . . 86  Appendix C. Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89  List of Tables  Table 1. Reading levels at a hypothetical school, as measured by entrance exami- nations, reported at the start of the school year on October 1. . . . . . . . 51  Table 2. Reading levels at a hypothetical school, as measured by entrance exami- nations, reported one month into the school year on November 1 after a  new student has transferred to the school. . . . . . . . . . . . . . . . . . . . 51  Table 3. Adjectives used for describing data in data releases. . . . . . . . . . . . . . 55  List of Figures  Fig. 1. The data life cycle as described by Michener et al. [80] . . . . . . . . . . . 25  Fig. 2. Chisholm’s view of the data life cycle is a linear process with a branching  point after data usage [25] . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25  Fig. 3. Altman’s “modern approach to privacy-aware government data releases” [75] 26  Fig. 4. Altman’s conceptual diagram of the relationship between post-transformation  identifability, level of expected harm, and suitability of selected privacy  controls for a data release [75] . . . . . . . . . . . . . . . . . . . . . . . . . . 27  Fig. 5. Advice for Practitioners: A Summary . . . . . . . . . . . . . . . . . . . . . . 68  v  NIST SP 800-188 3pd  November 2022  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  Acknowledgments  The authors wish to thank the U.S. Census Bureau for its help in researching and preparing this  publication, with specifc thanks to John Abowd, Ron Jarmin, Christa Jones, and Laura McKenna.  The authors would also like to thank Luk Arbuckle, Andrew Baker, Daniel Barth-Jones, Christi  Dant, Khaled El Emam, Robert Gellman, Tom Krenzke, Bradley Malin, Kevin Mangold, John  Moehrke, Linda Sanchez, Denise Sturdy, and Chris Traver for providing comments on previous  drafts and their valuable insights, all of which were helpful in creating this publication.  The authors also wish to thank several organizations that provided useful comments on previous  drafts of this publication: the Defense Contract Management Agency (DCMA) Information As- surance Directorate; the Offce of Chief Privacy Offcer within the U.S. Department of Education;  the Offce of Planning, Research, and Evaluation (OPRE) within the Administration for Children  and Families at the U.S. Department of Health and Human Services; the Millennium Challenge  Corporation (MCC) Department of Policy and Evaluation; Integrating the Healthcare Enterprise  (IHE), an ANSI-accredited standards organization focused on healthcare standards; and the Privacy  Tools project at Harvard University (including Micah Altman, Stephen Chong, Kobbi Nissim, David  O’Brien, Salil Vadhan, and Alexandra Wood).  Author Contributions  Simson Garfnkel: Conceptualization, Supervision, Writing (original draft preparation); Joseph  Near: Writing (original draft preparation); Aref N. Dajani: Writing (original draft preparation of  the section on reidentifcation studies); Phyllis Singer: Writing (original draft preparation of the  section on reidentifcation studies).  vi  NIST SP 800-188 3pd  November 2022  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304  305  306  307  308  309  310  311  312  Executive Summary  Every federal agency creates and maintains internal datasets that are vital for fulflling its  mission. The Foundation for Evidence-based Policymaking Act of 2018 [2] mandates that  agencies also collect and publish their government data in open, machine-readable formats,  when it is appropriate to do so. Agencies can use de-identifcation to make government  datasets available while protecting the privacy of the individuals whose data are contained  within those datasets.  Many Government documents use the phrase personally identifable information (PII) to  describe private information that can be linked to an individual [62, 79], although there are  a variety of defnitions for PII. As a result, it is possible to have information that singles  out individuals but that does not meet a specifc defnition of PII. This document therefore  presents ways of removing or altering information that can identify individuals that go  beyond merely removing PII.  For decades, de-identifcation based on simply removing of identifying information was  thought to be suffcient to prevent the re-identifcation of individuals in large datasets. Since  the mid 1990s, a growing body of research has demonstrated the reverse, resulting in new  privacy attacks capable of re-identifying individuals in “de-identifed” data releases. For  several years the goals of such attacks appeared to be the embarrassment of the publishing  agency and achieving academic distinction for the privacy researcher [50]. More recently,  as high-resolution de-identifed geolocation data has become commercially available, re- identifcation techniques have been used by journalists and activists [100, 140, 70] with the  goal of learning confdential information.  These attacks have become more sophisticated in recent years with the availability of ge- olocation data, highlighting the defciencies in traditional  Formal models of privacy, like k-anonymity [122] and differential privacy, [39] use math- ematically rigorous approaches that are designed to allow for the controlled use of conf- dential data while minimizing the privacy loss suffered by the data subjects. Because there  is an inherent trade-off between the accuracy of published data and the amount of privacy  protection afforded to data subjects, most formal methods have some kind of parameter  that can be adjusted to control the “privacy cost” of a particular data release. Informally, a  data release with a low privacy cost causes little additional privacy risk to the participants,  while a higher privacy cost results in more privacy risk. When they are available, formal  privacy methods shoudl be preferred over informal, ad hoc methods.  Decisions and practices regarding the de-identifcation and release of government data can  be integral to the mission and proper functioning of a government agency. As such, an  agency’s leadership should manage these activities in a way that assures performance and  results in a manner that is consistent with the agency’s mission and legal authority. One way  that agencies can manage this risk is by creating a formal Disclosure Review Board (DRB)  that consists of legal and technical privacy experts, stakeholders within the organization,  1  NIST SP 800-188 3pd  November 2022  313  314  315  316  317  318  319  320  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  and representatives of the organization’s leadership. The DRB evaluated applications for  data release that describe the confdential data, the techniques that will be used to mini- mize the risk of disclosure, the resulting protected data, and how the effectiveness of those  techniques will be evaluated.  Establishing a DRB may seem like an expensive and complicated administrative under- taking for some agencies. However, a properly constituted DRB and the development of  consistent procedures regarding data release should enable agencies to lower the risks as- sociated with each data release, which is likely to save agency resources in the long term.  Agencies can create or adopt standards to guide those performing de-identifcation, and  regarding regarding the accuracy of de-identifed data. If accuracy goals exist, then tech- niques such as differential privacy can be used to make the data suffciently accurate for the  intended purpose but not unnecessarily more accurate, which can limit the amount of pri- vacy loss. However, agencies must carefully choose and implement accuracy requirements.  If data accuracy and privacy goals cannot be well-maintained, then releases of data that are  not suffciently accurate can result in incorrect scientifc conclusions and policy decisions.  Agencies should consider performing de-identifcation with trained individuals using soft- ware specifcally designed for the purpose. While it is possible to perform de-identifcation  with off-the-shelf software like a commercial spreadsheet or fnancial planning program,  such programs typically lack the key functions required for proper de-identifcation. As a  result, they may encourage the use of simplistic de-identifcation methods, such as deleting  sensitive columns and manually searching and removing data that appears sensitive. This  may result in a dataset that appears de-identifed but that still contain signifcant disclosure  risks.  Finally, different countries have different standards and policies regarding the defnition and  use of de-identifed data. Information that is regarded as de-identifed in one jurisdiction  may be regarded as being identifable in another.  2  NIST SP 800-188 3pd  November 2022  339  340  341  342  343  344  345  346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368  369  370  371  372  373  374  375  376  1. Introduction  The U.S. Government collects, maintains, and uses many kinds of datasets. Every federal  agency creates and maintains internal datasets that are vital for fulflling its mission, such  as delivering services to taxpayers or ensuring regulatory compliance. There are also 13  principal federal statistical agencies, three recognized statistical units, and over 100 other  federal statistical programs that collect, compile, process, analyze, and distribute informa- tion for statistical purposes [126, 92].  Government programs collect information from individuals and organizations for taxation,  public benefts, public health, licensing, employment, censuses, and the production of of- fcial statistics. While privacy is integral, many individuals and organizations that provide  information to the Government do not typically have the right to opt-out of such requests.  For example, people and establishments in the United States are required by law to respond  to mandatory U.S. Census Bureau surveys.  Agencies make many of their datasets available to the public. The U.S. Government  publishes data to promote commerce, scientifc research, and public transparency. Many  datasets contain some data elements that should not be made public, and it is necessary to  remove such information before making the rest of the dataset available. Some datasets  are so sensitive that they cannot be made publicly available at all but can be available on a  limited basis to qualifed, vetted researchers in protected enclaves. In some cases, agencies  may also elect to release summary statistics of sensitive data or create synthetic datasets  that resemble the original data but that have a lower disclosure risk [8].  There is frequent tension between the goals of privacy protection and the release of useful  data to the public. One way that the Government attempts to resolve this tension is with  an offcial promise of confdentiality to individuals and organizations regarding the infor- mation that they provide [102]. A bedrock principle of offcial statistical programs is that  data provided to the Government should generally remain confdential and not be used in a  way that could harm the individual or the organization providing the data. One justifcation  for this principle is that it helps to ensure high data accuracy. If data providers did not feel  that the information they provide would remain confdential, they might not be willing to  provide information that is accurate.  Other information is created by the Government as a consequence of providing government  services. This information – sometimes called administrative data – is also increasingly  being used and made available for statistical purposes and must be protected.  In 2018, the U.S. Congress passed three laws that signifcantly increased the need for ex- pertise regarding privacy-preserving data analysis and data publishing techniques, such as  de-identifcation:  1. The Foundations for Evidence-Based Policymaking Act of 2018 [2], commonly  called the Evidence Act, requires federal agencies to track all of their data in data  3  NIST SP 800-188 3pd  November 2022  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  inventories, report public datasets to https://data.gov, perform systematic evidence- making and evaluation activities, and engage in capacity-building so that the federal  workforce can meet the requirements of data-centric, evidence-based operations. The  Evidence Act is based on the fndings of the U.S. Commission on Evidence-Based  Policymaking [27] and is implemented in part by OMB Memorandum M-19-23  [139].  The Evidence Act contains specifc guidance requiring that agencies publishing data  take into account “(A) risks and restrictions related to the disclosure of personally  identifable information, including the risk that an individual data asset in isolation  does not pose a privacy or confdentiality risk but when combined with other available  information may pose such a risk;” and “(B) security considerations, including the  risk that information in an individual data asset in isolation does not pose a security  risk but when combined with other available information may pose such a risk” [2].  2. The Open Government Data Act, which was passed as part of the Evidence Act,  requires that the U.S. Government publish data in machine-readable, open, non- proprietary formats when possible. This act largely codifed presidential Executive  Order 13642 of May 9, 2013, “Making Open and Machine Readable the New Default  for Government Information” [88] and its implementation in OMB Memorandum M- 13-13 [18].  3. The Geospatial Data Act of 2018, which requires that government agencies make  inventories of their geospatial data and that public geospatial data be registered on  the U.S. Government’s public geospatial platform, https://www.geoplatform.gov/.  Other laws, regulations, and policies that govern the release of statistics and data to the  public enshrine this principle of confdentiality. For example:  • The Confdential Information Protection and Statistical Effciency Act of 2002  states, “data or information acquired by an agency under a pledge of confdentiality  for exclusively statistical purposes shall not be disclosed by an agency in identif- able form for any use other than an exclusively statistical purpose, except with the  informed consent of the respondent.” [126, §512 (b)(1)] Commonly called CIPSEA,  the act further requires that federal statistical agencies “establish appropriate admin- istrative, technical, and physical safeguards to ensure the security and confdentiality  of records and to protect against any anticipated threats or hazards to their security  or integrity which could result in substantial harm, embarrassment, inconvenience,  or unfairness to any individual on whom information is maintained.”  • US Code Title 13, Section 9 governs the confdentiality of information provided to  the Census Bureau and prohibits “any publication whereby the data furnished by any  particular establishment or individual under this title can be identifed” [130].  • US Code Title 26, Section 6103 governs the confdentiality of information provided  to the U.S. Government on tax returns and other return information. These rules are  4  https://data.gov https://www.geoplatform.gov/  NIST SP 800-188 3pd  November 2022  416  417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448  449  450  now spelled out in IRS Publication 1075, “Tax Information Security Guidelines for  Federal, State and Local Agencies,” published by the IRS Offce of Safeguards [93].  • The Privacy Act of 1974 covers the release of personal information of U.S. citizens  and Lawful Permanent Residents by the Government. The Act recognizes that the  disclosure of records for statistical purposes is acceptable if the data are not “indi- vidually identifable” [103, at a(b)(5)].  Minimizing privacy risk is not an absolute goal of federal laws and regulations. Guidance  from the U.S. Department of Health and Human Services (HHS) on the Health Insurance  Portability and Accountability Act (HIPAA) de-identifcation standards notes that ”[b]oth  methods [the safe harbor and expert determination methods for de-identifcation], even  when properly applied, yield de-identifed data that retains some risk of identifcation. Al- though the risk is very small, it is not zero, and there is a possibility that de-identifed data  could be linked back to the identity of the patient to which it corresponds” [136].  U.S. law also balances privacy risk with other factors, such as transparency, accountabil- ity, and the opportunity for public good. An example of this balance is the handling of  personally identifable information collected by the Census Bureau as part of the decennial  census: this information remains confdential for 72 years and is then transferred to the  National Archives and Records Administration where it is released to the public [131, 5].  De-identifcation is a process that is applied to a dataset with the goal of preventing or lim- iting privacy risks to individuals, protected groups, and establishments while still allowing  for the production of aggregate statistics.1 De-identifcation is not a single technique, but  a collection of approaches, algorithms, and tools that can be applied to different kinds of  data with differing levels of effectiveness. In general, the potential risk to privacy posed by  a dataset’s release decreases as more aggressive de-identifcation techniques are employed,  but data accuracy and – in some cases – the ultimate utility of the de-identifed dataset  decreases as well.  Accuracy is traditionally defned as the “closeness of computations or estimates to the exact  or true values that the statistics were intended to measure” [9]. The data accuracy of  de-identifed data, therefore, refers to the degree to which inferences drawn on the de- identifed data will be consistent with inferences drawn on the original data. Data accuracy  can be measured by the ratio of a value computed with de-identifed data to the same value  computed using the underlying true confdential value.  In economics, Utility is traditionally defned as “the satisfaction derived from consumption  of a good or service”[138]. Data utility therefore refers to the value that data users can de- rive from data in general. When speaking of de-identifed data, utility comes from two pub-  1In Europe, the term data anonymization is frequently used as a synonym for de-identifcation, but the terms  may have subtly different defnitions in some contexts. For a more complete discussion of de-identifcation  and data anonymization, see NISTIR 8053, De-Identifcation of Personal Data [51].  5  NIST SP 800-188 3pd  November 2022  451  452  453  454  455  456  457  458  459  460  461  462  463  464  465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480  481  482  483  484  485  486  lic goods: the uses of the data and the privacy protection afforded by the de-identifcation  process.  This document uses the phrase data accuracy to refer to the abstract characteristic of the  data as determined by a specifc, measurable statistic, whereas data utility refers to the ben- eft derived from the application of the data to a specifc use. Although there has previously  been a tendency within offcial statistical organizations to confate these two terms, it is im- portant to keep them distinct because they are not necessary correlated. Data may have low  accuracy because they contain errors or substantial noise, yet users may nevertheless derive  high value from the data, giving the data high utility. Likewise, data that are very close to  the reality of the thing being measured may have high accuracy but may be fundamentally  worthless and, thus, have low utility.  In general, data accuracy decreases as more aggressive de-identifcation techniques are  employed. Therefore, any effort that involves the release of data that contain personal  information typically involves making a trade-off between identifability and data accuracy.  However, increased privacy protections do not necessarily result in decreased data utility.  Some users of de-identifed data may be able to use the data to make inferences about  private facts regarding the data subjects. They may even be able to re-identify the data  subjects. Both of these uses undo the privacy goals of de-identifcation. Agencies that  release data should understand what data they are releasing, what other data may already  be publicly or privately available, and the risk of re-identifcation. Agencies should aim to  make an informed decision about the fdelity of the data that they release by systematically  evaluating the risks and benefts and choosing de-identifcation techniques and data sharing  models that are tailored to their requirements. In addition, when telling individuals that  their de-identifed information will be released, agencies should disclose that privacy risks  may remain despite de-identifcation.  Planning is essential for successful de-identifcation and data release. In a research envi- ronment, this planning should include the research design, data collection, protection of  identifers, disclosure analysis, and data-sharing strategy. In an operational environment,  this planning includes a comprehensive analysis of the purpose of the data release and the  expected use of the released data, the privacy-related risks, and the privacy protecting con- trols. Both cases should review the appropriateness of various privacy controls given the  risks, intended uses, and the ways that those controls could fail.  De-identifcation can have signifcant costs, including time, labor, and data processing  costs. However, when properly executed, this effort can result in data that have high value  for a research community and the general public while still adequately protecting individual  privacy.  6  NIST SP 800-188 3pd  November 2022  487  488  489  490  491  492  493  494  495  496  497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512  513  514  515  516  517  518  1.1. Document Purpose and Scope  This document provides guidance on the selection, use, and evaluation of de-identifcation  techniques for U.S. Government datasets. It also provides a framework that can be adapted  by federal agencies to shape the governance of de-identifcation processes. The ultimate  goal of this document is to reduce disclosure risks that might result from an intentional data  release.  1.2. Intended Audience  This document is intended for use by government engineers, data scientists, privacy off- cers, disclosure review boards, and other offcials. It is also designed to be generally infor- mative to researchers and academics involved in the technical aspects of the de-identifcation  of government data. While this document assumes a high-level understanding of informa- tion system security technologies, it is intended to be accessible to a wide audience.  1.3. Organization  The remainder of this publication is organized as follows:  • Section 2, “Introducing De-Identifcation,” presents a background on the science  and terminology of de-identifcation.  • Section 3, “Governance and Management of Data De-Identifcation,” provides  guidance to agencies on the establishment of or improvement to a program that makes  privacy-sensitive data available to researchers and the public.  • Section 4, “Technical Steps for Data De-Identifcation,” provides specifc tech- nical guidance for performing de-identifcation using a variety of mathematical ap- proaches.  • Section 5, “Software Requirements, Evaluation, and Validation,” provides a rec- ommended set of features that should be in de-identifcation tools, which may be use- ful for potential purchasers or developers of such software. This section also provides  information for evaluating both de-identifcation tools and de-identifed datasets.  • Section 6, “Conclusion,” Section 6 is the conclusion.  Following the conclusion, this document provides a list of all publications referenced  in this document, as well as an Appendix that includes standards, related NIST pub- lications, other selected publications by the US and other governments, reports and  books, and a few articles of interest. A second appendix provides a list of symbols,  abbreviations and acronyms. The third appendix contains a glossary.  7  NIST SP 800-188 3pd  November 2022  519  520  521  522  523  524  525  526  527  528  529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544  545  546  547  548  549  550  551  552  553  554  2. Introducing De-Identifcation  This document presents recommendations for de-identifying government datasets.  If the information derived from personal data remains in a de-identifed dataset, the dataset  might inadvertently reveal attributes related to specifc individuals, specifc de-identifed  records could be linked back to specifc individuals. When this happens, the privacy pro- tection provided by de-identifcation is compromised. Even if a specifc individual cannot  be matched to a specifc data record, de-identifed data can be used to improve the accu- racy of inferences regarding individuals whose de-identifed data are in the dataset. This  so-called inference risk cannot be eliminated if there is any information in the de-identifed  data, but it can be minimized. Thus, the decision of how or whether to de-identify data  should be made in conjunction with decisions over how the de-identifed data will be used,  shared, or released.  De-identifcation is especially important for government agencies, businesses, and other or- ganizations that seek to make data available to outsiders. For example, signifcant medical  research resulting in societal beneft is made possible by the sharing of de-identifed patient  information under the framework established by the HIPAA Privacy Rule, the primary U.S.  regulation that provides for the privacy of medical records; billing records; enrollment, pay- ment, and claims records; and “other records that are used, in whole or in part, by or for  the covered entity to make decisions about individuals” [90]. The HIPAA Privacy Rule de- identifcation framework applies to both government organizations charged with protecting  government datasets as well as to private sector organizations, such as health plans and  health care providers.  Agencies may also be required to de-identify records when responding to a Freedom of  Information Act (FOIA) [134, 133] request in a manner that is consistent with Exemption  6, which protects information about individuals in “personnel and medical fles and similar  fles” when the disclosure of such information “would constitute a clearly unwarranted in- vasion of personal privacy,” and Exemption 7(C), which is limited to information compiled  for law enforcement purposes and protects personal information when disclosure “could  reasonably be expected to constitute an unwarranted invasion of personal privacy.” The  meaning of these exemptions has been clarifed by multiple cases before the US Supreme  Court [105, 117, 118].  2.1. Historical Context  The modern practice of de-identifcation comes from three overlapping intellectual tradi- tions.  1. For four decades, offcial statistical agencies have researched and investigated meth- ods broadly termed Statistical Disclosure Limitation (SDL) or Statistical Disclosure  8  NIST SP 800-188 3pd  November 2022  555  556  557  558  559  560  561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576  577  578  579  580  581  582  583  584  585  586  587  588  589  590  Control [29, 36].2 Statistical agencies created these methods so that they could re- lease statistical tables and public use fles (PUF) to allow users to learn information  and perform original research while protecting the privacy of the individuals in the  dataset. SDL is widely used in contemporary statistical reporting.  2. In the 1990s, there was a signifcant increase in the release of microdata fles for  public use in the form of both individual responses from surveys and administrative  records. Initially, these releases merely stripped obviously identifying information,  such as names and social security numbers (what are now called direct identifers).  Following some releases, researchers discovered that it was possible to re-identify  individuals’ data by triangulating with some of the remaining data (now called quasi- identifers or indirect identifers [28]). The research resulted in the invention of the  k-anonymity model for protecting privacy [124, 108, 109, 123] , which is refected  in the Offce of Civil Rights guidance on how to apply de-identifcation in a manner  consistent with the HIPAA Privacy Rule [89]. Today, variants of k-anonymity are  commonly used to allow for the sharing of medical microdata. This intellectual tra- dition is typically called de-identifcation, although this document uses that term to  describe all three intellectual traditions.  3. In the 2000s, research in theoretical computer science and cryptography developed  the theory of differential privacy [40], which is based on a mathematical defnition  of the privacy loss to an individual that results from queries on a database containing  that individual’s personal information. Differential privacy is termed a formal model  for privacy protection because its defnitions for privacy and privacy loss are based on  mathematical proofs.3 This does not mean that algorithms that implement differen- tial privacy cannot result in increased privacy risk. Rather, it means that the amount  of privacy risk that results from the use of these algorithms can be mathematically  bounded. These mathematical limits on privacy risk have created considerable inter- est in differential privacy in academia, commerce, and business. To date, however,  only a few systems that utilize differential privacy have been operationally deployed.  uring the frst decade of the 21st century, there was a growing awareness within the U.S.  overnment about the risks that could result from the improper handling and inadvertent  elease of personal identifying and fnancial information. This realization, combined with  growing number of inadvertent data disclosures within the U.S. Government, resulted  n President George Bush signing Executive Order 13402, which established an Identity  heft Task Force on May 10, 2006 [19]. One year later, the Offce of Management and  udget issued Memorandum M-07-16 [62], which required federal agencies to develop  nd implement breach notifcation policies. As part of this effort, NIST issued Special  D G r a i T B a  2A summary of the history of Statistical Disclosure Limitation can be found in Private Lives and Public  Policies: Confdentiality and Accessibility of Government Statistics [102].  3Other formal methods for privacy include cryptographic algorithms and techniques with provably secure  properties, privacy-preserving data mining, Shamir’s secret sharing, and advanced database techniques. A  summary of such techniques appears in [128].  9  NIST SP 800-188 3pd  November 2022  591  592  593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624  625  Publication (SP) 800-122, Guide to Protecting the Confdentiality of Personally Identifable  Information (PII) [79]. These policies and documents had the specifc goal of limiting the  accessibility of information that could be directly used for identity theft but did not create  a framework for processing government datasets so that they could be released without  impacting the privacy of the data subjects.  In 2015, NIST published NISTIR 8053, De-Identifcation of Personal Information [51],  which provided an overview of de-identifcation issues and terminology. It also sum- marized signifcant publications involving de-identifcation and re-identifcation. How- ever, NISTIR 8053 did not make recommendations regarding the appropriateness of de- identifcation or specifc de-identifcation algorithms. The following year, NIST convened  a Government Data De-Identifcation Stakeholder’s Meeting [52].  De-identifcation is one of several models for allowing the controlled sharing of personal  data and other kinds of sensitive data.4 Other models include the use of data processing en- claves, where computations are performed with confdential data using computers that are  physically isolated from the outside world. That isolation might be performed with locked  doors and guards, or it might be performed using silicon and encryption, as is the case  with enclaves implemented on some modern microprocessors. Another approach is to use  mathematical techniques – such as secure multiparty computation – so that computations  can be carried out on confdential data held by multiple parties without ever bringing all of  the confdential data together in a single location.  Techniques for privacy-preserving data-sharing and analysis can be layered to provide  stronger protection than any single technique would provide in isolation. Such comple- mentary models are discussed in Section 3.4. For a more complete description of data- sharing models, privacy-preserving data publishing, and privacy-preserving data mining,  see NISTIR 8053.  Many of the techniques discussed in this publication (e.g., fully synthetic data and differen- tial privacy) have limited use within the Federal Government due to cost, time constraints,  and the sophistication required of practitioners. However, these techniques are likely to  see increased use as agencies seek to make datasets that include identifying information  available.  2.2. Terminology  While each of the de-identifcation traditions has developed its own terminology and math- ematical models, they share many underlying goals and concepts. Where terminology  differs, this document relies on the terminology developed in previous documents by the  U.S. Government and standards organizations.  4For information on characterizing the sensitivity of information, see NIST SP 800 Volume I, Revision  1 [119].  10  NIST SP 800-188 3pd  November 2022  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640  641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656  657  De-identifcation is a process that is applied to a dataset with the goal of preventing or  limiting informational risks to individuals, protected groups, and establishments while still  allowing for the production of aggregate statistics.5 De-identifcation takes an original  dataset and produces de-identifed data.  Re-identifcation is the general term for any process that restores the association between a  set of de-identifed data and the data subject. Re-identifcation is not the only way that de- identifcation techniques can fail to protect privacy. Improperly de-identifed information  can also be used to infer private facts about individuals that were thought to have been  protected.  Re-identifcation risk is the likelihood that a third party can re-identify data subjects in a  de-identifed dataset. Re-identifcation risk is typically a function of the adverse impacts  that would arise if the re-identifcation were to occur and the likelihood of occurrence.  Re-identifcation risk is a specifc form of privacy risk.  Redaction is the removal of information from a document or dataset for legal or security  purposes. Also known as suppression, redaction is a kind of de-identifying technique that  relies on the removal of information. In general, redaction alone is not suffcient to provide  formal privacy guarantees, such as differential privacy. Redaction may also reduce the data  accuracy of the dataset since the use of selective redaction may result in the introduction of  non-ignorable bias.  Anonymization is a “process that removes the association between the identifying dataset  and the data subject” [66]. This term is reserved for de-identifcaiton processes that cannot  be reversed.  Some authors use the terms de-identifcation and anonymization interchangeably. In some  contexts, the term anonymization is used to describe the destruction of a table that maps  pseudonyms to real identifers.6 Both of these uses are potentially misleading, as many  de-identifcation procedures can be readily reversed if a dataset is discovered that maps a  unique attribute or combination of attributes to identities. For example, a medical dataset  may contain a list of names, medical identifers, the rooms where a patient was seen, the  time that the patient was seen, and the results of a medical test. Such a dataset could  be de-identifed by removing the name and medical identifcation numbers. However, the  dataset of medical test results should not be considered anonymized because the tests can  be re-identifed if the dataset is joined with a second dataset of room numbers, times, and  5ISO/TS 25237:2008 defnes de-identifcation as the “general term for any process of removing the asso- ciation between a set of identifying data and the data subject.” [66]. This document intentionally adopts  a broader defnition for de-identifcation that allows for noise-introducing techniques, such as differential  privacy and the creation of synthetic datasets that are based on privacy-preserving models.  6For example, “Anonymization is a step subsequent to de-identifcation that involves destroying all links  between the de-identifed datasets and the original datasets. The key code that was used to generate the new  identifcation code number from the original is irreversibly destroyed (i.e., destroying the link between the  two code numbers)” [127].  11  NIST SP 800-188 3pd  November 2022  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672  673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688  689  690  691  692  names. Since it is not possible to know whether such an auxiliary dataset exists, this publi- cation recommends avoiding the word anonymization and using the word de-identifcation  instead.  Because of the inconsistencies in the use and defnitions of the word “anonymization,” this  document avoids the term except in this section and in the titles of some references. Instead,  it uses the term “de-identifcation” with the understanding that sometimes de-identifed  information can be re-identifed, and sometimes it cannot.7  Pseudonymization is a “particular type of [de-identifcation]8 that both removes the asso- ciation with a data subject and adds an association between a particular set of character- istics relating to the data subject and one or more pseudonyms” [66]. The term coded  is frequently used in healthcare settings to describe data that has been pseudonymized.  Pseudonymization is commonly used so that multiple observations of an individual over  time can be matched and so that an individual can be re-identifed if there is a policy reason  to do so. Although pseudonymous data are typically re-identifed by consulting a key that  may be highly protected, the existence of the pseudonym identifers frequently increases  the risk of re-identifcation through other means.  Many U.S. Government documents use the phrase personally identifable information (PII)  to describe private information that can be linked to an individual [62, 79], although there  are a variety of defnitions for PII in various laws, regulations, and agency guidance docu- ments. Because of these differing defnitions, it is possible to have information that singles  out individuals but that does not meet a specifc defnition of PII. An added complication  is that some documents use the term PII to denote any information that is attributable to  individuals or information that is uniquely attributable to a specifc individual, while others  use the term strictly for data that are directly identifying.  This document avoids the term personally identifable information. Instead, it uses the  phrases personal data or personal information to denote information related to individu- als and identifying information for “information that can be used to distinguish or trace an  individual’s identity, such as their name, social security number, biometric records, etc.,  alone, or when combined with other personal or identifying information which is linked or  linkable to a specifc individual, such as date and place of birth, mother’s maiden name,  etc.” [62]. Under this defnition, identifying information is personal information, but per- sonal information is not necessarily identifying information.  Non-public personal information is used to describe personal information that is in a dataset  that is not publicly available. Non-public personal information is not necessarily identify- ing.  7Thus, where other references (e.g. [104]) might use the term anonymized fle or anonymized dataset to  describe a dataset that has been de-identifed, this publication will use the terms de-identifed fle and de- identifed dataset since the term de-identifed is descriptive while the term anonymized is aspirational.  8Here, the word anonymization in the ISO 25237 defnition is replaced with the more accurate and descriptive  term de-identifcation.  12  NIST SP 800-188 3pd  November 2022  693  694  695  696  697  698  699  700  701  702  703  704  705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720  721  722  723  724  725  726  727  728  729  730  The defnition of identifying information above suggests that it is easy – or at least possible  – to distinguish personal information from identifying information. Indeed, many tech- niques for de-identifcation require an expert to make this distinction and protect only the  identifying information. However, as understanding of privacy risk develops, it is increas- ingly apparent that all information is potentially identifying information.  This document envisions a de-identifcation process in which an original dataset that con- tains personal information is algorithmically processed to produce de-identifed data. The  result may be a de-identifed dataset, aggregate statistics such as summary tables, or a  synthetic dataset, in which the data are created by a model. This kind of de-identifcation  is envisioned as a batch process. Alternatively, the de-identifcation process may be a  system that accepts queries and returns responses that do not leak more identifying infor- mation than is allowable by policy. De-identifed results may be corrected or updated and  re-released on a periodic basis. The accumulated leakage of information from multiple  releases may be signifcant, even if the leakage from a single release is small. Issues that  arise from multiple releases are discussed in Section 3.4, “Data-Sharing Models.”  Disclosure is generally the exposure of data beyond the original collection use case. How- ever, when the goal of de-identifcation is to protect privacy, disclosure  ...relates to inappropriate attribution of information to a data subject, whether  an individual or an organization. Disclosure occurs when a specifc individual  can be associated with a corresponding record(s) in the released dataset with  high probability (identity disclosure), when an attribute described in a dataset is  held by a specifc individual, even if the record(s) associated with that individ- ual is (are) not identifed (attribute disclosure), or when it is possible to make  an inference about an individual, even if the individual was not in the dataset  prior to de-identifcation (inferential disclosure). [47, emphasis in original]  More information about disclosure can be found in Section 3.2.1, “Probability of Re- Identifcation.”  Disclosure limitation is a general term for the practice of allowing summary information  or queries on data within a dataset to be released without revealing information about spe- cifc individuals whose personal information is contained within the dataset. Thus, de- identifcation is a kind of disclosure limitation technique. Every disclosure limitation pro- cess introduces inaccuracy into the results [14, 11].  A primary goal of disclosure limitation is to protect the privacy of individuals while avoid- ing the introduction of non-ignorable biases [7] (e.g., bias that might lead a social scientist  to come to the wrong conclusion) into the de-identifed dataset. One way to measure the  amount of bias that has been introduced by the de-identifcation process is to compare  statistics or models generated by analyzing the original dataset with those that are gener- ated by analyzing the de-identifed datasets. Such biases introduced by the de-identifcation  13  NIST SP 800-188 3pd  November 2022  731  732  733  734  735  736  737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752  753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768  769  770  process are typically unrelated to any statistical biases that may also exist in the original  data.  Formal models of privacy can quantify the amount of privacy protection offered by a de- identifcation process. With methods based on differential privacy, this measurement takes  the form of a number called privacy loss, which quantifes the additional risk that an ad- versary might learn something new about an individual as a result of a de-identifed data  release. When a de-identifcation process is associated with low privacy loss, releasing the  data it produces results in little additional risk for individuals in the input dataset. Some  formal models, such as differential privacy, allow composing the privacy losses of multiple  data releases to quantify the total risk to individuals of the combined releases, while others  – such as k-anonymity – do not have this capability.  An upper bound on the total acceptable privacy loss of many data releases is often called  a privacy loss budget or simply a privacy budget. This number quantifes the total privacy  risk to an individual who participates in all of the releases.  Differential privacy [40] is a model based on a mathematical defnition of privacy that con- siders the risk to an individual from the release of a query on a dataset containing their  personal information. Statisticians, mathematicians, and other kinds of privacy engineers  then develop mathematical algorithms, called mechanisms, that process data in a way that  is consistent with the defnition. Differential privacy limits both identity and attribute dis- closure by adding non-deterministic noise (random values) to the results of mathematical  operations before the results are reported. Unlike k-anonymity and other de-identifcation  frameworks, differential privacy is based on information theory and makes no distinction  between what is private data and what is not. Differential privacy does not require that val- ues be classifed as direct identifers, quasi-identifers, and non-identifying values. Instead,  differential privacy assumes that all values in a record might be identifying and therefore  all must be de-identifed.  Differential privacy’s mathematical defnition requires that the result of an analysis of a  dataset should be roughly the same with or without the data of any single individual. The  defnition is usually satisfed by adding random noise to the result of a query, ensuring  that the added noise masks the contribution of any individual. The degree of sameness  is defned by the parameter ε (epsilon). The smaller the parameter ε , the more noise is  added, and the more diffcult it is to distinguish the contribution of a single individual. The  result is increased privacy for all individuals – both those in the sample and those in the  population from which the sample is drawn who are not present in the dataset. The research  literature describes differential privacy being used to solve a variety of tasks, including  statistical analysis, machine learning, and data sanitization [38]. Differential privacy can  be implemented in an online query system or in a batch mode in which an entire dataset  is de-identifed at one time. In common usage, the phrase “differential privacy” is used  to describe both the formal mathematical framework for evaluating privacy loss and for  algorithms that provably provide those privacy guarantees.  14  NIST SP 800-188 3pd  November 2022  771  772  773  774  775  776  777  778  779  780  781  782  783  784  785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800  801  802  803  804  805  806  807  808  The use of differential privacy algorithms does not guarantee that privacy will be preserved.  Instead, the algorithms guarantee that the amount of privacy risk introduced by data pro- cessing or data release will reside within specifc mathematical bounds. It is also important  to remember that the impact on privacy risk is limited to reducing the risk of identity and  attribute disclosures (see §3.2.1, “Probability of Re-Identifcation”) and not inferential dis- closure.  K-anonymity [108, 123] is a framework for quantifying the amount of manipulation re- quired of the quasi-identifers to achieve a desired level of privacy. The technique is based  on the concept of an equivalence class – the set of records that have the same values on the  quasi-identifers9. A dataset is said to be k-anonymous if there are no fewer than k match- ing records for every specifc combination of quasi-identifers. For example, if a dataset  that has the quasi-identifers (birth year) and (state) has k=4 anonymity, then there must  be at least four records for every combination of (birth year, state). Subsequent work has  refned k-anonymity by adding requirements for diversity of the sensitive attributes within  each equivalence class (known as l-diversity [76]) and requiring that the resulting data be  statistically close to the original data (known as t-closeness [73]).  K-anonymity and its subsequent refnements defne formal privacy models but come with  two important drawbacks. First, they require an expert to determine the set of quasi- identifers by distinguishing between identifying and non-identifying information. As de- scribed earlier, this task can be diffcult or impossible in some contexts. If identifying  information is not marked as a quasi-identifer, then the resulting k-anonymous dataset will  not prevent the re-identifcation of data subjects. Second, k-anonymity and related tech- niques are not compositional – they do not quantify the cumulative privacy loss of multiple  data releases, and multiple releases can result in a catastrophic loss of privacy.  When data releases containing information about the same individual accumulate, then  privacy loss accumulates. This accumulation of privacy loss is not refected in k-anonymity,  nor is it refected in HIPAA privacy rule guidance [136]. Nevertheless, the accumulation  is real. In 2003, Dinur and Nissim discovered that it was possible to reconstruct private  microdata from a query interface even if the results of each query were systematically  infused with small amount of noise [33]. The researchers showed that the amount of  noise added to prevent an accurate reconstruction increases as the amount of queries on the  dataset increase. If a query interface allows for an ulimited number of queries, no amount  of noise is suffcient. Organizations should keep this in mind and try to assess the overall  accumulated risk. The discovery in this paper led directly to the invention of differential  privacy.  Some agencies (notably those that publish data for accountability and enforcement pur- poses) view perturbative Statistical Disclosure Limitation methods (e.g., those that add  noise, such as differential privacy) as being inherently unacceptable, since the noise intro-  9A quasi-identifer is a variable that can be used to identify an individual through association with other  information.  15  NIST SP 800-188 3pd  November 2022  809  810  811  812  813  814  815  816  817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832  833  duced by the methods can void their ability to be used for accountability. For example, if  school would lose funding if the promotion rate for any class fell below a certain thresh- ld, then a method that protects the privacy of students within each class by introducing  oise could mask whether the school did or did not make that target. Thus, despite their  eaknesses and faws, program agencies often prefer to use suppression as the preferred  rotection method for these purposes because the data are either reported as is or sup- ressed, eliminating the uncertainty. Agencies should realize that suppression alone is not  uffcient to protect privacy, and if a large enough number of queries is released based on  he same confdential dataset, it is frequently possible to reconstruct even data that have  een suppressed.  raditional disclosure limitation and k-anonymity start with specifc disclosure limitation  echanisms that were designed to hide information while allowing for useful data analysis  nd attempting to reach the goal of privacy protection. In contrast, differential privacy starts  ith an information-theoretic defnition of privacy and has attempted to evolve mechanisms  hat produce useful (but privacy-preserving) results. These techniques are currently the  ubject of academic research, so it is reasonable to expect new techniques to be developed  n the coming years that simultaneously increase privacy protection while providing for the  igh accuracy of resulting de-identifed data. Indeed, some authors have shown that the  odels can be viewed synergistically [114] under some circumstances.  inally, privacy harms are not the only kinds of harms that can result from the release of  e-identifed data. Analysts working with de-identifed data often have no way of knowing  ow inaccurate their statistical results are due to statistical distortions introduced by the  e-identifcation process. Thus, de-identifcation operations intended to shield individuals  rom harm could result in inaccurate research fndings. Such research might also cause  arm if it is used to support harmful policies.  a o n w p p s t b  T m a w t s i h m  F d h d f h  16  NIST SP 800-188 3pd  November 2022  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848  849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864  865  866  867  868  869  870  3. Governance and Management of Data De-Identifcation  The decisions and practices regarding the de-identifcation and release of government data  can be integral to the mission and proper functioning of a government agency. As such,  these activities should be managed by an agency’s leadership in a way that assures that  performance and results that are consistent with the agency’s mission and legal authority.  As discussed above, the need for attention arises because of the conficting goals of data  transparency and privacy protection. Although many agencies once assumed that it was  relatively straightforward to remove privacy-sensitive data from a dataset so that the re- mainder could be released without restriction, history shows that this is not the case [51,  §2.4, §3.6].  Given this history, there may be a tendency for government agencies to either over-protect  data or to simply avoid its release. Limiting the release of data clearly limits the privacy risk  that might result from a data release. However, limiting the release of data also creates costs  and risks for other government agencies (which will then not have access to the identifed  data), external organizations, and society. For example, absent the data release, external  organizations will suffer the cost of recollecting the data (if it is possible to do so) or the  risk of incorrect decisions that might result from having insuffcient information.  This section begins with a discussion of why agencies might wish to de-identify data and  how agencies should balance the benefts of data release with risks to the data subjects. It  then discusses where de-identifcation fts within the data life cycle. Finally, it discusses  options that agencies have for adopting de-identifcation standards.  3.1. Identifying Goals and Intended Uses of De-Identifcation  Before engaging in de-identifcation, agencies should clearly articulate their goals regard- ing transparency and disclosure limitation in making a data release. They should then  develop a written plan that explains how de-identifcation will be used to accomplish those  goals.  For example:  • Federal Statistical Agencies collect, process, and publish data for use by researchers,  business planners, and other well-established purposes. These agencies are likely to  have established standards and methodologies for de-identifcation. As these agen- cies evaluate new approaches for de-identifcation, they should document their ra- tionale for adopting legacy versus new approaches, evaluate how successful their  approaches have been over time, and address inconsistencies between data releases.  • Federal Awarding Agencies are allowed under OMB Circular A-110 to require that  institutions of higher education, hospitals, and other non-proft organizations that  receive federal grants provide the U.S. Government with “the right to (1) obtain,  reproduce, publish or otherwise use the data frst produced under an award; and  17  NIST SP 800-188 3pd  November 2022  871  872  873  874  875  876  877  878  879  880  881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896  897  898  899  900  901  902  903  904  905  906  907  908  (2) authorize others to receive, reproduce, publish, or otherwise use such data for  Federal Purposes” [91, see §36 (c) (1) and (2)]. To realize this policy, awarding  agencies can require that awardees establish data management plans for making re- search data publicly available. Such data are used for a variety of purposes, including  transparency and reproducibility. In general, research data that contain personal in- formation should be de-identifed by the awardee prior to public release. Awarding  agencies may establish de-identifcation standards to ensure the protection of per- sonal information and may consider audits to assure that awardees have performed  de-identifcation in an appropriate manner.  • Federal Research Agencies may wish to make de-identifed data available to the  public to further the objectives of research transparency and allow others to reproduce  and build upon their results. These agencies are generally prohibited from publishing  research data that contain personal information, requiring the use of de-identifcation.  • All Federal Agencies that wish to make administrative or operational data available  for transparency, accountability, or program oversight or to enable academic research  may wish to employ de-identifcation to avoid sharing sensitive personally identif- able information of employees, customers, or others. These agencies may wish to  evaluate the effectiveness of simple feld suppression, de-identifcation that involves  aggregation, and the creation and release of synthetic data as alternatives for realizing  their commitment to open data.  3.2. Evaluating Risks that Arise from De-Identifed Data Releases  Once the purpose of the data release is understood, agencies should identify the risks that  might result from the data release. As part of this risk analysis, agencies should specifcally  evaluate the anticipated negative actions that might result from re-identifcation, as well as  strategies for remediation. NIST provides detailed information on how to conduct risk  assessments in NIST SP 800-30 [23].  Risk assessments should be based on objective scientifc factors and consider the best inter- ests of the individuals in the dataset, the responsibilities of the agency holding the data, and  the anticipated benefts to society. The goal of a risk evaluation is not to eliminate risk but  to identify which risks can be reduced while still meeting the objectives of the data release  and then deciding whether the residual risk is justifed by the goals of the data release. An  agency decision-making process may choose to accept or reject the risk that might result  from a release of de-identifed data, but participants in the risk assessment should not be  empowered to prevent risk from being documented and discussed. Centralized processes  also allow for standardization of the risk assessment and the amount of “acceptable risk”  across different programs’ releases.  It is diffcult to measure re-identifcation risk in ways that are both general and meaningful.  For example, it is possible to measure the similarity between individuals in the dataset  18  NIST SP 800-188 3pd  November 2022  909  910  911  912  913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928  929  930  931  932  933  934  935  936  937  938  939  940  941  under a variety of different parameters and to model how that similarity is impacted when  the larger population is considered. However, such calculations may result in different  levels of risk for different groups. There may be some individuals in a dataset who would  be signifcantly adversely impacted by re-identifcation and for whom the likelihood of  re-identifcation might be quite high, but these individuals might represent a tiny fraction  of the entire dataset. This represents an important area for research in the feld of risk  communication.  3.2.1. Probability of Re-Identifcation  As discussed in Section 2.2, “Terminology,” the potential impacts on individuals from the  release and use of de-identifed data include [143]:  Identity disclosures: Associating a specifc individual with the corresponding record(s)  in the dataset with high probability. Identity disclosure can result from insuffcient  de-identifcation, re-identifcation by linking, or pseudonym reversal.  Attribute disclosure: Determining that an attribute described in the dataset is held by a  specifc individual with high probability, even if the records associated with that indi- vidual are not identifed. Attribute disclosure can occur without identity disclosure if  the de-identifed dataset contains data from a signifcant number of relatively homo- geneous individuals [51, p.13]. In these cases, traditional de-identifcation does not  protect against attribute disclosure, although differential privacy can. Membership  inference is an example of attribute disclosure.  Inferential disclosure: Being able to make an inference about an individual (typically a  member of a group) with high probability, even if the individual was not in the dataset  prior to de-identifcation. “Inferential disclosure is of less concern in most cases  as inferences are designed to predict aggregate behavior, not individual attributes,  and thus are often poor predictors of individual data values” [60]. Traditional de- identifcation does not protect against inferential disclosure. Such disclosures can  never be eliminated; they can only be controlled.  Re-identifcation probability10 is the estimated probability that an outside party will be able  to use information contained in a de-identifed dataset to make identity-related inferences  about individuals. This outside party was originally termed a data intruder, although the  terms adversary and attacker are also used, borrowing from the colorful language of infor- mation security. Different kinds of re-identifcation probabilities for this data intruder can  be calculated.  10Previous publications described identifcation probability as “re-identifcation risk” and used scenarios such  as a journalist seeking to discredit a national statistics agency or a prosecutor seeking to fnd information  about a suspect as the bases for probability calculations. That terminology is not presented in this document  because of the possible unwanted connotations of those terms and in the interest of bringing the terminology  of de-identifcation into agreement with the terminology used in contemporary risk analysis processes [42].  19  NIST SP 800-188 3pd  November 2022  942  943  944  945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960  961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  Here are several kinds of probabilities, as well as proposals for new, declarative, self- describing names:  Known inclusion re-identifcation probability (KIRP) is the probability of fnding the  record that matches a specifc individual known to be in the sample corresponding to  a specifc record. KIRP can be expressed as the probability for a specifc individual  or the probability averaged over the entire dataset (AKIRP).11  Unknown inclusion re-identifcation probability (UIRP) is the probability of fnding the  record that matches a specifc individual without frst knowing whether the individual  is in the dataset. UIRP can be expressed as a probability for an individual record in  the dataset averaged over the entire population (AUIRP).12  Record matching probability (RMP) is the probability of fnding the record that matches  a specifc individual chosen from the population. RMP can be expressed as the prob- ability for a specifc record (RMP), the probability averaged over the entire dataset  (ARMP), or the maximum probability over the entire dataset.  Inclusion probability (IP) is the probability that a specifc individual’s presence in the  dataset can be inferred.  Whether it is necessary to quantitatively estimate these probabilities depends on the specifcs  of each intended data release. For example, many cities publicly disclose whether taxes  have been paid on a property. Given that this information is already a matter of public  record, it may not be necessary to consider inclusion probability when a dataset of property  taxpayers for a specifc dataset is released. Likewise, there may be some attributes in a  dataset that are already public and may not need to be protected with disclosure limitation  techniques. However, the existence of such attributes may pose a re-identifcation risk for  other information in the dataset or in other de-identifed datasets. The fact that information  is public may not negate the responsibility of an agency to provide protection for that in- formation, as the aggregation and distribution of information may cause privacy risk that  was not otherwise present. Agencies may also be legally prohibited from releasing copies  of information that is similar to information that is already in the public domain.  Although disclosures are commonly thought to be discrete events involving the release of  specifc data, such as an individual’s name matched to a record, disclosures can result from  the release of data that merely changes a data intruder’s probabilistic belief. For example,  a disclosure might change an intruder’s estimate that a specifc individual is present in a  dataset from a 50% probability to 90%. The intruder still does not know if the individual  is in the dataset or not (and the individual might not, in fact, be in the dataset), but a  11Some texts refer to KIRP as “prosecutor risk.” The scenario is that a prosecutor is looking for records that  belong to a specifc, named individual.  12Some texts refer to UIRP as “journalist risk.” The scenario is that a journalist has obtained a de-identifed  fle and is trying to identify one of the data subjects, but the journalist fundamentally does not care who is  identifed.  20  NIST SP 800-188 3pd  November 2022  976  977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992  993  994  995  996  997  998  999  1000  1001  1002  1003  1004  1005  1006  1007  1008  1009  1010  1011  1012  1013  1014  probabilistic disclosure has still occurred because the intruder’s estimate of the individual  has been changed by the data release.  It may be diffcult to estimate specifc re-identifcation probabilities, as the ability to re- identify depends on the original dataset, the de-identifcation technique, the technical skill  of the data intruder, the intruder’s available resources, and the availability of additional  data (publicly available or privately held) that can be linked with the de-identifed data.  It is likely that the true probability of re-identifcation increases over time as techniques  improve and more contextual information becomes available to potential data intruders.  Indeed, some researchers have claimed that computing these probabilities “is a fundamen- tally meaningless exercise” because the calculations are based on assumptions that cannot  be validated (e.g., the lack of a database that could link specifc quasi-identifers or sensi- tive, non-identifying values to identities) [83].  De-identifcation practitioners have traditionally quantifed re-identifcation probability, in  part, based on the skills and abilities of a potential data intruder. Datasets that were thought  to have little possibility for exploitation were deemed to have a lower re-identifcation  probability than datasets containing sensitive or otherwise valuable information. Such ap- proaches are not appropriate when attempting to evaluate the re-identifcation probability  of government datasets that will be publicly released.  • Although a specifc de-identifed dataset may not be recognized as sensitive, re- identifying that dataset may be an important step in re-identifying another dataset  that is sensitive. Alternatively, the data intruder may merely wish to embarrass the  government agency. Thus, adversaries may have a strong incentive to re-identify  datasets that are seemingly innocuous.  • Although the public may not generally be skilled in re-identifcation, many resources  on the internet make it easy to acquire specialized datasets, tools, and experts for  specifc re-identifcation challenges. Family members, friends, colleagues, and oth- ers may also possess substantial personal knowledge about individuals in the data  that can be used for re-identifcation.  Instead, de-identifcation practitioners should assume that de-identifed government datasets  could be subjected to sustained, worldwide re-identifcation attempts, and they should  gauge their de-identifcation requirements accordingly. Of course, it is unrealistic to as- sume that all of the world’s resources will be used to attempt to re-identify every publicly  released fle. Therefore, de-identifcation requirements should be gauged using a risk as- sessment [75]. More information on conducting risk assessments can be found in NIST SP  800-30, Guide for Conducting Risk Assessments [23].  Members of vulnerable populations (e.g., prisoners, children, people with disabilities) may  be more susceptible to having their identities disclosed by de-identifed data than non- vulnerable populations because the thing that makes these individuals vulnerable may also  make them stand out in the dataset. Likewise, residents of areas with small populations  21  NIST SP 800-188 3pd  November 2022  1015  1016  1017  1018  1019  1020  1021  1022  1023  1024  1025  1026  1027  1028  1029  1030  1031  1032  1033  1034  1035  1036  1037  1038  1039  1040  1041  1042  1043  1044  1045  1046  1047  may be more susceptible to having their identities disclosed than residents of urban areas.  Individuals with multiple traits will generally be more identifable if the individual’s loca- tion is geographically restricted. For example, data belonging to a person who is labeled as  a pregnant, unemployed female veteran will be more identifable if restricted to Baltimore  County, Maryland, than to all of North America.  If agencies determine that the potential for harm is large in a contemplated data release, one  way to manage the risk is by increasing the level of de-identifcation and accepting a lower  data accuracy level. Other options include data controls, such as restricting the availability  of data to qualifed researchers in a data enclave.  3.2.2. Adverse Impacts of Re-Identifcation  As part of a risk analysis, agencies should attempt to enumerate specifc kinds of adverse  impacts that can result from the re-identifcation of de-identifed information. These can  include potential impacts on individuals, the agency, and society.  Potential adverse impacts on individuals include:  • Increased availability of personal information that leads to an increased risk of fraud,  identity theft, discrimination, or abuse  • Increased availability of an individual’s location that puts that person at risk for bur- glary, property crime, assault, or other kinds of violence  • Increased availability of an individual’s non-public personal information that causes  psychological harm by exposing potentially embarrassing information or information  that the individual may not otherwise choose to reveal to the public or to family  members and that potential affects opportunities in the economic marketplace (e.g.,  employment, housing, college admission)  Potential adverse impacts on agencies include:  • Mandatory reporting under breach reporting laws, regulations, or policies  • Embarrassment or reputational damage  • Harm to agency operations if some aspect of those operations required that the de- identifed data remain confdential (e.g., an agency that is forced to discontinue a  scientifc experiment because the data release may have biased the study participants)  • Financial impacts that result from the harm to the individuals (e.g., lawsuits)  • Civil or criminal sanctions against employees or contractors that result from a data  release contrary to U.S. law  Potential adverse impacts on society include:  22  NIST SP 800-188 3pd  November 2022  1048  1049  1050  1051  1052  1053  1054  1055  1056  1057  1058  1059  1060  1061  1062  1063  1064  1065  1066  1067  1068  1069  1070  1071  1072  1073  1074  1075  1076  1077  1078  • Undermining the reputation of researchers in general and the willingness of the pub- lic to support/tolerate research and provide accurate information to government agen- cies and researchers  • Engendering a lack of trust in government – individuals may stop consenting to the  use of their data, may stop providing data, or may provide false data  • Damaging the practice of using de-identifed information – de-identifcation is an  important tool for promoting research and accountability, and poorly executed de- identifcation efforts may negatively impact the public’s view of this technique and  limit its use  One way to calculate an upper bound on impact to an individual or the agency is to es- timate the impact that would result from the inadvertent release of the original dataset.  This approach will not calculate the upper bound on the societal impact, however, since  that impact includes reputational damage to the practice of de-identifcation itself. That is,  every time data are compromised because of a poorly executed de-identifcation effort, it  becomes harder to justify the use of de-identifcation in future data releases.  As part of a risk analysis process, organizations should enumerate specifc measures that  they will take to minimize the risk of successful re-identifcation. Organizations may wish  to consider both the actual risk and the perceived risk to those in the dataset and in the  broader community.  As part of the risk assessment, an organization may determine that there is no way to  achieve the de-identifcation goal in terms of data accuracy and identifability. In these  cases, the organization will need to decide whether it should adopt additional measures to  protect privacy (e.g., administrative controls or data use agreements), accept a higher level  of risk, or choose not to proceed with the project.  3.2.3. Impacts Other Than Re-Identifcation  The use of de-identifed data can lead to adverse impacts other than those that might re- sult from re-identifcation. Risk assessments that evaluate the risks of re-identifcation can  address these other risks as well. Such risks might include:  • The risk of excessive inferential disclosures  • The risk that the de-identifcation process might introduce bias or inaccuracies into  the dataset that result in incorrect decisions13  13For example, a personalized warfarin dosing model created with data that had been modifed in a manner  consistent with the differential privacy de-identifcation model produced higher mortality rates in simulation  than a model created from unaltered data [49]. Educational data de-identifed with the k-anonymity model  can also result in the introduction of bias that leads to spurious results [14, 125].  23  NIST SP 800-188 3pd  November 2022  1079  1080  1081  1082  1083  1084  1085  1086  1087  1088  1089  1090  1091  1092  1093  1094  1095  1096  1097  1098  1099  1100  1101  1102  1103  1104  1105  1106  1107  1108  1109  1110  1111  1112  1113  • The risk that releasing a de-identifed dataset might reveal non-public information  about an agency’s policies or practices  It is preferable to use de-identifcation processes that include assessments of accuracy (e.g.,  confdence intervals) with respect to the bias and precision of statistical properties of the  data. Where it does not provide information that may aid data intruders, it is also useful to  reveal the de-identifcation process itself so that analysts can understand any potential in- accuracies that might be introduced by the de-identifcation. This is consistent with Kerck- hoffs’ principle [67], a widely accepted system design principle that holds that the security  of a system should not rely on the secrecy of the methods that it employs.  3.2.4. Remediation  As part of a risk analysis process, agencies should attempt to enumerate techniques that  could be used to mitigate or remediate harm that would result from a successful re-identifcation  of de-identifed information. Remediation could include victim education, the procurement  of monitoring or security services, the issuance of new identifers, or other measures.  3.3. Data Life Cycle  The NIST Big Data Interoperability Framework defnes the data life cycle as “the set of  processes in an application that transform raw data into actionable knowledge” [85]. The  data life cycle can be used in the de-identifcation process to help analyze the expected  benefts, intended uses, privacy threats, and vulnerabilities of de-identifed data. As such,  the data life cycle concept can be used to select appropriate privacy controls based on a  reasoned analysis of the threats. For example, privacy-by-design concepts [22] can be  employed to decrease the number of identifers collected, minimizing requirements for de- identifcation prior to data release. The data life cycle can also be used to design a tiered  access mechanism based on this analysis [12].  Several data life cycles have been proposed, but none are widely accepted as a standard.  Michener et al. [80] (Figure 1) describe the data life cycle as a true cycle:  → Assure → Describe → Deposit → Preserve → Discover → Integrate → Analyze →  Collect  Stobierski [120] also describes the data life cycle as a cycle with different steps:  Generation → Collection → Processing → Storage → Management → Analysis →  Visualization → Interpretation → Generation  De-identifcation does not ft into a circular data life cycle model, as the data owner typ- ically retains access to the identifed data. However, if the organization employs de- identifcation, it could be performed during Collect or between Collect and Assure if iden- tifed data were collected but the identifying information was not actually needed. Alter-  24  NIST SP 800-188 3pd  November 2022  NIST SP 800-188  DE-IDENTIFYING GOVERNMENT DATASETS  30  Figure 1 Michener et al.’s view of the data life cycle is a true cycle, with analysis guiding future collection.  It is unclear how de-identification fits into a circular life cycle model, as the data owner typically  retains access to the identified data. However, if the organization employs de-identification, it  could be performed during the Collect, or between Collect and Assure if identified data were  collected but the identifying information was not actually needed. Alternatively, de-identification  could be applied after Describe and prior to Deposit, to avoid archiving identifying information.  Chisholm and others describe the data life cycle as a linear process that involves Data Capture →  Data Maintenance → Data Synthesis → Data Usage → {Data Publication & Data Archival} →  Data Purging:99  Figure 2 Chisholm's view of the data life cycle is a linear process with a branching point after data usage.  99 Malcolm Chisholm, 7 Phases of a Data Life Cycle, Information Management, July 9, 2015. http://www.information- management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html  Fig. 1. The data life cycle as described by Michener et al. [80]  NIST SP 800-188  DE-IDENTIFYING GOVERNMENT DATASETS  30  Figure 1 Michener et al.’s view of the data life cycle is a true cycle, with analysis guiding future collection.  It is unclear how de-identification fits into a circular life cycle model, as the data owner typically  retains access to the identified data. However, if the organization employs de-identification, it  could be performed during the Collect, or between Collect and Assure if identified data were  collected but the identifying information was not actually needed. Alternatively, de-identification  could be applied after Describe and prior to Deposit, to avoid archiving identifying information.  Chisholm and others describe the data life cycle as a linear process that involves Data Capture →  Data Maintenance → Data Synthesis → Data Usage → {Data Publication & Data Archival} →  Data Purging:99  Figure 2 Chisholm's view of the data life cycle is a linear process with a branching point after data usage.  99 Malcolm Chisholm, 7 Phases of a Data Life Cycle, Information Management, July 9, 2015. http://www.information- management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html  Fig. 2. Chisholm’s view of the data life cycle is a linear process with a branching point after  data usage [25]  1114 natively, de-identifcation could be applied after Describe and prior to Deposit to avoid  1115 archiving identifying information.  1116 Chisholm and others [25] (Figure 2) describe the data life cycle as a linear process with a  1117 fork for data publication:  1118 Data Capture → Data Maintenance → Data Synthesis → Data Usage →  1119 {Data Publication & Data Archival → Data Purging}  1120 Using this formulation, de-identifcation can take place either during Data Capture or fol- 1121 lowing Data Usage. However, agencies should consider data release requirements from  1122 the very beginning of the planning process for each new data collection. By knowing in  1123 advance how they intend to publish and for what purposes and by having a plan for how  1124 disclosure limitation will be applied, agencies can tailor information collection accordingly.  1125 For example, if specifc identifers are not needed for maintenance, synthesis, and usage,  1126 then those identifers should not be collected. If fully identifed data are needed within the  25  NIST SP 800-188 3pd  November 2022  NIST SP 800-188  DE-IDENTIFYING GOVERNMENT DATASETS  32  Figure 3 Life cycle model for government data releases, from Altman et al. Fig. 3. Altman’s “modern approach to privacy-aware government data releases” [75]  1127  1128  1129  1130  1131  1132  1133  1134  1135  1136  organization, the identifying information can be removed prior to the data being published,  shared, or archived. Applying de-identifcation throughout the data life cycle minimizes  privacy risk and signifcantly eases the process of public release. However, agencies should  be cognizant of the potential loss of future utility if identifers are permanently removed.  For this reason, agencies may wish to retain an identifed dataset or data linking informa- tion, as it may be diffcult to predict future needs.  Altman et al. [75] (Figures 3 and 4) propose a “modern approach to privacy-aware gov- ernment data releases” that incorporates progressive levels of de-identifcation as well as  different kinds of access and administrative controls in line with the sensitivity of the data.  Agencies that perform de-identifcation should document that:  26  NIST SP 800-188 3pd  November 2022  NIST SP 800-188  DE-IDENTIFYING GOVERNMENT DATASETS  33  Figure 4 Conceptual diagram of the relationship between post-transformation identifiability, level of expected  harm, and suitability of selected privacy controls for a data release. From Altman et al.  Agencies performing de-identification should document that:  • Techniques used to perform the de-identification are theoretically sound and generally  accepted;100  • Software used to perform the de-identification is reliable for the intended task;  100 Specifically, agencies may wish to mirror the language of the HIPAA Privacy Rule’s expert determination method, which  states: “The second way to de-identify PHI is to have a qualified statistician determine, using(1) A person with appropriate  knowledge of and experience with generally accepted statistical and scientific principles and methods, for rendering  information not individually identifiable: (i) Applying such principles and methods, determines that the risk is very small  that the information could be used, alone or in combination with other reasonably available information, by thean  anticipated recipient to identify thean individual who is a subject of the information. The qualified statistician must  document; and  (ii) Documents the methods and results of the analysis that justify such a determination.” ;  See  https://www.hhs.gov/hipaa/for- professionals/privacy/special-topics/de-identification/index.html#guidancedetermination.  Fig. 4. Altman’s conceptual diagram of the relationship between post-transformation  identifability, level of expected harm, and suitability of selected privacy controls for a data  release [75]  27  NIST SP 800-188 3pd  November 2022  1137  1138  1139  1140  1141  1142  1143  1144  1145  1146  1147  1148  1149  1150  1151  1152  1153  1154  1155  1156  1157  • The techniques used to perform the de-identifcation are theoretically sound and gen- erally accepted.14  • The software used to perform the de-identifcation is reliable for the intended task.  • The individuals who performed the de-identifcation were suitably qualifed.  • The tests that were used to evaluate the effectiveness of the de-identifcation were  validated for that purpose.  • Ongoing monitoring is in place to ensure the continued effectiveness of the de- identifcation strategy.  No matter where de-identifcation is applied in the data life cycle, agencies should docu- ment the answers to the following questions for each de-identifed dataset:  • Are direct identifers collected with the dataset?  • Even if direct identifers are not collected, is it still possible to identify the data  subjects through the presence of quasi-identifers?  • Where in the data life cycle is de-identifcation performed? Is it performed in only  one place or in multiple places?  • Is the original dataset retained after de-identifcation?  • Is there a key or map retained so that specifc data elements can be re-identifed later?  • How are decisions made regarding de-identifcation and re-identifcation?  • Are there specifc datasets that can be used to re-identify the de-identifed data? If so,  what controls are in place to prevent intentional or unintentional re-identifcation?  • Is it a problem if some records in a dataset are re-identifed?  14To determine that a technique is theoretically sound and generally accepted, agencies that wish to adopt  guidance that mirrors the language that the HHS November 26, 2012 Guidance Regarding Methods for  De-identifcation of Protected Health Information in Accordance with the Health Insurance Portability  and Accountability Act (HIPAA) Privacy Rule [136]. uses in its discvussion of the Privacy Rule’s “expert  determination method,” which states on page 7:  “A covered entity may determine that health information is not individually identifable health information  only if:  (1) A person with appropriate knowledge of and experience with generally accepted statistical and sci- entifc principles and methods for rendering information not individually identifable:  (i) Applying such principles and methods, determines that the risk is very small that the informa- tion could be used, alone or in combination with other reasonably available information, by an  anticipated recipient to identify an individual who is a subject of the information; and  (ii) Documents the methods and results of the analysis that justify such determination;”  28  NIST SP 800-188 3pd  November 2022  1158  1159  1160  1161  1162  1163  1164  1165  1166  1167  1168  1169  1170  1171  1172  1173  1174  1175  1176  1177  1178  1179  1180  1181  1182  1183  1184  1185  1186  1187  1188  1189  1190  1191  1192  1193  1194  1195  • Is there a mechanism that will inform the de-identifying agency if there is an attempt  to re-identify the de-identifed dataset? Is there a mechanism that will inform the  agency if the attempt is successful?  3.4. Data-Sharing Models  Agencies should decide on the data-sharing model that will be used to make the data avail- able outside of the agency after the data have been de-identifed [51, p.14]. Specifc models  combine security and privacy techniques to reduce privacy risks to individuals. Security  refers to techniques that limit who can view the data. Encryption is an example of a secu- rity technique – it allows only the party holding the encryption key to view the data. Pri- vacy refers to techniques that limit what information the data contains. The two concepts  can be considered orthogonally. In practice, however, who has access to the data makes  a signifcant difference in the expected risk of disclosure and therefore infuences the ex- tent to which privacy techniques must be used to limit the presence of sensitive personally  identifable information in the data.  A number of possible models exist at different points in the spectrum of security and pri- vacy protections. Figure 4 summarizes this spectrum: its x-axis describes various privacy  techniques that can limit the informational content of the data; its y-axis describes how  much harm would occur if the underlying information were disclosed; and the regions of  the graph are labeled with suggested security techniques. Some common combinations of  security and privacy techniques include:  The Release and Forget Model [94]. The de-identifed data may be released to the pub- lic, typically by being published on the internet. It can be diffcult or impossible for  an organization to recall the data once released in this fashion and may limit infor- mation for future releases.  The Data Use Agreement (DUA) Model. The de-identifed data may be made available  under a legally binding data use agreement that details what can and cannot be done  with the data. Typically, data use agreements may prohibit attempted re-identifcation,  linking to other data, and redistribution of the data without a similarly binding DUA.  A DUA will typically be negotiated between the data holder and qualifed researchers  (the “qualifed investigator model” [44]) or members of the general public (e.g., cit- izen scientists or the media), although they may be simply posted on the internet  with a click-through license agreement that must be agreed to before the data can be  downloaded (the “click-through model” [44]).  The Synthetic Data with Verifcation Model. Statistical disclosure limitation techniques  are applied to the original dataset and used to create a synthetic dataset that contains  many of the aspects of the original dataset but does not contain disclosing infor- mation. The synthetic dataset is released, either publicly or to vetted researchers.  The synthetic dataset can then be used as a proxy for the original dataset, and if  29  NIST SP 800-188 3pd  November 2022  1196  1197  1198  1199  1200  1201  1202  1203  1204  1205  1206  1207  1208  1209  1210  1211  1212  1213  1214  1215  1216  1217  1218  1219  1220  1221  1222  1223  1224  1225  1226  1227  1228  1229  1230  1231  1232  1233  constructed well, the results of statistical analyses should be similar. If used in con- junction with an enclave model as below, researchers may use the synthetic dataset  to develop queries and/or analytic software. These queries and/or software can then  be taken to the enclave or provided to the agency and be applied on the original data.  The Enclave Model [44, 87, 113]. The de-identifed data may be kept in a segregated en- clave that restricts the export of the original data and instead accepts queries from  qualifed researchers, runs the queries on the de-identifed data, and responds with  results. Enclaves can be physical or virtual and can operate under a variety of dif- ferent models. For example, vetted researchers may travel to the enclave to perform  their research, as is done with the Federal Statistical Research Data Centers operated  by the U.S. Census Bureau. Enclaves may be used to implement the verifcation step  of the Synthetic Data with Verifcation Model. Queries made in the enclave model  may be vetted automatically or manually (e.g., by the DRB). Vetting can try to screen  for queries that might violate privacy or are inconsistent with the stated purpose of  the research.  Sharing models should consider the possibility of multiple or periodic releases. Just as  repeated queries to the same dataset may leak personal data from the dataset, repeated de- identifed releases (whether from the same dataset or from different datasets containing  some of the same individuals) by an agency may result in compromising the privacy of in- dividuals unless each subsequent release is viewed in light of the previous release. Even if  a contemplated release of a de-identifed dataset does not directly reveal identifying infor- mation, federal agencies should ensure that the release – combined with previous releases  – will also not reveal identifying information [137].  Instead of sharing an entire dataset, the data owner may choose to release a sample. If only  a sample is released, the probability of re-identifcation decreases because a data intruder  will not know if a specifc individual from the data universe is present in the de-identifed  dataset [43]. However, releasing only a sample may decrease the statistical power of tests  on the data, may cause users to draw incorrect inferences if proper statistical sampling  methods are not used, and may not align with agency goals regarding transparency and  accountability.  3.5. The Five Safes  Agencies that make data available to outsiders should use a repeatable methodology for  evaluating the terms under which that data will be made available. The Five Safes [31] is  such a framework.  The Five Safes was created in the United Kingdom to assist a national statistical agency in  evaluating proposed collaborative projects with the larger research community. The frame- work is designed to assist in “designing, describing and evaluating” data access systems.  Here, the term “data access system” is viewed broadly as any mechanism that allows out-  30  NIST SP 800-188 3pd  November 2022  1234  1235  1236  1237  1238  1239  1240  1241  1242  1243  1244  1245  1246  1247  1248  1249  1250  1251  1252  1253  1254  1255  1256  1257  1258  1259  1260  1261  1262  1263  1264  1265  1266  1267  1268  1269  siders to gain access to the agency’s confdential data. That is, a data access system might  include setting up an enclave for academic researchers who undergo extensive background  checks, but it also includes publishing data on the internet.  The Five Safes framework gets its name from the use of fve categories (called “risk” or  “access” dimensions) that are used in the evaluation. They are:  1. Safe projects Is this use of the data appropriate?  2. Safe people Can the researchers be trusted to use it in an appropriate manner?  3. Safe data Is there a disclosure risk in the data itself?  4. Safe settings Does the access facility limit unauthorized use?  5. Safe outputs Are the statistical results non-disclosive?  Each of these dimensions is independent. That is, the legal, moral, and ethical review of  each dimension is independent of the others. In practice, this might mean that the project  is safe (the proposed use of the data is appropriate), the people are safe (the researchers are  noted academics with respected histories of collaborative work), the data are safe (there is  no disclosure risk in the data), and the output is safe (it will not disclose personal infor- mation). However, because the setting is not safe (perhaps the facility has poor internal  security), the project should not go forward. In this example, the Five Safes framework  would provide a decision-maker with the tools to separate each of these dimensions and  resolve the problems so that the project could proceed.  One of the positive aspects of the Five Safes framework is that it forces data controllers  to consider many different aspects of data release when evaluating data access proposals.  Frequently, the authors write, it is common for data owners to “focus on one, and only  one, particular issue (such as the legal framework surrounding access to their data or IT  solutions).” With the Five Safes, people who may be specialists in one area are forced to  consider (or to explicitly not consider) aspects of privacy protection with which they may  not be familiar and might otherwise overlook.  The Five Safes framework can be used as a tool for designing access systems, for evaluating  existing systems, for communication, and for training. Agencies should consider using a  framework such as The Five Safes for organizing risk analyses of data release efforts.  3.6. Disclosure Review Boards  Disclosure Review Boards (DRBs), also known as Data Release Boards, are administrative  bodies created within an organization that are charged with ensuring that intended dis- closures meet the policy and procedural requirements of that organization. DRBs should  be governed by a written mission statement and charter (or equivalent document) that  are ideally approved by the same mechanisms that the organization uses to approve other  organization-wide policies.  31  NIST SP 800-188 3pd  November 2022  1270  1271  1272  1273  1274  1275  1276  1277  1278  1279  1280  1281  1282  1283  1284  1285  1286  1287  1288  1289  1290  1291  1292  1293  1294  1295  1296  1297  1298  1299  1300  1301  1302  1303  1304  1305  1306  The DRB should have a mission statement that guides its activities. For example, the U.S.  Department of Education’s DRB has the mission statement:  The Mission of the Department of Education Disclosure Review Board (ED- DRB) is to review proposed data releases by the Department’s principal offces  (POs) through a collaborate technical assistance, aiding the Department to re- lease as much useful data as possible, while protecting the privacy of individ- uals and the confdentiality of their data, as required by law. [41]  The DRB charter specifes the mechanics of how the mission is implemented. A formal,  written charter promotes transparency in the decision-making process and ensures consis- tency in the applications of its policies.  Most DRBs will be established to weigh the interests of data release against those of in- dividual privacy protection. However, a DRB may also be chartered to consider group  harms [51, p.13] that can result from the release of a dataset. Such harms go beyond the  harm to the privacy interests of a specifc individual.  The DRB charter should frame the DRB’s responsibilities in reference to existing orga- nizational policies, regulations, and laws. Some agencies may balance these concerns by  employing data use models other than de-identifcation (e.g., by establishing data enclaves  where a limited number of vetted researchers can access sensitive datasets in a way that  provides data value while minimizing the possibility for harm or by authorizing the use  of secure multi-party computation, homomorphic encryption, or other Privacy Preserving  Data Analytics to compute various statistics). In those agencies, a DRB would be empow- ered to approve the use of such mechanisms.  Certain agencies may engage in data disclosure on a routine basis (such as research and  evaluation agencies), in which case it may be benefcial for the DRB to establish policies  and procedures for de-identifcation rather than being responsible for every review. In  these cases, the DRB charter should clearly specify how the group will provide oversight  and ensure organizational accountability to the agreed-upon policies.  The DRB charter should specify the DRB’s composition. To be effective, the DRB should  include representatives from multiple groups and experts in both technology and privacy  policy. Specifcally, DRBs may wish to have as members:  • Individuals who represent the interests of potential users (such individuals need not  come from outside of the organization)  • Representation from among the public, specifcally from groups represented in the  datasets if they have a limited scope  • Representation from the organization’s leadership team, such as a representation of  the Senior Agency Offcial for Privacy [4, Appendix II, section 4] (such representa- tion helps to establish the DRB’s credibility with the rest of the organization)  32  NIST SP 800-188 3pd  November 2022  1307  1308  1309  1310  1311  1312  1313  1314  1315  1316  1317  1318  1319  1320  1321  1322  1323  1324  1325  1326  1327  1328  1329  1330  1331  1332  1333  1334  1335  1336  1337  1338  1339  1340  • A representative of the organization’s senior privacy offcial  • Subject matter experts  • Outside experts  The charter should establish rules for ensuring a quorum and specify whether members can  designate alternates on a standing or meeting-by-meeting basis. The DRB should specify  the mechanism by which members are nominated and approved, their tenure, conditions  for removal, and removal procedures.15  The charter should set policy expectations for record keeping and reporting, including  whether records and reports are considered public or restricted. For example, the char- ter could specify that a DRB issue an annual report with a list of every dataset that was  approved for release. The charter should indicate whether it is possible to exclude sensitive  decisions from these reporting requirements and the mechanism for doing so. Ideally, the  charter should be a public document to promote transparency.  To meet its requirement of evaluating data releases, the DRB should require that writ- ten applications be submitted to the DRB that specify the nature of the dataset, the de- identifcation methodology, and the result. An application may require that the proposer  present the re-identifcation risk, the risk to individuals if the dataset is re-identifed, and  a proposed plan for detecting and mitigating successful re-identifcation. In addition, the  DRB should require that when individuals are informed that their information will be de- identifed, they also be informed that privacy risks may remain despite de-identifcation.  The DRB should keep accurate records of its request memos, their associated documen- tation, the DRB decision, and the actual fles released. These records should be appropri- ately archived and curated so that they can be recovered. In the case of large data releases,  the defnitive version of the released data should be curated using an externally validated  procedure, such as a recorded cryptographic hash value or signature, and a digital object  identifer (DOI) [64].  DRBs may wish to institute a two-step process in which the applicant frst proposes and  receives approval for a specifc de-identifcation process that will be applied to a specifc  dataset and then submits and receives approval for the release of the dataset that has been  de-identifed according to the proposal. However, because it is theoretically impossible  to predict the results of applying an arbitrary process to an arbitrary dataset [26, 129],  the DRB should be empowered to reject a proposed release of a dataset even if it has  been de-identifed in accordance with an approved procedure because performing the de- identifcation may demonstrate that the procedure was insuffcient to protect privacy. The  15For example, in 2022, the Census Bureau’s DRB had 12 voting members: two technical co-chairs, a repre- sentative from the Policy Coordination Offce, a representative from the Associate Director for Communica- tions, two representatives from the Center for Enterprise Dissemination-Disclosure Avoidance (CED-DA),  two representatives from the Economic Programs Directorate, two representatives from the Demographic  Programs Directorate, and two representatives from the Decennial Programs Directorate [24].  33  NIST SP 800-188 3pd  November 2022  1341  1342  1343  1344  1345  1346  1347  1348  1349  1350  1351  1352  1353  1354  1355  1356  1357  1358  1359  1360  1361  1362  1363  1364  1365  1366  1367  1368  1369  1370  1371  1372  1373  1374  DRB should be able to delegate the responsibility of reviewing the de-identifed dataset,  but such responsibility should not be delegated to the individual or group that performed  the de-identifcation.  The DRB charter should specify whether the DRB needs to approve each data release by  the organization or if it may grant blanket approval for all data of a specifc type that is de- identifed according to a specifc methodology. The charter should specify the duration of  the approval. Given advances in the science and technology of de-identifcation, it is inad- visable that a Board be empowered to grant release authority for an indefnite or unlimited  amount of time.  In most cases, a single privacy protection methodology will be insuffcient to protect the  varied datasets that an agency may wish to release. That is, different techniques might best  optimize the trade-off between re-identifcation risk and data usability, depending on the  specifcs of each kind of dataset. Nevertheless, the DRB may wish to develop guidance, rec- ommendations, and training materials regarding specifc de-identifcation techniques that  are to be used. Agencies that standardize on a small number of de-identifcation techniques  will gain familiarity with these techniques and are likely to have results with a higher level  of consistency and success than those that have no such guidance or standardization.  Although it is envisioned that DRBs will work in a cooperative, collaborative, and conge- nial manner with those inside an agency seeking to release de-identifed data, there will  at times be a disagreement of opinion. For this reason, the DRB’s charter should state  whether the DRB has the fnal say over disclosure matters or if the DRB’s decisions can be  overruled, by whom, and by what procedure. For example, an agency might give the DRB  fnal say over disclosure matters but allow the agency’s leadership to replace members of  the DRB as necessary. Alternatively, the DRB’s rulings might merely be advisory, with all  data releases being individually approved by agency leadership or its delegates.16  Finally, agencies should decide whether the DRB charter will include any kind of perfor- mance timetables or be bound by a service-level agreement (SLA) that defnes a level of  service to which the DRB commits.  The key elements of a Disclosure Review Board include:  • A written mission statement and charter  • Members represent different groups within the organization, including leadership  • The Board receives written applications to release de-identifed data  • The Board reviews both the proposed methodology and the results of applying the  methodology  16At the Census Bureau, “staff members [who] are not satisfed with the DRB’s decision . . . may appeal to a  steering committee consisting of several Census Bureau Associate Directors. Thus far, there have been few  appeals, and the Steering Committee has never reversed a decision made by the Board” [130, p.35].  34  NIST SP 800-188 3pd  November 2022  1375  1376  1377  1378  1379  1380  1381  1382  1383  1384  1385  1386  1387  1388  1389  1390  1391  1392  1393  1394  1395  1396  1397  1398  1399  1400  1401  1402  1403  1404  1405  1406  • Applications should identify the risks associated with data release, including re- identifcation probability, potentially adverse events that would result if individuals  are re-identifed, and a mitigation strategy if re-identifcation takes place  • Approvals may be valid for multiple releases but should not be valid indefnitely  • Reliable records management for applications, approvals, and released data  • Mechanisms for dispute resolution  • Timetable or service-level agreement (SLA)  • Legal and technical understanding of privacy  Example outputs of a DRB include specifying access methods for different kinds of data  releases, establishing acceptable levels of re-identifcation risk, and maintaining detailed  records of previous data releases that ideally include the dataset that was released and the  privacy-preserving methodology that was employed.  There is some similarity between DRBs as envisioned here and the Institutional Review  Board (IRBs) system created by the Common Rule17 for regulating human subject research  in the United States. However, there are also important differences:  • While the purpose of IRBs is to protect human subjects involved in human subject  research, DRBs are charged with protecting data subjects, institutions, and – poten- tially – society as a whole.  • Whereas IRBs are required to have “at least one member whose primary concerns  are in nonscientifc areas” and “at least one member who is not otherwise affliated  with the institution and who is not part of the immediate family of a person who is  affliated with the institution,” there does not appear to be a requirement for such  members on a DRB.  • Whereas IRBs give approval for research and then typically receive reports only dur- ing an annual review or when a research project terminates, DRBs may be involved  at multiple points during the process.  • Whereas approval of an IRB is required before research with human subjects can  commence, DRBs are typically involved after research has taken place and prior to  data or other research fndings being released.  • Whereas service on an IRB requires knowledge of the Common Rule and an under- standing of ethics, service on a DRB requires knowledge of statistics, computation,  public policy, and some familiarity with the data being considered for release.  17The Federal Policy for the Protection of Human Subjects or the “Common Rule” was published in 1991  and codifed in separate regulations by 15 federal departments and agencies. The Revised Common Rule  was published in the Federal Register (FR) on January 19, 2017, and was amended to delay the effective  and compliance dates on January 22, 2018, and June 19, 2018 [135].  35  NIST SP 800-188 3pd  November 2022  1407  1408  1409  1410  1411  1412  1413  1414  1415  1416  1417  1418  1419  1420  1421  1422  1423  1424  1425  1426  1427  1428  1429  1430  1431  1432  1433  1434  1435  1436  1437  1438  1439  1440  1441  1442  3.7. De-Identifcation Standards  Agencies can rely on de-identifcation standards to provide standardized terminology, pro- cedures, and performance criteria for de-identifcation efforts. Agencies can adopt existing  de-identifcation standards or create their own. De-identifcation standards can be prescrip- tive or performance-based.  3.7.1. Benefts of Standards  De-identifcation standards assist agencies with the process of de-identifying data prior to  public release. Without standards, data owners may be unwilling to share data, as they may  be unable to assess whether a procedure for de-identifying data is suffcient to minimize  privacy risk.  Standards can increase the availability of individuals with appropriate training by iden- tifying a specifc body of knowledge and practice that training should address. Absent  standards, agencies may forego opportunities to share data. De-identifcation standards can  help practitioners develop a community, as well as certifcation and accreditation processes.  Standards decrease uncertainty and provide data owners and custodians with best practices  to follow. Courts can consider standards as acceptable practices that should generally be  followed. In the event of litigation, an agency can point to the standard and say that it  followed good data practice.  3.7.2. Prescriptive De-Identifcation Standards  A prescriptive de-identifcation standard specifes an algorithmic procedure that – if fol- lowed – results in data that are de-identifed to an established benchmark.  The “Safe Harbor” method of the HIPAA Privacy Rule [3] is an example of a prescriptive  de-identifcation standard. The intent of the Safe Harbor method is to “provide covered en- tities with a simple method to determine if the information is adequately de-identifed” [89].  It does this by specifying that health information is considered to be de-identifed through  the removal of 18 kinds of identifers and the assurance that the entity does not have actual  knowledge that the remaining information can be used to identify an individual who is the  subject of the information. Once de-identifed, the dataset is no longer subject to HIPAA  privacy, security, and breach notifcation regulations. Nevertheless, “a covered entity may  require the recipient of de-identifed information to enter into a data use agreement to ac- cess fles with known disclosure risk” [89].  The Privacy Rule states that a covered entity that employs the Safe Harbor method must  have no “actual knowledge” that the information – once de-identifed – could still be used  to re-identify individuals. However, covered entities are not obligated to employ experts  or mount re-identifcation attacks against datasets to verify that the use of the Safe Harbor  method has in fact resulted in data that cannot be re-identifed.  36  NIST SP 800-188 3pd  November 2022  1443  1444  1445  1446  1447  1448  1449  1450  1451  1452  1453  1454  1455  1456  1457  1458  1459  1460  1461  1462  1463  1464  1465  1466  1467  1468  1469  1470  1471  1472  1473  1474  1475  1476  1477  Prescriptive standards have the advantage of being relatively easy for users to follow, but  developing, testing, and validating such standards can be burdensome. Because prescrip- tive de-identifcation standards do not depend on the particulars of a specifc case, there  is a tendency for them to be more conservative than is necessary, resulting in an unneces- sary decrease in data for corresponding levels of risk. Even so, there is no assurance that  following a prescriptive standard actually produces the intended outcome.  Agencies that create prescriptive de-identifcation standards should ensure that data de- identifed according to the standards have a suffciently small risk of being re-identifed  consistent with the intended level of privacy protection. Such assurances frequently can- not be made unless formal privacy techniques, such as differential privacy, are employed.  However, agencies may determine that public policy goals furthered by having an easy- to-use prescriptive standard outweighs the risk of a standard that does not have provable  privacy guarantees.  Prescriptive de-identifcation standards carry the risk that the standard may not suffciently  de-identify to avoid the risk of re-identifcation, especially as methodology advances and  more data sources become available.  A second risk when adopting prescriptive standards is that different agencies (or govern- ments) may adopt inconsistent rules. In such a case, information that is legally de-identifed  for one purpose or in one jurisdiction may not be legally de-identifed in another.  3.7.3. Performance-Based De-Identifcation Standards  Performance-based de-identifcation standards specify the properties that de-identifed data  must have. For example, under the “Expert Determination” method of the HIPAA Privacy  Rule, a technique for de-identifying data is suffcient if an appropriate expert applying  generally accepted statistical and scientifc principles and methods “determines that the  risk is very small that the information could be used, alone or in combination with other  reasonably available information, by an anticipated recipient to identify an individual who  is a subject of the information” [89]. The rule requires that experts document their methods  and the results of their analyses.  Performance-based standards have the advantage of allowing users many different ways to  solve a problem by leaving room for innovation. Another advantage is that they can require  the desired outcome rather than specifying an aspirational mechanism.  Performance-based standards should be suffciently detailed to perform in a manner that is  reliable and repeatable. For example, standards that call for the use of experts can specify  how an expert’s expertise should be determined. Standards that call for the reduction of  risk to an acceptable level should provide a procedure for determining that level.  37  NIST SP 800-188 3pd  November 2022  1478  1479  1480  1481  1482  1483  1484  1485  1486  1487  1488  1489  1490  1491  1492  1493  1494  1495  1496  1497  1498  1499  1500  1501  1502  1503  1504  1505  1506  1507  1508  1509  1510  1511  3.8. Education, Training, and Research  De-identifying data in a manner that preserves privacy can be a complex mathematical,  statistical, administrative, and data-driven process. Frequently, the opportunities for iden- tity disclosure will vary from dataset to dataset. Privacy-protecting mechanisms developed  for one dataset may not be appropriate for others. For these reasons, agencies that engage  in de-identifcation should ensure that their workers have adequate education and training  in the subject domain. Agencies may wish to establish education or certifcation require- ments for those who work directly with the datasets or to adopt industry standards such  as the HITrust De-Identifcation Framework [77]. Because de-identifcation techniques are  modality-dependent, agencies using de-identifcation may need to institute research efforts  to develop and test appropriate data release methodologies.  3.9. Defense in Depth  In addition to de-identifcation, there are other technologies and methodologies that can  secure sensitive data. Many of these approaches can complement de-identifcation and  further reduce privacy risk to data subjects. Combining techniques is an example of defense  in depth and should be considered whenever possible.  3.9.1. Encryption and Access Control  Encrypting sensitive data at rest can prevent attackers from obtaining the data directly  (e.g., by compromising the server that stores it). Encryption can also serve as a form of  access control (i.e., it can control who can access the data) because examining the data  requires access to the encryption keys. If the original data (with identities) are retained,  they should be stored encrypted, and access should be limited. Even after de-identifcation,  more sensitive data not intended for public release can be provided to select individuals by  limiting access via encryption.  3.9.2. Secure Computation  Two technologies enable computing on encrypted data without decrypting it:  1. Fully-homomorphic encryption (FHE) [55] allows a server to compute a function  f (x) on an encrypted value x without decrypting it. The result is a new encrypted  value that can only be decrypted by someone who holds the original encryption key.  2. Secure multi-party computation (MPC) [74] allows multiple servers to jointly  compute a function f (x1, . . . ,xk), where each server provides one of the inputs xi,  and no server learns any of the others’ inputs.  Both of these approaches are general-purpose in that they can be used to compute any  function, and both are considerably slower than performing the equivalent computation  38  NIST SP 800-188 3pd  November 2022  1512  1513  1514  1515  1516  1517  1518  1519  1520  1521  1522  1523  1524  1525  1526  1527  1528  1529  1530  1531  1532  1533  1534  1535  1536  1537  1538  1539  1540  1541  1542  1543  1544  1545  1546  with unencrypted data on a single computer. Nevertheless, both approaches are now suf- fciently performant that they can be used for many practical kinds of privacy-preserving  data analysis.18  3.9.3. Trusted Execution Environments  Trusted Execution Environments (TEEs) (also called trusted hardware enclaves or secure  hardware enclaves) are another approach for computing on encrypted data. TEEs are im- plemented in computer hardware, typically within the silicon of a modern CPU, and pro- tect programs that run on that CPU from the surrounding environment. For example, a  TEE can cause data from a computer’s CPU to be automatically encrypted when written  to main memory and decrypted when read back to the CPU. In this way, data in memory  are protected from other devices that can access memory, such as a network interface card.  In addition to encryption, TEEs typically support attestation so that a program running on  a TEE can attest to a remote system that the program is a true, legitimate, and faithful  execution of the program.  Traditional cloud services require trusting the cloud provider, who may have a compro- mised environment (e.g., an operating system that records encryption keys). A TEE de- creases the need for trust because it allows a user to validate that they are communicating  with the remote program and offers assurance that no other program running in the cloud  provider can access the program’s data. Secure enclaves can thus be used to allow untrusted  infrastructure to operate on sensitive data in much the same way as technologies like FHE  and MPC.  Intel’s Software Guard Extensions (SGX) [112], ARM’s TrustZone [101], and AMD’s Se- cure Encrypted Virtualization (SEV) [13] are all examples of secure hardware enclaves.  All of these products are designed to provide similar security to cryptographic techniques  while also providing performance similar to a single CPU operating on unencrypted data.  These secure hardware products are necessarily complex, and various implementation er- rors have been discovered that can allow attackers to defeat their security protections. Se- cure hardware enclaves certainly offer increased security for data compared to plaintext  computation, but agencies should carefully consider the trade-off between performance  and security when choosing between secure hardware and cryptographic techniques.  3.9.4. Physical Enclaves  For extremely sensitive data, a physical enclave (see Section 3.4) may provide additional  security. In this model, data are stored on a computer not connected to any network and  are accessible only via physical access to a particular room. Access to the data is then  controlled by limiting access to the room. This approach can be quite cumbersome.  18More information about these and other kinds of secure computation can be found on the NIST Privacy- Enhancing Cryptography (PEC) project website at https://csrc.nist.gov/projects/pec.  39  https://csrc.nist.gov/projects/pec  NIST SP 800-188 3pd  November 2022  1547  1548  1549  1550  1551  1552  1553  1554  1555  1556  1557  1558  1559  1560  1561  1562  1563  1564  1565  1566  1567  1568  1569  1570  1571  1572  1573  1574  1575  1576  1577  s  r  4. Technical Steps for Data De-Identifcation  The goal of de-identifcation is to transform data in a way that protects privacy while pre- erving the validity of inferences drawn on that data within the context of a target use-case.  This section discusses technical options for performing de-identifcation and verifying the  esult of a de-identifcation procedure.  Agencies should adopt a detailed, written process for de-identifying data prior to com- mencing work on a de-identifcation project. The details of the process will depend on the  particular de-identifcation approach that is pursued. In developing technical steps for data  de-identifcation, agencies may wish to consider existing de-identifcation standards, such  as the HIPAA Privacy Rule, the IHE De-Identifcation Handbook [61], or the HITRUST  De-Identifcation Framework [77].  4.1. Determine the Privacy, Data Usability, and Access Objectives  Agencies intent on de-identifying data for release should understand the nature of the  data that they intend to de-identify and determine the policies and standards that will be  used to determine acceptable levels of data accuracy, de-identifcation, and the risk of re- identifcation. For example:  • Where did the data come from?  • What promises were made when the data were collected?  • What are the legal and regulatory requirements regarding data privacy and release?  • What is the purpose of the data release?  • What is the intended use of the data?  • What data-sharing model (Section 3.4) will be used?  • Which standards for privacy protection or de-identifcation will be used?  • What is the level of risk that the project is willing to accept?  • What are the goals for limiting re-identifcation? For example:  – No one can be re-identifed.  – Only a few people can be re-identifed.  – Only a few people can be re-identifed in theory, but no one will actually be  re-identifed in practice.  – Only outliers can be re-identifed.  – Only people who are not outliers can be re-identifed.  40  NIST SP 800-188 3pd  November 2022  1578  1579  1580  1581  1582  1583  1584  1585  1586  1587  1588  1589  1590  1591  1592  1593  1594  1595  1596  1597  1598  1599  1600  1601  1602  1603  1604  1605  1606  1607  1608  1609  1610  1611  1612  1613  – There is a small percentage chance of re-identifcation that is shared by every- one in the dataset.  – There is a small percentage chance of re-identifcation, but some people in the  dataset are signifcantly more likely to be re-identifed, and the re-identifcation  probability is somehow bounded.  • What harm might result from re-identifcation, and what techniques will be used to  mitigate those harms?  • How should compliance with that level of risk be determined?  Some goals and objectives are synergistic, while others are in opposition.  4.2. Conducting a Data Survey  Different kinds of data require different kinds of de-identifcation techniques. As a result,  an important early step in the de-identifcation of government data is to identify the data  modalities that are present in the dataset and formulate a plan for de-identifcation that takes  into account goals for data release, data accuracy, privacy protection, and the best available  science.  For example:  • Tabular numeric and categorical data is the subject of the majority of de-identifcation  research and practice. These datasets are most frequently de-identifed by using tech- niques based on the designation and removal of direct identifers and the manipula- tion of quasi-identifers. The chief criticism of de-identifcation based on direct and  quasi-identifers is that administrative determinations of quasi-identifers may miss  variables that can be uniquely identifying when combined and linked with external  data, including data that are not available at the time the de-identifcation is per- formed but become available in the future.  K-anonymity [122] is a common framework for performing and evaluating the de- identifcation of tabular numeric and categorical data. However, risk determinations  based on this kind of de-identifcation will be incorrect if direct and quasi-identifers  are not properly classifed. For example, if there exist quasi-identifers that are not  identifed as such and not subjected to k-anonymity, then it may be easy to re-identify  records in the de-identifed dataset.  Tabular data may also be used to create a synthetic dataset that preserves some infer- ence validity but does not have a one-to-one correspondence to the original dataset.  • Dates and times require special attention when de-identifying because temporal in- formation is inherently linked to an external dataset: the natural progression of time.  Some dates and times (e.g., February 22, 1732) are highly identifying, while others  are not. Dates that refer to matters of public record (e.g., date of birth, death, or home  41  NIST SP 800-188 3pd  November 2022  1614  1615  1616  1617  1618  1619  1620  1621  1622  1623  1624  1625  1626  1627  1628  1629  1630  1631  1632  1633  1634  1635  1636  1637  1638  1639  1640  1641  1642  1643  1644  1645  1646  1647  1648  1649  purchase) should be routinely taken as having high re-identifcation potential. Dates  may also form the basis of linkages between dataset records or even within a record.  For example, a record may contain the date of admission, the date of discharge, and  the number of days in residence. Thus, care should be taken when de-identifying  dates to locate and properly handle potential linkages and relationships. Applying  different techniques to different felds may result in information being left in a dataset  that can be used for re-identifcation. Specifc issues regarding date de-identifcation  are discussed in Section 4.3.4, “De-Identifying Dates.”  • Geographic and map data also require special attention when de-identifying, as  some locations can be highly identifying, other locations are not identifying at all,  and some locations are only identifying at specifc times. As with dates and times,  de-identifying geographic locations is challenging because locations inherently link  to an external reality, and some locations during specifc time periods are highly  correlated with specifc individuals (e.g., 38.8977° N, 77.0365° W). Identifying lo- cations can be de-identifed through the use of perturbation or generalization. The  effectiveness of such de-identifcation techniques for protecting privacy in the pres- ence of external information has not been well-characterized [51, p.37][115]. Spe- cifc issues regarding geographical de-identifcation are discussed in Section 4.3.5,  “De-Identifying Geographical Locations.”  • Unstructured text may contain direct identifers, such as a person’s name, or may  contain additional information that can serve as a quasi-identifer. Finding such iden- tifers invariably requires domain-specifc knowledge [51, p. 30]. Note that unstruc- tured text may be present in tabular datasets and require special attention.19  • Photos and video may contain identifying information, such as printed names (e.g.,  name tags), as well as metadata in the fle format. A range of biometric techniques  also exists for matching photos of individuals against a dataset of photos and identi- fers [51, p. 32].  • Medical imagery poses additional problems over photographs and video due to the  presence of technical, medically specifc information. For example, identifying in- formation may be present in the image itself (e.g., a photo may show an identifying  scar or tattoo), an identifer may be “burned in” to the image area (e.g., an identif- cation plate containing a patient name that is included in an X-Ray), or an identifer  may be present in the fle metadata. The body part in the image itself may also be  recognized using a biometric algorithm and dataset [51, p.35].  • Genetic sequences and other kinds of sequence information can be identifed by  using existing databanks that match sequences and identities. There is also evi-  19For an example of how unstructured text felds can damage the policy objectives and privacy assurances of  a larger structured dataset, see Andrew Peterson’s article, “Why the names of six people who complained  of sexual assault were published online by Dallas police” [98].  42  NIST SP 800-188 3pd  November 2022  1650  1651  1652  1653  1654  1655  1656  1657  1658  1659  1660  1661  1662  1663  1664  1665  1666  1667  1668  1669  1670  1671  1672  1673  1674  1675  1676  1677  1678  1679  dence that genetic sequences from individuals who are not in datasets can be matched  through genealogical triangulation – a process that uses genetic information and other  information as quasi-identifers to single out a specifc identity [51, p.36]. At present,  there is no known method to reliably de-identify genetic sequences. Specifc issues  regarding the de-identifcation of genetic information is discussed in Section 4.3.6,  “De-Identifying Genomic Information.”  In many cases, data are complex and contain multiple modalities. Such mixtures may  complicate risk determinations.  4.3. De-Identifcation by Removing Identifers and Transforming Quasi-Identifers  De-identifcation based on the removal of identifers and the transformation of quasi-identifers  is one of the most common approaches currently in use. It has the advantage of being con- ceptually straightforward, and there is a long institutional history of using this approach  within both federal statistical agencies and the healthcare industry. This approach has the  disadvantage of not being based on formal methods for assuring privacy protection. The  lack of formal methods does not mean that this approach cannot protect privacy, but it does  mean that privacy protection is not assured.  Below is a sample process for de-identifying data by removing identifers and transforming  quasi-identifers:20  1. Determine the re-identifcation risk threshold. The organization determines accept- able risk for working with the dataset and possibly mitigating controls based on  strong precedents and standards.21  2. Determine the information in the dataset that could be used to identify the data sub- jects. Identifying information can include:  Direct identifers including names, phone numbers, and other information that un- ambiguously identifes an individual.  Quasi-identifers that could be used in a linkage attack. Typically, quasi-identifers  identify multiple individuals and can be used to triangulate a specifc individual.  High-dimensional data [10] that can be used to single out data records and thus  constitute a unique pattern that could be identifying if the values exist in a  secondary source to link against.22  20This protocol is based on a protocol developed by Professors Khaled El Emam and Bradley Malin [44].  21See the Federal Committee on Statistical Methodology’s Data Protection Toolkit at  https://nces.ed.gov/fcsm/dpt.  22For example, Narayanan and Shmatikov demonstrated that the set of movies that a person had watched  could be used as an identifer given the existence of a second dataset of movies that had been publicly  rated [84].  43  https://nces.ed.gov/fcsm/dpt  NIST SP 800-188 3pd  November 2022  1680  1681  1682  1683  1684  1685  1686  1687  1688  1689  1690  1691  1692  1693  1694  1695  1696  1697  1698  1699  1700  1701  1702  1703  1704  1705  1706  1707  1708  1709  1710  1711  1712  1713  3. Determine the direct identifers in the dataset. An expert determines the elements in  the dataset that only serve to identify the data subjects.  4. Mask (transform) direct identifers. The direct identifers are either removed or re- placed with pseudonyms. Options for performing this operation are discussed in  Section 4.3.1.  5. Perform threat modeling. The organization determines the additional information  they might be able to use for re-identifcation, including both quasi-identifers and  non-identifying values that a data intruder might use for re-identifcation.  6. Determine minimal acceptable data accuracy. The organization determines what uses  can or will be made with the de-identifed data.  7. Determine the transformation process that will be used to manipulate the quasi- identifers. Pay special attention to the data felds that contain dates and geographical  information, removing or recoding as necessary.  8. Import (sample) data from the source dataset. Because the effort to acquire data from  the source (identifed) dataset may be substantial, some researchers recommend a test  data import run to assist in planning [44].  9. Review the results of the trial de-identifcation. Correct any coding or algorithmic  errors that are detected.  10. Transform the quasi-identifers for the entire dataset.  11. Evaluate the actual re-identifcation risk, which is calculated. As part of this evalua- tion, every aspect of the released dataset should be considered in light of the question,  “Can this information be used to identify someone?”  12. Compare the actual re-identifcation risk with the threshold specifed by the policy- makers.  13. If the data do not pass the actual risk threshold, adjust the procedure and repeat Steps  11 and 12. For example, additional transformations may be required. Alternatively,  it may be necessary to remove outliers. Removing data will of course impact data  quality, but it will also protect the privacy of the individuals whose data has been  removed.  4.3.1. Removing or Transforming of Direct Identifers  There are many possible processes for removing direct identifers from a dataset, including:  • Removal and replacement. Replace identifers with the value used by the database  to indicate a missing value, such as NULL or NA.  • Masking. Replace identifers with a repeating character, such as XXXXXX or 999999.  44  NIST SP 800-188 3pd  November 2022  1714  1715  1716  1717  1718  1719  1720  1721  1722  1723  1724  1725  1726  1727  1728  1729  1730  1731  1732  1733  1734  1735  1736  1737  1738  1739  1740  1741  1742  1743  1744  1745  1746  1747  1748  • Encryption. Encrypt the identifers with a strong encryption algorithm. After en- cryption, the key can be discarded the cryptographic key to prevent decryption. How- ever, if there is a desire to employ the same transformation at a later point in time,  the key should not be discarded but rather stored in a secure location separate from  the de-identifed dataset. Encryption used for this purpose carries special risks that  need to be addressed with specifc controls (see Section 4.3.2 below for further in- formation).  • Hashing with a keyed hash. A keyed hash is a special kind of hash function that  produces different hash values for different keys. The hash key should have suffcient  randomness to defeat a brute force attack aimed at recovering the hash key (e.g.,  SHA-256 HMAC [20] with a 256-bit randomly generated key). As with encryption,  the key should be discarded unless there is a desire for repeatability. Hashing used  for this purpose carries special risks that need to be addressed with specifc controls  (see Section 4.3.2 below for further information).  • Replacement with keywords. This approach transforms identifers such as George  Washington to PATIENT. Note that some keywords may be equally identifying, such  as transforming George Washington to PRESIDENT.  • Replacement with realistic surrogate values. This approach transforms identifers  such as George Washington to surrogates that blend in, such as 23 Abraham Polk.  Encryption, hashing with a keyed hash, and replacement with realistic surrogate values are  pseudonymization techniques. The technique used to remove direct identifers should be  clearly documented for users of the dataset – especially if the technique of replacement by  realistic surrogate names is used – so that future data users have documentation that the  dataset has been de-identifed.  If the agency plans to make data available for longitudinal research and contemplates mul- tiple data releases, then the transformation process should be repeatable, and the resulting  transformed identities should be pseudonyms. The mapping between the direct identifer  and the pseudonym is performed using a lookup table or a repeatable transformation. In  either case, the release of the lookup table or the information used for the repeatable trans- formation will result in compromised identities. Thus, the lookup table or the information  for the transformation must be highly protected. When using a lookup table, the pseudonym  must be randomly assigned.  A signifcant risk of using a repeatable transformation is that a data intruder may be able  to determine the transformation and – in so doing – gain the capability to re-identify all of  the records in the dataset.  23A study by Carrell et. al found that using realistic surrogate names in de-identifed text like John Walker  and 3900 Pennsylvania Ave instead of generic labels like PATIENT and ADDRESS could decrease or  mitigate the risk of re-identifying the few names that remained in the text because “the reviewers were  unable to distinguish the residual (leaked) identifers from the...surrogates” [21].  45  NIST SP 800-188 3pd  November 2022  1749  1750  1751  1752  1753  1754  1755  1756  1757  1758  1759  1760  1761  1762  1763  1764  1765  1766  1767  1768  1769  1770  1771  1772  1773  1774  1775  1776  1777  1778  1779  1780  1781  1782  1783  1784  When multiple organizations use the same pseudonymization scheme, they can trade data  and perform matching on the pseudonyms. However, this practice also allows the orga- nizations to re-identify each other’s shared datasets. As an alternative, organizations can  participate in a private set intersection protocol, of which there are many in the crypto- graphic literature [78, 34, 69].  4.3.2. Special Security Note Regarding the Encryption or Hashing of Direct  Identifers  The transformation of direct identifers through encryption or hashing carries special risks,  as errors in procedure or the release of the encryption key can compromise identities for  the entire dataset.  When information is protected with encryption, the security of the encrypted data depends  entirely on the security of the encryption key. If a key is improperly chosen, it may be  possible for a data intruder to discover the key using a brute force search. Because there  is no visual difference between data that are encrypted with a strong encryption key and  data that are encrypted with a weak key, organizations must utilize administrative controls  to ensure that keys are both unpredictable and suitably protected. The use of encryption or  hashing to protect direct identifers is, therefore, not recommended.  4.3.3. De-Identifying Numeric Quasi-Identifers  Once a determination is made regarding quasi-identifers, they should be transformed. A  variety of techniques are available to transform quasi-identifers:  • Top and bottom coding. Outlier values that are above or below certain values are  coded appropriately. For example, the HIPAA Privacy Rules calls for ages over 89  to be “aggregated into a single category of age 90 or older” [132, § 164.514 (b)].  • Micro aggregation. Individual microdata are combined into small groups that pre- serve some data analysis capability while providing for some disclosure protec- tion [110].  • Generalize categories with small counts. When preparing contingency tables, sev- eral categories with small values may be combined. For example, rather than re- porting that there is one person with blue eyes, two people with green eyes, and one  person with hazel eyes, it may be reported that there are four people with blue, green,  or hazel eyes.  • Data suppression. Cells in contingency tables with counts lower than a predefned  threshold can be suppressed to prevent the identifcation of attribute combinations  with small numbers [141].  • Blanking and imputing. Specifc values that are highly identifying can be removed  and replaced with imputed values.  46  NIST SP 800-188 3pd  November 2022  1785  1786  1787  1788  1789  1790  1791  1792  1793  1794  1795  1796  1797  1798  1799  1800  1801  1802  1803  1804  1805  1806  1807  1808  1809  1810  1811  1812  1813  1814  1815  1816  1817  1818  1819  1820  1821  1822  • Attribute or record swapping. Attributes or data values are swapped within a set  of similar records. For example, data that represent families in two similar towns  within a county might be swapped with each other. “Swapping has the additional  quality of removing any 100-percent assurance that a given record belongs to a given  household” [130, p.31] while preserving the accuracy of regional statistics, such as  sums and averages. In this case, the average number of children per family in the  county would be unaffected by data swapping. However, swapping may damage  or destroy important relationships within the data and introduce systematic biases,  depending on how the swapping candidates are selected.  • Noise infusion. Also called “partially synthetic data,” this approach adds small ran- dom values to attributes. For example, instead of reporting that a person is 84 years  old, the person may be reported as being 79 years old. Noise infusion increases  variance in reported statistics and leads to attenuation bias in estimated regression  coeffcients and correlations among attributes [36, 7]. When combined with a re- quirement for non-negative reporting of attributes, such as age or population, noise  infusion also introduces systematic bias since more values are increased in value than  decreased.  These techniques (and others) are described in detail in several publications, including:  • Statistical Policy Working Paper #22. (Second version, 2005) by the Federal Com- mittee on Statistical Methodology [47]. This 137-page paper includes worked exam- ples of disclosure limitation, specifc recommended practices for federal agencies,  profles of federal statistical agencies conducting disclosure limitation, and an ex- tensive bibliography. This document has been superseded by the Data Protection  Toolkit.  • The Data Protection Toolkit (BETA). A website maintained by the Federal Com- mittee on Statistical Methodology for the purpose of promoting data access while  protecting confdentiality throughout the federal statistical system [48]. https://nces.  ed.gov/fcsm/dpt  • The Anonymisation Decision-Making Framework. By Mark Elliot, Elaine MacKey,  Kieron O’Hara and Caroline Tudor, UKAN, University of Manchester, Manchester,  UK, 2016. This 156-page book provides tutorials and worked examples for de- identifying data and calculating risk.  • IHE IT Infrastructure Handbook: De-Identifcation. (Integrating the Health- care Enterprise, June 6, 2014) IHE offers a variety of guides, including one on de- identifcation at http://www.ihe.net/User Handbooks/.  Swapping and noise infusion both introduce noise into the dataset, such that records lit- erally contain incorrect data. Certain kinds of noise infusion have been mathematically  proven to provide formal privacy guarantees. Swapping has no such guarantees.  47  https://nces.ed.gov/fcsm/dpt https://nces.ed.gov/fcsm/dpt https://nces.ed.gov/fcsm/dpt http://www.ihe.net/User_Handbooks/  NIST SP 800-188 3pd  November 2022  1823  1824  1825  1826  1827  1828  1829  1830  1831  1832  1833  1834  1835  1836  1837  1838  1839  1840  1841  1842  1843  1844  1845  1846  1847  1848  1849  1850  1851  1852  1853  1854  1855  1856  1857  1858  1859  All of these techniques impact data accuracy, but whether they impact data utility depends  on the downstream uses of the data. For example, top-coding household incomes will not  impact a measurement of the 90-10 quantile ratio, but it will impact a measurement of the  top 1% of household incomes [99].  Prior to the adoption of differential privacy by the U.S. Census Bureau, federal statistical  agencies largely did not document the specifc statistical disclosure techniques they used  when performing statistical disclosure limitation. Likewise, statistical agencies did not  document the parameters used in the transformations nor the amount of data that have been  transformed, as documenting these techniques can allow a data intruder to reverse-engineer  the specifc values, eliminating privacy protection [7]. This lack of transparency sometimes  resulted in erroneous conclusions on the part of data users. This is another example of why  it is important for documentation of the de-identifcation process to accompany the release  of de-identifed data and is one of the motivations for the U.S. Census Bureau to to adopt  data privacy techniques that do not rely on secrecy for their effectiveness [56, 121, 58, 6].  4.3.4. De-Identifying Dates  Dates can exist in many ways in a dataset. Dates may be in particular kinds of typed  columns, such as a date of birth or the date of an encounter. Dates may be present as  a number, such as the number of days since an epoch like January 1, 1900. Dates may  be present in the free text narratives or in photographs (e.g., a photograph that shows a  calendar or a picture of a computer screen with date information).  Several strategies have been developed for de-identifying dates:  • Under the HIPAA Privacy Rule, dates must be generalized to no greater specifcity  than the year (e.g., July 4, 1776, becomes 1776).  • Dates within a single person’s record can be systematically adjusted by a random  amount. For example, dates of a hospital admission and discharge might be system- atically moved the same number of days – a date of admission and discharge of July  4, 1776, and July 9, 1776, become Sept. 10, 1777, and Sept. 15, 1777 [89]. However,  this does not eliminate the risk that a data intruder will make inferences based on the  interval between dates.  • In addition to a systematic shift, the intervals between dates can be perturbed to  protect against re-identifcation attacks that involve identifable intervals while still  maintaining the order of events.  • Some dates cannot be arbitrarily changed without compromising data accuracy. For  example, it may be necessary to preserve the day of the week, whether a day is a  workday or a holiday, or a relationship to a holiday or event.  • Some ages can be randomly adjusted without impacting data accuracy while others  cannot. For example, in many cases, the age of an individual can be randomly ad-  48  NIST SP 800-188 3pd  November 2022  1860  1861  1862  1863  1864  1865  1866  1867  1868  1869  1870  1871  1872  1873  1874  1875  1876  1877  1878  1879  1880  1881  1882  1883  1884  1885  1886  1887  1888  1889  1890  1891  1892  1893  1894  1895  justed ±2 years if the person is over the age of 25 but not if their age is between  one and three. However, individuals become eligible for specifc benefts at specifc  ages, such as Social Security retirement at age 62, so changes to ages around these  milestones may also result in data accuracy problems.  4.3.5. De-Identifying Geographical Locations and Geolocation Data  Geographical data can exist in many ways in a dataset. Geographical locations may be  indicated by map coordinates (e.g., 39.1351966, -77.2164013), a street address (e.g., 100  Bureau Drive), or a postal code (e.g., 20899). Geographical locations can also be embedded  in textual narratives.  Some geographical locations are not identifying (e.g., a crowded train station), while others  may be highly identifying (e.g., a house in which a single person lives). Other locations  may be identifying at some times of day and not others or during some months or some  years. The amount of noise required to de-identify geographical locations signifcantly  depends on the availability of external data, including geographical surveys. Identity may  be shielded in an urban environment by adding ±100 m, whereas a rural environment may  only require ±5 km or more to introduce suffcient ambiguity.  A prescriptive de-identifcation rule – even one that accounts for varying population densi- ties – may still be insuffcient for de-identifcation if the rule fails to consider the interaction  between geographic locations and other quasi-identifers in the dataset. Noise should be  added with caution to avoid the creation of inconsistencies in underlying data (e.g., mov- ing the location of a residence along a coast into a body of water or across geopolitical  boundaries).  Single locations may become identifying if they represent locations linked to a single in- dividual that are recorded over time (e.g., a work/home commuting pair). Such behavioral  time-location patterns can be quite distinct and allow for re-identifcation even with a small  number of recorded locations per individual [82, 81]. Research in 2021 concluded that  “[t]he risk of re-identifcation remains high even in country-scale location datasets” [46].  Data that are of higher resolution are typically more identifying. For example, in July  2021, the Catholic publication The Pillar published a report in which it had purchased  the de-identifed geolocation information for users of a homosexual dating platform. With  this data, the journalists identifed a prominent Catholic offcial as a user of the platform  by simply matching the geolocation data to the offcial’s offcial residence. The offcial  promptly resigned [100].  4.3.6. De-Identifying Genomic Information  Deoxyribonucleic acid (DNA) is the molecule inside human cells that carries genetic in- structions used for the proper functioning of living organisms. DNA present in the cell  49  NIST SP 800-188 3pd  November 2022  1896  1897  1898  1899  1900  1901  1902  1903  1904  1905  1906  1907  1908  1909  1910  1911  1912  1913  1914  1915  1916  1917  1918  1919  1920  1921  1922  1923  1924  1925  1926  1927  1928  1929  1930  1931  1932  1933  1934  nucleus is inherited from both parents, while DNA present in the mitochondria is only  inherited from an organism’s mother.  DNA is a repeating polymer that is made from four chemical bases: adenine (A), guanine  (G), cytosine (C), and thymine (T). Human DNA consists of roughly 3 billion bases, of  which 99% are the same in all people [54]. Modern technology allows for the complete  specifc sequence of an individual’s DNA to be chemically determined, although this is  rarely done in practice. With current technology, it is far more common to use a DNA  microarray to probe for the presence or absence of specifc DNA sequences at predeter- mined points in the genome. This approach is typically used to determine the presence  or absence of specifc single nucleotide polymorphisms (SNPs) [53]. DNA sequences and  SNPs are the same for monozygotic (identical) twins, individuals resulting from divided  embryos, and clones. With these exceptions, it is believed that no two humans have the  same complete DNA sequence.  Individual SNPs may be shared by many individuals, but a suffciently large number of  SNPs that show suffcient variability is generally believed to produce a combination that  is unique to an individual. Thus, there are some sections of the DNA sequence and some  combinations of SNPs that have high variability within the human population and oth- ers that have signifcant conservation between individuals within a specifc population or  group. When there is high variability, DNA sequences and SNPs can be used to match an  individual with a historical sample that has been analyzed and entered into a dataset. The  inheritability of genetic information has also allowed researchers to determine the surnames  and even the complete identities of some individuals [57].  As the number of individuals who have their DNA and SNPs measured increases, scientists  are realizing that the characteristics of DNA and SNPs in individuals may be more com- plicated than the preceding paragraphs imply. DNA changes as individuals age because of  senescence, transcription errors, and mutation. DNA methylation, which can impact the  functioning of DNA, also changes over time [17]. Individuals who are made up of DNA  from multiple individuals – typically the result of the fusion of twins in early pregnancy  – are known as chimera or mosaic. In 2015, a man in the United States failed a paternity  test because the genes in his saliva were different from those in his sperm [68]. A hu- man chimera was identifed in 1953 because the person’s blood contained a mixture of two  blood types: A and O [37]. The incidence of human chimeras is unknown.  Because of the high variability inherent in DNA, complete DNA sequences may be iden- tifable by linking with an external dataset. Likewise, biological samples for which DNA  can be extracted may be identifable. Subsections of an individual’s DNA sequence and  collections of highly variable SNPs may be identifable unless it is known that there are  many individuals who share the region of DNA or those SNPs. Furthermore, genetic infor- mation may not only identify an individual but could also identify an individual’s ancestors,  siblings, and descendants.  50  NIST SP 800-188 3pd  November 2022  Reading Level at Start of School Year # of Students  Below grade level 30-39  At grade level 50-59  Above grade level 20-29  Table 1. Reading levels at a hypothetical school, as measured by entrance examinations,  reported at the start of the school year on October 1.  Reading Level at Start of School Year # of Students  Below grade level 30-39  At grade level 50-59  Above grade level 30-39  Table 2. Reading levels at a hypothetical school, as measured by entrance examinations,  reported one month into the school year on November 1 after a new student has transferred to  the school.  1935  1936  1937  1938  1939  1940  1941  1942  1943  1944  1945  1946  1947  1948  1949  1950  1951  1952  1953  1954  1955  4.3.7. De-Identifying Text Narratives and Qualitative Information  Researchers must devote specifc attention when they de-identify text narratives and other  kinds of qualitative information. Many approaches developed in the 1980s and 1990s that  provided reasonable privacy assurances at the time may no longer provide adequate protec- tion in an era with high-quality internet search and social media [96, 97]. This is an area of  active research.  4.3.8. Challenges Posed by Aggregation Techniques  Aggregation does not necessarily provide privacy protection, especially when data are pre- sented in multiple data releases. Consider a hypothetical example of a school that reports  on its website the number of students performing below, at, and above grade level at the  start of the school year (table 1). Then consider that a new student enrolls at the school on  October 15, and the school updates the table on its website (table 2).  By comparing the two tables, it is possible to infer that the student who joined the school  is likely performing above grade level. This reveals protected information. Moreover, if  a person who views both tables knows the specifc student who enrolled in October, they  have learned a private fact about that student.  Aggregation does not inherently protect privacy, and thus aggregation alone is not suffcient  to provide formal privacy guarantees. However, the differential privacy literature does pro- vide methods for performing aggregation that are both formally private and highly accurate  when applied to large datasets. These methods work through the addition of carefully cali- brated noise.  51  NIST SP 800-188 3pd  November 2022  1956  1957  1958  1959  1960  1961  1962  1963  1964  1965  1966  1967  1968  1969  1970  1971  1972  1973  1974  1975  1976  1977  1978  1979  1980  1981  1982  1983  1984  1985  1986  1987  4.3.9. Challenges Posed by High-Dimensional Data  Even after removing all of the unique identifers and manipulating the quasi-identifers,  data can still be identifying if it is of suffciently high dimensionality and if there exists a  way to link the supposedly non-identifying values to an identity.24  4.3.10. Challenges Posed by Linked Data  Data can be linked in many ways. Pseudonyms allow data records from the same individual  to be linked together over time. Family identifers allow data from parents to be linked with  their children. Device identifers allow data to be linked to physical devices and potentially  link together all data coming from the same device. Data can also be linked to geographical  locations.  Data linkage increases the risk of re-identifcation by providing more attributes that can be  used to distinguish the true identity of a data record from others in the population. For ex- ample, survey responses that are linked together by household are more readily re-identifed  than survey responses that are not linked. Heart rate measurements may not be considered  identifying, but given a long sequence of tests, each individual in a dataset would have a  unique constellation of heart rate measurements, and the dataset could be susceptible to  being linked with another dataset that contains the same values.25 Geographical location  data can – when linked over time – create individual behavioral time-location patterns that  can be used to classify and identify unlabeled data, even with a small number of recorded  locations per individual [82, 81].  Dependencies between records may result in record linkages even when there is no explicit  linkage identifer. For example, it may be that an organization has new employees take a  profciency test within seven days of being hired. This information would allow links to be  drawn between an employee dataset that accurately reported an employee’s start date and a  training dataset that accurately reported the date that the test was administered, even if the  sponsoring organization did not intend for the two datasets to be linkable.  4.3.11. Challenges Posed by Composition  In computer science, the term composition refers to combining multiple functions to create  more complicated ones. One of the defning characteristics of complex systems is that they  have unpredictable behavior, even when they are composed of very simple components. A  challenge of composition is to develop approaches for limiting or eliminating such unpre- dictable behavior. Typically, this is done by proactively limiting the primitives that can be  24For example, consider a dataset of an anonymous survey that links together responses from parents and their  children. In such a dataset, a child might be able to fnd their parents’ confdential responses by searching  for their own responses and then following the link [84].  25This is a different approach than characterizing an individual’s heartbeat pattern so that it can be used as a  biometric. In this case, it is a specifc sequence of heartbeats that is recognized.  52  NIST SP 800-188 3pd  November 2022  1988  1989  1990  1991  1992  1993  1994  1995  1996  1997  1998  1999  2000  2001  2002  2003  2004  2005  2006  2007  2008  2009  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019  2020  2021  2022  composed. De-identifcation is such a primitive that statisticians and data scientists must  carefully control to ensure that the results of de-identifcation efforts can be composed.  Without such controls, the results of composition can become unpredictable.  Specifcally, it is important to understand whether the techniques used for de-identifying  will retain their privacy guarantees when they are subject to composition. For example, if  the same dataset is made available through two different de-identifcation regimes, what  will happen to the privacy guarantees if the two downstream datasets are recombined? One  of the primary advantages of differential privacy is that its operators are composable. This  is not true of most other de-identifcation techniques.  Composition concerns can arise when:  • The same dataset is provided to multiple downstream users.  • Snapshots of a dataset are published on a periodic basis.  • Changes in computer technology result in new aspects of a dataset being made avail- able.  • Legal proceedings require that aspects of the dataset (attributes or a subset of records)  are made available without de-identifcation.  Privacy risk can result from unanticipated composition, which is one of the reasons that  released datasets should be subjected to periodic review and reconsideration.  4.3.12. Potential Failures of De-Identifcation  The de-identifcation process outlined in this section can fail to prevent a disclosure for a  number of different reasons. In addition, failures of data utility can also occur, in which  the de-identifcation process removes too much information, and the de-identifed dataset  is not useful for its intended purpose.  • If an inappropriate risk threshold is selected, then the risk of re-identifcation may  be higher than intended. Agencies should select risk thresholds conservatively to  address this issue.  • If direct or quasi-identifers are missed, then identifying information may remain in  the de-identifed dataset, leading to increased re-identifcation risk. Agencies should  be mindful of the ways in which personal information can be used to identify indi- viduals and – in ambiguous situations – assume that such information is identifying.  • If threats are missed during threat modeling, then the re-identifcation risk could  be higher than intended. In particular, if other datasets that could be linked with  the de-identifed dataset are not considered, then the risk could be much higher than  anticipated. Agencies should carefully consider existing and future data releases  during threat modeling.  53  NIST SP 800-188 3pd  November 2022  2023  2024  2025  2026  2027  2028  2029  2030  2031  2032  2033  2034  2035  2036  2037  2038  2039  2040  2041  2042  2043  2044  2045  2046  2047  2048  2049  2050  2051  2052  2053  2054  2055  2056  2057  2058  • If the selected transformations fail to remove identifying information, then the  risk of de-identifcation could be higher than intended. Agencies should select trans- formations with well-understood properties and a history of successful use.  • If the de-identifed dataset does not produce accurate results for its intended use,  then it may not satisfy the goals of the data release. Future data custodians may be  forced to oversee additional data releases, and those future releases might be com- bined with the already released datasets in ways that are unforeseen. Agencies should  understand how the de-identifed data will be used and make sure to carefully evalu- ate its utility for those purposes before releasing it.  4.3.13. Post-Release Monitoring  Following the release of a de-identifed dataset, the releasing agency should monitor it to  ensure that the assumptions made during the de-identifcation remain valid. This is because  the identifability of a dataset can only increase over time. For example, the de-identifed  dataset may contain information that can be linked to an internal dataset that is later the  subject of a data breach. In such a situation, the data breach could also result in the re- identifcation of the de-identifed dataset. The de-identifed dataset might also be linked  to an external dataset released by a completely separate organization. Agencies have no  control over the release of such datasets, and even monitoring may be challenging in this  situation. In some cases, the de-identifed dataset might be linked with privately held data,  making monitoring impossible.  Agencies may wish to make releasing units responsible for post-release monitoring or to  centralize the post-release monitoring in a single location. However, proper post-release  monitoring requires knowledge of the datasets that have been released and the kinds of data  that would allow for a re-identifcation attack. These requirements are likely to increase  costs to organizations that wish to delegate post-release monitoring to other organizations  or third parties. One way to decrease the requirement for post-release monitoring is to  perform the de-identifcation using a formal privacy model (e.g., differential privacy) that  provides for privacy without making assumptions about background information available  to the data intruder.  4.4. Synthetic Data  An alternative to de-identifying using the technique presented in the previous section is to  use the original dataset to create a synthetic dataset [35, p.8].  Synthetic data can be created by two approaches:  1. Sampling an existing dataset and either adding noise to specifc cells likely to have a  high risk of disclosure or replacing those cells with imputed values. This is known  as a “partially synthetic” dataset (see Table 3).  54  NIST SP 800-188 3pd  November 2022  Data adjective Description  Datasets without formal guarantees:  Partially synthetic Data for which there may be one-to-one mappings between records  in the original dataset and the synthetic dataset but for which some  attributes may have been altered or swapped between records. This  approach is sometimes called blank-and-impute.  Datasets with formal guarantees if the original dataset is not used to create the data:  Test Data that resemble the original dataset in terms of structure and  the range of values but for which there is no attempt to ensure that  inferences drawn on the test data will be like those drawn on the  original data. Test data may also include extreme values that are  not in the original data but are present for testing software.  Realistic Data that have a characteristic that is like the original data but that  is not developed by modifying original data and which contains no  information that is privacy-sensitive.  Datasets with formal guarantees when formal techniques are used:  Fully synthetic Data for which there is no one-to-one mapping between any record  in the original dataset and the synthetic dataset.  Table 3. Adjectives used for describing data in data releases.  2059  2060  2061  2062  2063  2064  2065  2066  2067  2068  2069  2070  2. Using the existing dataset to create a model and then using that model to create a  synthetic dataset. This is known as a “fully synthetic” dataset (see Table 3).  In both cases, formal privacy techniques can be used to quantify the privacy protection  offered by the synthetic dataset.  4.4.1. Partially Synthetic Data  A partially synthetic dataset is one in which some of the data have been altered from the  original dataset using probabilistic models. For example, data that belong to two families  in adjoining towns may be swapped to protect the identity of the families. Alternatively, the  data for an outlier variable may be removed and replaced with a range value that is incorrect  (e.g., replacing the value “60” with the range “30-35”). It is considered best practice for  the data publisher to indicate that some values have been modifed or otherwise imputed  but not to reveal the specifc values that have been modifed.  55  NIST SP 800-188 3pd  November 2022  2071  2072  2073  2074  2075  2076  2077  2078  2079  2080  2081  2082  2083  2084  2085  2086  2087  2088  2089  2090  2091  2092  2093  2094  2095  2096  2097  2098  2099  2100  2101  2102  2103  2104  2105  4.4.2. Test Data  It is also possible to create test data that is syntactically valid but does not convey accu- rate information when analyzed. Such data can be used for software development. When  creating test data, it is useful for the names, addresses, and other information in the data to  be conspicuously non-natural so that the test data are not inadvertently confused with true  confdential data. For example, use the name “FIRSTNAME1 LASTNAME2” rather than  “JOHN SMITH.”  4.4.3. Fully Synthetic Data  A fully synthetic dataset is a dataset for which there is no one-to-one mapping between  data in the original dataset and data in the de-identifed dataset. One approach to creating a  fully synthetic dataset is to use the original dataset to create a high-fdelity model and then  to use a simulation to produce individual data elements that are consistent with the model.  Special efforts must be taken to maintain marginal and joint probabilities when creating  partially or fully synthetic data.  Fully synthetic datasets cannot provide more information to the downstream user than was  contained in the original model. Nevertheless, some users may prefer to work with the fully  synthetic dataset instead of the model for a variety of reasons:  • Synthetic data provides users with the ability to develop queries and other techniques  that can be applied to the real data without exposing real data to users during the  development process. The queries and techniques can then be provided to the data  owner, who can run the queries or techniques on the real data and provide the results  to the users.  • Many hypotheses not represented exactly in the original model may be informed by  the synthetic data because they are correlated with hypotheses (effects) that are in the  model.  • Some users may place more trust in a synthetic dataset than in a model.  • When researchers form their hypotheses from synthetic data and then verify their  fndings on actual data, they can be protected from pretest estimation and false- discovery bias [7, p.257].  Because of the possibility of false discovery, analysts should be able to validate their dis- coveries against the original data to ensure that the things they discover are in the original  data and not artifacts of the data generation process.  Both high-fdelity models and synthetic data generated from models may leak personal  information that is potentially re-identifable. The amount of leakage can be controlled us- ing formal privacy models (e.g., differential privacy) that typically involve the introduction  56  NIST SP 800-188 3pd  November 2022  2106  2107  2108  2109  2110  2111  2112  2113  2114  2115  2116  2117  2118  2119  2120  2121  2122  2123  2124  2125  2126  2127  2128  2129  2130  2131  2132  2133  2134  2135  2136  2137  2138  2139  2140  2141  of noise. Section 4.4.6 describes the construction of fully synthetic data with differential  privacy.  There are several advantages for agencies that choose to release de-identifed data as a fully  synthetic dataset:  • It can be very diffcult or even impossible to map records to actual people.  • The privacy guarantees can potentially be mathematically established and proven (cf.  the section below on “Creating a synthetic dataset with differential privacy”).  • The privacy guarantees can remain in force even if there are future data releases.  Fully synthetic data also have these disadvantages and limitations:  • It is not possible to create pseudonyms that map back to actual people because the  records are fully fabricated.  • The data release may be less useful for accountability or transparency. For example,  investigators equipped with a synthetic data release would be unable to fnd the actual  “people” who make up the release because they would not actually exist.  • It is diffcult to fnd meaningful correlations or abnormalities in synthetic data that  are not represented in the model. For example, if a model contains only main effects  and frst-order interactions, then all second-order interactions can only be estimated  from the synthetic data to the extent that their design is correlated with the main or  frst-order interactions.  • Users of the data may not realize that the data are synthetic. Simply providing doc- umentation that the data are fully synthetic may not be suffcient public notifcation  since the dataset may be separated from the documentation. Instead, it is best to  indicate in the data itself that the values are synthetic. For example, names like  “SYNTHETIC PERSON” or “FIRSTNAME1 LASTNAME1” may be placed in the  data.  • Releasing a synthetic dataset may not be regarded by the public as a legitimate act of  transparency, or the public may question the validity of the data based on its perceived  lack of relationship to the original dataset. These concerns can be addressed with  public education and by documenting the accuracy of the synthetic dataset.  In addition, it can be extremely challenging to construct the high-fdelity models that enable  good synthetic datasets. The best known techniques for constructing these models are  designed around ensuring that specifc properties of the data (e.g., correlations between  certain data attributes) are preserved when the model is constructed. Models constructed  this way may not necessarily refect other properties that were present in the original data.  It is often possible to construct very high-fdelity models when the desirable properties  of the synthetic data are known in advance (e.g., when it is known what questions future  57  NIST SP 800-188 3pd  November 2022  2142  2143  2144  2145  2146  2147  2148  2149  2150  2151  2152  2153  2154  2155  2156  2157  2158  2159  2160  2161  2162  2163  2164  2165  2166  2167  2168  2169  2170  2171  2172  2173  2174  2175  2176  analysts will want to answer using the synthetic data). Constructing synthetic data that  faithfully represents all properties of the original data is much more challenging.  4.4.4. Synthetic Data with Validation  Agencies that share or publish synthetic data can optionally provide a validation service  that takes queries or algorithms developed with synthetic data and applies them to actual  data. The results of these queries or algorithms can then be compared with the results  of running the same queries on the synthetic data, and the researchers can be warned if  the results are different. Alternatively, results can be provided to the researchers after the  application of additional statistical disclosure limitation.  4.4.5. Synthetic Data and Open Data Policy  Releases of synthetic data can be confusing to the lay public.  • It may not be clear to data users that the synthetic data release is actually synthetic.  Members of the public may assume instead that the synthetic data are simply an  operational dataset that has had identifying columns suppressed.  • Synthetic data may contain synthetic individuals who appear similar to actual indi- viduals in the population.  • Fully synthetic datasets do not have a zero-disclosure risk because they still contain  information derived from non-public information about individuals. The disclosure  risk may be greater when synthetic data are created with traditional statistical mod- eling or data imputing techniques rather than those based on formal privacy models,  such as differential privacy, as the formal models have provisions for tracking the  accumulated privacy loss that results from multiple data operations, as discussed in  Section 4.4.6.  4.4.6. Creating a Synthetic Dataset with Diferential Privacy  A growing number of mathematical algorithms have been developed for creating syn- thetic datasets that meet the mathematical defnition of privacy provided by differential  privacy [40]. Most of these algorithms will transform a dataset containing private data into  a new dataset that contains synthetic data that nevertheless provides reasonably accurate  results in response to a variety of queries. However, there is no algorithm or implemen- tation currently in existence that can be used by a person who is unskilled in the area of  differential privacy.  The idea of differential privacy is that the result of a data analysis function κ applied to a  dataset should not change very much if an arbitrary person p’s data is added to or removed  from a dataset D. That is, κ(D) ≈ κ(D − p). The degree to which the two values are  approximately equal is determined by the privacy loss parameter ε .  58  NIST SP 800-188 3pd  November 2022  2177  2178  2179  2180  2181  2182  2183  2184  2185  2186  2187  2188  2189  2190  2191  2192  2193  2194  2195  2196  2197  2198  2199  2200  2201  2202  2203  2204  2205  2206  2207  2208  2209  2210  2211  2212  2213  2214  In the mathematical formulation of differential privacy, ε can range from 0 to ∞. When  ε = 0, the output of κ does not depend on the input dataset. When ε = ∞, the output of κ  is entirely dependent upon the input dataset, such that changing a single record results in  an unambigous measurable change in κ’s output. Thus, larger values of ε provide for more  accuracy but result in increased privacy loss.  When ε is set appropriately, differential privacy limits the privacy loss that a data subject  experiences from the use of their private data to the maximum privacy loss necessary for  a given statistical purpose. Note that this particular notion of privacy does not protect all  secrets about a person. It only protects the secrets that an observer would not have been  able to learn if the person’s data was not present in the dataset. Stated another way, differ- ential privacy protects individuals from additional harm resulting from their participation  in the data analysis but does not protect them from harm that would have occurred even  if their data were not present. For example, if a study concludes that residents of Ver- mont overwhelmingly drive 4-wheel-drive vehicles, one might conclude that a particular  Vermonter drives a 4-wheel-drive vehicle even if that individual did not participate in the  study. Differential privacy does not attempt to prevent inferences of this type.  Many academic papers on differential privacy assume a value of 1.0 for ε but do not explain  the rationale of the choice. Some researchers working in the feld of differential privacy  have tried mapping existing privacy regulations to the choice of ε , but these efforts invari- ably result in values of ε = 1. Principled approaches for setting ε is a subject of current  academic research [72].  There are relatively few scholarly publications regarding the deployment of differential pri- vacy in real-world situations, and there are few papers that provide guidance in choosing  appropriate values of ε . Thus, agencies that are interested in using differential privacy al- gorithms to allow for querying of sensitive datasets or the creation of synthetic data should  ensure that the techniques are appropriately implemented and that the privacy protections  are appropriate to the desired application.  4.5. De-Identifying with an Interactive Query Interface  Another model for granting public access to de-identifed agency information is to construct  an interactive query interface that allows members of the public or qualifed investigators to  run queries over the agency’s dataset. This option has been developed by several agencies,  and there are many ways that it can be implemented. For example:  • If the queries are run on actual data, the results can be altered through the injection  of noise to protect privacy, potentially satisfying a formal privacy model such as  differential privacy. Alternatively, individual queries can be reviewed by agency staff  to verify that privacy thresholds are maintained.  • Queries can be run on synthetic data. In this case, the agency can also run queries  on the actual data and warn the external researchers if the queries run on synthetic  ̸  59  NIST SP 800-188 3pd  November 2022  2215  2216  2217  2218  2219  2220  2221  2222  2223  2224  2225  2226  2227  2228  2229  2230  2231  2232  2233  2234  2235  2236  2237  2238  2239  2240  2241  2242  2243  2244  2245  data deviate signifcantly from the queries run on the actual data (ensuring that the  warning itself does not compromise the privacy of some individual).  • Query interfaces can be made freely available on the public internet, or they can be  made available in a restricted manner to qualifed researchers operating in secure  locations.  A signifcant privacy risk with interactive queries is that each query results in additional  privacy loss [33].26 For this reason, query interfaces should also log both queries and  query results in order to deter and detect malicious use.  One of the advantages of synthetic data is that the privacy loss budget can be spent on  creating the synthetic dataset rather than on responding to interactive queries.  4.6. Validating a De-Identifed Dataset  Agencies should validate datasets after they are de-identifed to ensure that the resulting  dataset meets the agency’s goals in terms of both data usefulness and privacy protection.  4.6.1. Validating Data Usefulness  De-identifcation decreases data accuracy and the usefulness of the resulting dataset. It is  therefore important to ensure that the de-identifed dataset is still useful for the intended  application. Otherwise, there is no reason to go through the expense and added risk of  de-identifcation.  Several approaches exist for validating data usefulness. For example, insiders can perform  statistical calculations on both the original dataset and the de-identifed dataset and compare  the results to see if the de-identifcation resulted in unacceptable changes. Agencies can  engage trusted outsiders to examine the de-identifed dataset and determine whether the  data could be used for the intended purpose.  Recognizing that there is an inherent trade-off between data accuracy and privacy protec- tion, agencies can adopt accuracy goals for the data that they make available to a broad  audience. An accuracy goal specifes how accurate data must be in order to be ft for an  intended use. Limiting data accuracy to this goal is an important technique for protecting  the privacy of data subjects.  4.6.2. Validating Privacy Protection  Several approaches exist for validating the privacy protection provided by de-identifcation,  including:  26If a fnite privacy loss budget is allocated, the data controller needs to respond by increasing the amount of  noise added to each response, accepting a higher level of privacy risk, or ceasing to answer questions as the  budget nears exhaustion. This can result in equity issues if the frst users to query the dataset obtain better  answers than later users.  60  NIST SP 800-188 3pd  November 2022  2246  2247  2248  2249  2250  2251  2252  2253  2254  2255  2256  2257  2258  2259  2260  2261  2262  2263  2264  2265  2266  2267  2268  2269  2270  2271  2272  2273  2274  • Examining the resulting data fles to make sure that no identifying information is  unintentionally included in fle data or metadata.  • Examining the resulting data fles to make sure that the data meet stated goals for  ambiguity under a k-anonymity model, if such a standard is desired.  • Critically evaluating all default assumptions used by software that performs data  modifcation or modeling.  • Conducting a motivated intruder test to see if reasonably competent outside indi- viduals can perform re-identifcation using publicly available datasets, commercially  available datasets, or even private datasets that might be available to certain data  intruders. Motivations for an intruder can include prurient interest, causing embar- rassment or harm, revealing private facts about public fgures, or engaging in a rep- utation attack. Details for how to conduct a motivated intruder test can be found in  Anonymisation: Managing data protection risk code of practice, published by the  United Kingdom’s Information Commissioner’s Offce [63].  • Providing the team conducting the motivated intruder test with some confdential  agency data to understand how a data intruder might be able to take advantage of  data leaked as a result of a breach or a hostile insider.  These approaches do not provide provable guarantees on the protection offered by de- identifcation, but they may be useful as part of an overall agency risk assessment.27 Ap- plications that require provable privacy guarantees should rely on formal privacy methods,  such as differential privacy, when planning their data releases.  Validating the privacy protection of de-identifed data is greatly simplifed by using vali- dated de-identifcation software, as discussed in Section 5, “Evaluation.”  4.6.3. Re-Identifcation Studies  Re-identifcation studies are motivated intruder tests. These studies can identify issues that  would allow external actors to successfully re-identify de-identifed data. Re-identifcation  studies look for vulnerabilities in a dataset that could be used for re-identifying data sub- jects. They do not determine whether someone with intimate knowledge of a specifc re- spondent can fnd that respondent in the database. The only way to protect a single specifc  27Although other documents that discuss de-identifcation use the term risk assessment to refer to a specifc  calculation of ambiguity using the k-anonymity de-identifcation model, this document uses the term risk  assessment to refer to a much broader process. Specifcally, risk assessment is defned as, “The process  of identifying, estimating, and prioritizing risks to organizational operations (including mission, functions,  image, reputation), organizational assets, individuals, other organizations, and the Nation, resulting from  the operation of an information system. Part of risk management incorporates threat and vulnerability  analyses and considers mitigations provided by security controls planned or in place. Synonymous with  risk analysis” [23].  61  NIST SP 800-188 3pd  November 2022  2275  2276  2277  2278  2279  2280  2281  2282  2283  2284  2285  2286  2287  2288  2289  2290  2291  2292  2293  2294  2295  2296  2297  2298  2299  2300  2301  2302  2303  2304  2305  2306  2307  2308  2309  2310  2311  2312  individual perceived to be at high risk of re-identifcation is through data perturbation (e.g.,  noise injection) or information reduction (e.g., removing the observation altogether).  The key statistic calculated in re-identifcation studies is the conditional re-identifcation  rate. This statistic is a proxy for disclosure risk. The rate is the number of confrmed links  between the dataset and another dataset divided by the number of putative (suspected)  links, unduplicated by “defender” ID, expressed as a percentage. If the conditional re- identifcation rate falls above an agreed upon threshold for any publication strata, it suggests  that the data should not be released outside of a controlled environment.  Re-identifcation studies are often an iterative process. If a re-identifcation study uncovers  problems with the de-identifed data, the data curator can engage with subject matter ex- perts, make changes to the dataset, and perform another re-identifcation study. Changes to  the dataset might involve coarsening linking variables, eliminating highly disclosive link- ing variables from the microdata to be released, or coarsening strata. This continues until  the study concludes that the de-identifed data can be disseminated.  There are two very different types of re-identifcation studies:  1. Micro (or targeted) re-identifcation studies, where one is looking for a specifc  person. A well-known example is that of former Governor William Weld of Mas- sachusetts, whose medical records in a hospital discharge summary were record  linked to voter records [16]. As noted earlier, individual targets are supremely hard  to protect as there is often extensive publicly available information about them.  2. Macro (or wholesale) re-identifcation studies, where one seeks to embarrass or  discredit the organization releasing the data. This is accomplished by linking easily  procurable external intruder data to the protected microdata that are being released.  Several metrics can be calculated to uncover putative links, and several methods can  be used to confrm putative links. Python has record linkage objects that probabilis- tically link fles using a wide variety of metrics.  Formal privacy parameters often appear opaque and elusive to non-theoreticians. Subject  matter experts and decision-makers more clearly understand disclosure risk after reviewing  the results of re-identifcation studies.  External intruders may calculate low or high suspected re-identifcation rates, given the  information they have available to them. They may even purport to have successfully linked  their external data to a de-identifed dataset. By conducting a re-identifcation study a  priori, those seeking to disseminate the de-identifed data know how successful the external  intruder’s re-identifcation attempt was if both parties have access to the same external  internal data.  The conditional re-identifcation rate is identical to the metric of precision in the record  linkage and health science literature. It represents the ratio of true positives to the sum  of true positives and false positives. Data owners should not be alarmed if an external  62  NIST SP 800-188 3pd  November 2022  2313  2314  2315  2316  2317  2318  2319  2320  2321  2322  2323  2324  2325  2326  2327  2328  2329  2330  2331  2332  2333  2334  2335  2336  2337  2338  2339  2340  2341  2342  2343  2344  2345  2346  2347  2348  organization reports a relatively high suspected re-identifcation rate as long as they know  that the conditional re-identifcation rate is low [45, 59, 111].  Confrmed re-identifcation rates are defned in Section 3.2.1 as re-identifcation probabil- ities. On its own, a low confrmed re-identifcation probability does not indicate that an or- ganization should disseminate a de-identifed dataset. Even when a confrmed rate is low,  a high conditional rate should direct an organization to not disseminate the de-identifed  microdata.  Re-identifcation studies may identify problems that can direct improvements to any or- ganization’s disclosure avoidance methods. Re-identifcation studies are not designed to  replace legacy or modern provable privacy methods but to act as a quality control to vali- date that the methods – old and new – protect as they were designed.  5. Software Requirements, Evaluation, and Validation  Agencies should clearly defne the requirements for de-identifcation algorithms and the  software that implements those algorithms. They should be sure that the algorithms that  they intend to use are validated, that the software implements the algorithms as expected,  and that the data that result from the operation of the software are correct.  Today, there a growing number of algorithms and tools for performing de-identifcation,  data masking, and performing other privacy-preserving operations. NIST maintains a list of  some of these tools at https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/  collaboration-space/focus-areas/de-id/tools. Such tools are also increasingly being eval- uated in academic literature [116] and by NIST [107, 1], although there are no widely  accepted performance standards or certifcation procedures at present.  5.1. Evaluating Privacy-Preserving Techniques  There have been decades of research in the feld of statistical disclosure limitation and  de-identifcation, and understanding in the feld has evolved over time. Agencies should  not base their technical evaluation of a technique solely on the fact that the technique has  been published in peer-reviewed literature or that the agency has a long history of using  the technique and has not experienced any problems. Instead, it is necessary to evaluate  proposed techniques through the totality of scientifc experience and with regard to current  threats.  Traditional statistical disclosure limitation and de-identifcation techniques base their risk  assessments – in part – on an expectation of what kinds of data are available to a data  intruder to conduct a linkage attack. Where possible, these assumptions should be docu- mented and published along with a description of the privacy-preserving techniques that  were used to transform the datasets prior to release so that they can be reviewed by external  experts and the scientifc community.  63  https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/tools https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/tools https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/tools  NIST SP 800-188 3pd  November 2022  2349  2350  2351  2352  2353  2354  2355  2356  2357  2358  2359  2360  2361  2362  2363  2364  2365  2366  2367  2368  2369  2370  2371  2372  2373  2374  2375  2376  2377  2378  2379  2380  2381  Because our understanding of privacy technology and the capabilities of privacy attacks  are rapidly evolving, techniques that have been previously established should be periodi- cally reviewed. New vulnerabilities may be discovered in techniques that have been pre- viously accepted. Alternatively, new techniques may be developed that allow agencies to  re-evaluate the trade-offs they have made with respect to privacy risk and data usability.  5.2. De-Identifcation Tools  A de-identifcation tool is a program that is involved in the creation of de-identifed datasets.  5.2.1. De-Identifcation Tool Features  De-identifcation tools may perform many functions, including:  • Detecting identifying information  • Calculating re-identifcation risk  • Performing de-identifcation  • Mapping identifers to pseudonyms  • Providing for the selective revelation of pseudonyms  De-identifcation tools may handle a variety of data modalities. For example, tools may  be designed for tabular data or for multimedia. Tools may attempt to de-identify all data  types or be developed for specifc modalities. A potential risk of using de-identifcation  tools is that a tool could be equipped to handle some but not all of the different modalities  in a dataset. For example, a tool could de-identify the categorical information in a table  according to a de-identifcation standard but might not detect or attempt to address the  presence of identifying information in a text feld. For this reason, de-identifcation tools  should be validated for the specifc kinds of data that the agency intends to use.  5.2.2. Data Provenance and File Formats  Output fles created by de-identifcation tools and data masking tools can record provenance  information, such as metadata regarding input datasets, the de-identifcation methods used,  and the resulting decrease in data accuracy. Output fles can also be explicitly marked to in- dicate that they have been de-identifed. For example, de-identifcation profles that are part  of the Digital Imaging and Communications in Medicine (DICOM) specifcation indicate  which elements are direct versus quasi-identifers and which de-identifcation algorithms  have been employed [32, Appendix E, “Attribute Confdentiality Profles”].  5.2.3. Data Masking Tools  Data masking tools are programs that can remove or replace designated felds in a dataset  while maintaining relationships between tables. These tools can be used to remove direct  64  NIST SP 800-188 3pd  November 2022  2382  2383  2384  2385  2386  2387  2388  2389  2390  2391  2392  2393  2394  2395  2396  2397  2398  2399  2400  2401  2402  2403  2404  2405  2406  2407  2408  2409  2410  2411  2412  2413  2414  2415  identifers but generally cannot identify or modify quasi-identifers in a manner consistent  with a privacy policy or risk analysis.  Data masking tools were developed to allow software developers and testers access to  datasets that contain realistic data while providing minimal privacy protection. Absent  additional controls or data manipulations, data masking tools should not be used for the  de-identifcation of datasets that are intended for public release nor as the sole mechanism  to ensure confdentiality in non-public data sharing.  5.3. Evaluating De-Identifcation Software  Once techniques are evaluated and approved, agencies should ensure that the techniques are  faithfully executed by their chosen software. Privacy software evaluation should consider  the trade-off between data usability and privacy protection. Privacy software evaluation  should also seek to detect and minimize the chances of tool error and user error.  For example, agencies should verify:  • Correctness. The software properly implements the chosen algorithms.  • Containment. The software does not leak identifying information in expected or  unexpected ways, such as through the inaccuracies of foating-point arithmetic or the  differences in execution time (if observable to a data intruder).  • Usability. The software can be operated effciently and with minimal error, and users  can detect and correct errors when they happen.  Agencies should also evaluate the performance of the de-identifcation software, such as:  • Effciency. How long does it take to run on a dataset of a typical size?  • Scalability. How much does it slow down when moving from a dataset of N to 100N?  • Repeatability. If the tool is run twice on the same dataset, are the results similar? If  two different people run the tool, do they get similar results?  Ideally, software should be able to track the accumulated privacy leakage from multiple  data releases.  5.4. Evaluating Data Accuracy  Finally, agencies should evaluate the accuracy of the de-identifed data to verify that it is  suffcient for the intended use. For example, researchers at MIT and Harvard applied k- anonymity de-identifcation to educational data collected by a massive open online course  operated by MITx and HarvardX on the edX platform and found that de-identifcation  resulted in meaningful biases that changed the meaning of some statistics. For example, in  one case, de-identifcation decreased the number of enrolled female students from 29% to  26% because of the need to suppress attributes for specifc microdata [30].  65  NIST SP 800-188 3pd  November 2022  2416  2417  2418  2419  2420  2421  2422  2423  2424  2425  2426  2427  2428  2429  2430  2431  2432  2433  2434  2435  2436  2437  2438  2439  2440  2441  2442  2443  2444  2445  2446  2447  2448  2449  2450  The feld of statistical disclosure control has developed approaches for gauging the impact  of SDC techniques on microdata [142]. The literature examines the mathematical impact  of SDC procedures (e.g., sampling, recoding, suppression, rounding, and noise infusion)  and computes the possible impact on various statistical measurements.  Approaches for evaluating data accuracy include [71]:  • Demonstrating that machine learning algorithms trained on the de-identifed data can  accurately predict the original data and vice versa  • Verifying that statistical distributions do not incur undue bias because of the de- identifcation procedure  • Publishing suffcient information about the statistical properties of the disclosure lim- itation methods to permit the correction of inferences using these properties  Agencies can create or adopt standards regarding the accuracy of de-identifed data. If  data accuracy cannot be well-maintained along with data privacy goals, then the release of  data that is inaccurate for statistical analyses could potentially result in incorrect scientifc  conclusions and incorrect policy decisions.  6. Conclusion  Government agencies can use de-identifcation technology to make datasets available to  researchers and the public without compromising the privacy of the people contained within  the data.  There are currently three primary models available for de-identifcation:  1. agencies can make data available with traditional de-identifcation techniques that  rely on the suppression of identifying information (direct identifers) and the manip- ulation of information that partially identifes (quasi-identifers);  2. agencies can create synthetic datasets; and  3. agencies can make data available through a query interface.  These models can be mixed within a single dataset to provide different kinds of access for  different users or intended uses.  Privacy protection can be strengthened when agencies employ formal models for privacy  protection, such as differential privacy, because the mathematical models that these sys- tems use are designed to ensure privacy protection irrespective of future data releases or  developments in re-identifcation technology. However, the mathematics underlying these  systems is very new, and there is little experience within the Government in using these  systems. Thus, agencies should understand the implications of these systems before de- ploying them in place of traditional de-identifcation approaches that do not offer formal  privacy guarantees.  66  NIST SP 800-188 3pd  November 2022  2451  2452  2453  2454  2455  2456  2457  2458  2459  Agencies that use de-identifcation should establish appropriate governance structures to  support de-identifcation, data release, and post-release monitoring. Such structures will  typically include a Disclosure Review Board as well as appropriate education, training,  and research efforts.  A summary of this document’s advice for practitioners appears in Figure 5.  In closing, it is important to remember that different jurisdictions may have different stan- dards and policies regarding the defnition and use of de-identifed data. Information that  is considered de-identifed in one jurisdiction may be regarded as being identifable in an- other.  67  NIST SP 800-188 3pd  November 2022  Governance and Management (Section 3) The management of de-identifcation in- cludes identifying the goals of the de-identifcation process and considering risks  to participants in the data release. To guide this process, this document describes  several tools:  • Consider all phases of the Data Life Cycle (Section 3.3).  • Consider different Data Sharing Models (Section 3.4), including complemen-  tary protections like Data Use Agreements, Synthetic Data, and Enclaves.  • Leverage the Five Safes (Section 3.5), a methodology for evaluating risk.  • Form a Disclosure Review Board (Section 3.6) to oversee the implementation  of de-identifcation policies.  • Follow existing de-identifcation standards when possible (Section 3.7).  Technical Steps (Section 4) The technical process of de-identifcation should leverage the  best practices developed over the past several decades. In particular, NIST recom- mends that agencies:  • Conduct a Data Survey (Section 4.2) to identify de-identifcation requirements  specifc to the data.  • Determine identifers and quasi-identifers in the data, and select a method for  de-identifying each one (Section 4.3).  • Consider the existing auxiliary data (Section 4.3) that could be used to enable  a re-identifcation attack.  • Practice defense in depth by combining security measures with de- identifcation when possible, and consider using Synthetic Data (Section 4.4)  or an Interactive Query Interface (Section 4.5).  • When possible, use formal privacy techniques to quantify privacy loss asso- ciated with the release of de-identifed data (Section 4.4.6).  • Validate the utility and privacy of the de-identifed data (Section 4.6). In par- ticular, establish accuracy goals for de-identifcation data so that the data is not  more accurate than required for the intended purpose.  Software (Section 5) In general, agencies should:  • Utilize automated, repeatable, software-based approaches for performing de-  identifcation.  • Carefully consider the software used to implement de-identifcation to ensure  that the algorithms used have been validated and that the software correctly  implements those algorithms.  • Consider the effciency, scalability, and repeatability properties of software  tools, and evaluate the accuracy of the tool’s output.  Fig. 5. Advice for Practitioners: A Summary  68  NIST SP 800-188 3pd  November 2022  2460  2461  2462  2463  2464  2465  2466  2467  2468  2469  2470  2471  2472  2473  2474  2475  2476  2477  2478  2479  2480  2481  2482  2483  2484  2485  2486  2487  2488  2489  2490  2491  2492  2493  2494  2495  References  [1] July 2020. URL: https://www.nist.gov/blogs/cybersecurity- insights/differential- privacy-privacy-preserving-data-analysis-introduction-our.  [2] 115th Congress (2017–2018). Public Law 115-435: The Foundations for Evidence- Based Policymaking Act of 2018. 2018. URL: https://www.congress.gov/bill/115th- congress/house-bill/4174.  [3] 45 CFR 164 Health Insurance Portability and Accountability Act of 1996 (HIPAA)  Privacy Rule Safe Harbor method Standard: De-identifcationof protected health  information.  [4] 81 FR 49689: Revision of OMB Circular No. A-130, “Managing Information as a  Strategic Resource. July 2016. URL: https://www.cio.gov/policies-and-priorities/  circular-a-130/.  [5] 95th Congress. Public Law 95-416. Oct. 1978. URL: https : / / www. census . gov /  history/pdf/NARA Legislation.pdf.  [6] John Abowd et al. “The 2020 Census Disclosure Avoidance System TopDown Al- gorithm”. In: Harvard Data Science Review Special Issue 2 (June 2022). URL:  https://hdsr.mitpress.mit.edu/pub/7evz361i.  [7] John M. Abowd and Ian M. Schmutte. Economic Analysis and Statistical Dis- closure Limitation. Mar. 2015. URL: https: / /www.brookings.edu/bpea- articles /  economic-analysis-and-statistical-disclosure-limitation/.  [8] John M. Abowd and Lars Vilhuber. “How Protective are Synthetic Data?” In: Lec- ture Notes in Computer Science: Privacy in Statistical Databases 5262 (2008),  pp. 239–246.  [9] “Accuracy”. In: Glossary of Statistical Terms (Sept. 2001). Last accessed June 23,  2022. URL: https://stats.oecd.org/glossary/detail.asp?ID=21.  [10] Charu C. Aggarwal. “On K-Anonymity and the Curse of Dimensionality”. In: Pro- ceedings of the 31st International Conference on Very Large Data Bases. VLDB  ’05. Trondheim, Norway: VLDB Endowment, 2005, pp. 901–909. ISBN: 1595931546.  [11] J. Trent Alexander, Michael Davern, and Betsey Stevenson. “Inaccurate age and  sex data in the census PUMS fles: Evidence and implications”. In: Public Opinion  Quarterly 74 (3 2010), pp. 551–569. URL: https://doi.org/10.1093/poq/nfq033.  [12] Micah Altman et al. “Towards a Modern Approach to Privacy-Aware Government  Data Releases”. In: Berkeley Technology Law Journal 30 (3), pp. 1967–2072. URL:  http://papers.ssrn.com/sol3/papers.cfm?abstract id=2779266.  [13] AMD. AMD Secure Encrypted Virtualization (SEV). Last accessed July 13, 2022.  2022. URL: https://developer.amd.com/sev/.  69  https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our https://www.congress.gov/bill/115th-congress/house-bill/4174 https://www.congress.gov/bill/115th-congress/house-bill/4174 https://www.congress.gov/bill/115th-congress/house-bill/4174 https://www.cio.gov/policies-and-priorities/circular-a-130/ https://www.cio.gov/policies-and-priorities/circular-a-130/ https://www.cio.gov/policies-and-priorities/circular-a-130/ https://www.census.gov/history/pdf/NARA_Legislation.pdf https://www.census.gov/history/pdf/NARA_Legislation.pdf https://www.census.gov/history/pdf/NARA_Legislation.pdf https://hdsr.mitpress.mit.edu/pub/7evz361i https://www.brookings.edu/bpea-articles/economic-analysis-and-statistical-disclosure-limitation/ https://www.brookings.edu/bpea-articles/economic-analysis-and-statistical-disclosure-limitation/ https://www.brookings.edu/bpea-articles/economic-analysis-and-statistical-disclosure-limitation/ https://stats.oecd.org/glossary/detail.asp?ID=21 https://doi.org/10.1093/poq/nfq033 http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2779266 https://developer.amd.com/sev/  NIST SP 800-188 3pd  November 2022  2496  2497  2498  2499  2500  2501  2502  2503  2504  2505  2506  2507  2508  2509  2510  2511  2512  2513  2514  2515  2516  2517  2518  2519  2520  2521  2522  2523  2524  2525  2526  2527  2528  2529  2530  2531  2532  2533  [14] Olivia Angiuli, Joe Blitzstein, and Jim Waldo. “How to De-Identify Your Data”.  In: Commun. ACM 58.12 (Nov. 2015), pp. 48–55. ISSN: 0001-0782. DOI: 10.1145/  2814340. URL: https://doi.org/10.1145/2814340.  [15] ASTM International. ASTM E1869-04 (Reapproved 2014) Standard Guide for Con- fdentiality, Privacy, Access, and Data Security Principles for Health Information  Including Electronic Health Records. 2014.  [16] Daniel Barth-Jones. The ‘Re-Identifcation’ of Governor William Weld’s Medical  Information: A Critical Re-Examination of Health Data Identifcation Risks and  Privacy Protections, Then and Now. July 2012. URL: https://ssrn.com/abstract=  2076397%20or%20http://dx.doi.org/10.2139/ssrn.2076397.  [17] Hans T Bjornsson et al. “Intra-individual Change Over Time in DNA Methylation  with Familial Clustering”. In: JAMA 299 (24 June 2008), pp. 2877–2833. URL:  https://pubmed.ncbi.nlm.nih.gov/18577732/.  [18] Sylvia M. Burwell. Open Data Policy-Managing Information as an Asset. May  2013. URL: https : / / obamawhitehouse . archives . gov / sites / default / fles / omb /  memoranda/2013/m-13-13.pdf.  [19] George Bush. Executive Order 13402:Strengthening Federal Efforts to Protect Against  Identity Theft. May 2006. URL: https:/ /www.gpo.gov/fdsys/pkg/FR- 2006- 05- 15/pdf/06-4552.pdf.  [20] G. Camarillo, C. Holmberg, and Y. Gao. Re-INVITE and Target-Refresh Request  Handling in the Session Initiation Protocol (SIP). RFC 6141 (Proposed Standard).  Internet Engineering Task Force, Mar. 2011. URL: www.ietf.org/rfc/rfc6141.txt.  [21] David Carrell et al. “Hiding in plain sight: Use of realistic surrogates to reduce  exposure of protected health information in clinical text”. In: Journal of the Ameri- can Medical Informatics Association : JAMIA 20 (2 July 2012), pp. 342–348. DOI:  10.1136/amiajnl-2012-001034.  [22] Ann Cavoukian. Privacy by Design: The 7 Foundational Principles. Ontario, CA,  Jan. 2011. URL: https://www.ipc.on.ca/wp-content/uploads/Resources/7foundationalprinciples.  pdf.  [23] Jennifer Cawthra et al. Securing Telehealth Remote Patient Monitoring Ecosystem.  2022. DOI: 10.6028/NIST.SP.1800-30. URL: https://nvlpubs.nist.gov/nistpubs/  SpecialPublications/NIST.SP.1800-30.pdf.  [24] Census Bureau Data Stewardship Program. DS025: Organization of the Disclosure  Review Board. Dec. 2019. URL: https://www2.census.gov/foia/ds policies/ds025.  pdf.  [25] Malcolm Chisholm. “7 Phases of a Data Life Cycle”. In: Information Manage- ment (July 2015). URL: http: / /www.information- management .com/news/data- management/Data-Life-Cycle-Defned-10027232-1.html.  70  https://doi.org/10.1145/2814340 https://doi.org/10.1145/2814340 https://doi.org/10.1145/2814340 https://doi.org/10.1145/2814340 https://ssrn.com/abstract=2076397%20or%20http://dx.doi.org/10.2139/ssrn.2076397 https://ssrn.com/abstract=2076397%20or%20http://dx.doi.org/10.2139/ssrn.2076397 https://ssrn.com/abstract=2076397%20or%20http://dx.doi.org/10.2139/ssrn.2076397 https://pubmed.ncbi.nlm.nih.gov/18577732/ https://obamawhitehouse.archives.gov/sites/default/files/omb/memoranda/2013/m-13-13.pdf https://obamawhitehouse.archives.gov/sites/default/files/omb/memoranda/2013/m-13-13.pdf https://obamawhitehouse.archives.gov/sites/default/files/omb/memoranda/2013/m-13-13.pdf https://www.gpo.gov/fdsys/pkg/FR-2006-05-15/pdf/06-4552.pdf https://www.gpo.gov/fdsys/pkg/FR-2006-05-15/pdf/06-4552.pdf https://www.gpo.gov/fdsys/pkg/FR-2006-05-15/pdf/06-4552.pdf www.ietf.org/rfc/rfc6141.txt https://doi.org/10.1136/amiajnl-2012-001034 https://www.ipc.on.ca/wp-content/uploads/Resources/7foundationalprinciples.pdf https://www.ipc.on.ca/wp-content/uploads/Resources/7foundationalprinciples.pdf https://www.ipc.on.ca/wp-content/uploads/Resources/7foundationalprinciples.pdf https://doi.org/10.6028/NIST.SP.1800-30 https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1800-30.pdf https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1800-30.pdf https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1800-30.pdf https://www2.census.gov/foia/ds_policies/ds025.pdf https://www2.census.gov/foia/ds_policies/ds025.pdf https://www2.census.gov/foia/ds_policies/ds025.pdf http://www.information-management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html http://www.information-management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html http://www.information-management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html  NIST SP 800-188 3pd  November 2022  2534  2535  [26] A. Church. “A Note on the ‘Entscheidungsproblem’”. In: Journal of Symbolic  Logic 1 (1936), pp. 40–41.  2536  2537  2538  [27] Commission on Evidence-Based Policymaking. The Promise of Evidence-Based  Policymaking. Sept. 2017. URL: https://www.acf.hhs.gov/opre/project/commission- evidence-based-policymaking-cep.  2539  2540  [28] Tore Dalenius. “Finding a Needle in a Haystack, or Identifying Anonymous Census  Records”. In: Journal of Offcial Statistics 2 (3 1986), pp. 329–336.  2541  2542  [29] Tore Dalenius. “Towards a methodology for statistical disclosure control”. In: Statis- tik Tidskrift 15 (1977), pp. 429–444.  2543  2544  [30] Jon P. Daries et al. “Privacy, Anonymity, and Big Data in the Social Sciences”. In:  Communications of the ACM 57 (6 Sept. 2014), pp. 56–63.  2545  2546  [31] Tanvi Desai, Felix Ritchie, and Richard Welpton. Economics Working Paper Series  1601. 2016. URL: http://dx.doi.org/10.13140/RG.2.1.3661.1604.  2547  2548  2549  [32] DICOM Standards Committee. DICOM PS3.15 2016e — Security and System Man- agement Profles. 2016. URL: http : / / dicom . nema . org / medical / dicom / current /  output/html/part15.html#chapter E.  2550  2551  2552  2553  2554  [33] Irit Dinur and Kobbi Nissim. “Revealing Information While Preserving Privacy”.  In: Proceedings of the Twenty-second ACM SIGMOD-SIGACT-SIGART Sympo- sium on Principles of Database Systems. PODS ’03. San Diego, California: ACM,  2003, pp. 202–210. ISBN: 1-58113-670-6. DOI: 10 .1145 /773153 .773173. URL:  doi.acm.org/10.1145/773153.773173.  2555  2556  2557  2558  2559  [34] Changyu Dong, Liqun Chen, and Zikai Wen. “When Private Set Intersection Meets  Big Data: An Effcient and Scalable Protocol”. In: Proceedings of the 2013 ACM  SIGSAC Conference on Computer and Communications Security. CCS ’13. Berlin,  Germany: Association for Computing Machinery, 2013, pp. 789–800. ISBN: 9781450324779.  DOI: 10.1145/2508859.2516701. URL: https://doi.org/10.1145/2508859.2516701.  2560  2561  2562  2563  [35] Jörg Drechsler, Stefan Bender, and Susanne Rässler. Comparing fully and partially  synthetic datasets for statistical disclosure control in the German IAB Establish- ment Panel (Working paper 11). New York, 2007. URL: http : / / fdz . iab.de /342 /  section.aspx/Publikation/k080530j05.  2564  2565  [36] George T. Duncan, Mark Elliot, and Juan-José Salazar-Gonzalez. Statistical Conf- dentiality: Principles and Practice. Springer, 2011, p. 113.  2566  2567  [37] I. Dunsford et al. “A human blood-group chimera”. In: British Medical Journal 81  (July 1953). URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2028470/.  2568  2569  2570  [38] Cynthia Dwork and Aaron Roth. “The Algorithmic Foundations of Differential Pri- vacy”. In: Foundations and Trends in Theoretical Computer Science. Vol. 9. 3–4.  NOW, 2014, pp. 211–407.  71  https://www.acf.hhs.gov/opre/project/commission-evidence-based-policymaking-cep https://www.acf.hhs.gov/opre/project/commission-evidence-based-policymaking-cep https://www.acf.hhs.gov/opre/project/commission-evidence-based-policymaking-cep http://dx.doi.org/10.13140/RG.2.1.3661.1604 http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E https://doi.org/10.1145/773153.773173 doi.acm.org/10.1145/773153.773173 https://doi.org/10.1145/2508859.2516701 https://doi.org/10.1145/2508859.2516701 http://fdz.iab.de/342/section.aspx/Publikation/k080530j05 http://fdz.iab.de/342/section.aspx/Publikation/k080530j05 http://fdz.iab.de/342/section.aspx/Publikation/k080530j05 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2028470/  NIST SP 800-188 3pd  November 2022  2571  2572  2573  [39] Cynthia Dwork et al. “Calibrating Noise to Sensitivity in Private Data Analysis”.  In: Theory of Cryptography. Ed. by Shai Halevi and Tal Rabin. Berlin, Heidelberg:  Springer Berlin Heidelberg, 2006, pp. 265–284. ISBN: 978-3-540-32732-5.  2574  2575  2576  2577  [40] Cynthia Dwork et al. “Calibrating Noise to Sensitivity in Private Data Analysis”.  In: Proceedings of the Third Conference on Theory of Cryptography. TCC’06. New  York, NY: Springer-Verlag, 2006, pp. 265–284. ISBN: 3540327312. DOI: 10.1007/  11681878 14. URL: doi.org/10.1007/11681878 14.  2578  2579  2580  2581  [41] Department of Education (ED) Disclosure Review Board (DRB). The Data Disclo- sure Decision. Version 1.0. 2015. URL: https://s3.amazonaws.com/sitesusa/wp- content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department- of-Education-Case-Study Mar-2015.pdf.  2582  2583  [42] Mark Elliot and Angela Dale. Scenarios of attack: the data intruder’s perspective  on statistical disclosure risk. Spring 1999.  2584  2585  2586  [43] El Emam. Methods for the de-identifcation of electronic health records for genomic  research. 2011. URL: https : / / genomemedicine . biomedcentral . com / articles / 10 .  1186/gm239.  2587  2588  2589  2590  [44] K. El Emam and B. Malin. “Appendix B: Concepts and Methods for De-Identifying  Clinical Trial Data”. In: Sharing Clinical Trial Data: Maximizing Benefts, Mini- mizing Risk. Washington, DC: Institute of Medicine of the National Academies,  The National Academies Press, 2015.  2591  2592  [45] Khaled El Emam and Luk Arbuckle. Anonymizing Health Data: Case Studies and  Methods to Get you Started. Sebastopol, CA: O’Reilly Media, 2013.  2593  2594  2595  2596  2597  [46] Ali Farzanehfar, Florimond Houssiau, and Yves-Alexandre de Montjoye. “The risk  of re-identifcation remains high even in country-scale location datasets”. In: Pat- terns 2.3 (2021), p. 100204. ISSN: 2666-3899. DOI: https : / /doi .org /10 .1016/ j .  patter. 2021 .100204. URL: https : / /www.sciencedirect . com/science / article /pii /  S2666389921000143.  2598  2599  2600  2601  [47] Confdentiality and Data Access Committee. Statistical Policy Working Paper 22:  Report on Statistical Disclosure Limitation Methodology. Tech. rep. Federal Com- mittee on Statistical Methodology, 2005. URL: https://www.hhs.gov/sites/default/  fles/spwp22.pdf.  2602  2603  [48] Federal Committee on Statistical Methodology. Data Protection Toolkit. Sept. 2020.  URL: https://nces.ed.gov/fcsm/dpt.  2604  2605  2606  2607  [49] Matthew Fredrikson et al. “Privacy in Pharmacogenetics: An End-to-End Case  Study of Personalized Warfarin Dosing”. In: 23rd USENIX Security Symposium.  San Diego, CA. URL: https://www.usenix.org/system/fles/conference/usenixsecurity14/  sec14-paper-fredrikson-privacy.pdf.  72  https://doi.org/10.1007/11681878_14 https://doi.org/10.1007/11681878_14 https://doi.org/10.1007/11681878_14 doi.org/10.1007/11681878_14 https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://genomemedicine.biomedcentral.com/articles/10.1186/gm239 https://genomemedicine.biomedcentral.com/articles/10.1186/gm239 https://genomemedicine.biomedcentral.com/articles/10.1186/gm239 https://doi.org/https://doi.org/10.1016/j.patter.2021.100204 https://doi.org/https://doi.org/10.1016/j.patter.2021.100204 https://doi.org/https://doi.org/10.1016/j.patter.2021.100204 https://www.sciencedirect.com/science/article/pii/S2666389921000143 https://www.sciencedirect.com/science/article/pii/S2666389921000143 https://www.sciencedirect.com/science/article/pii/S2666389921000143 https://www.hhs.gov/sites/default/files/spwp22.pdf https://www.hhs.gov/sites/default/files/spwp22.pdf https://www.hhs.gov/sites/default/files/spwp22.pdf https://nces.ed.gov/fcsm/dpt https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf  NIST SP 800-188 3pd  November 2022  2608  2609  2610  [50] Simson Garfnkel. De-Identifcation of Personally Identifable Information. Tech.  rep. NIST IR 8053. National Institute of Science and Technology, Nov. 2015. URL:  http://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf.  2611  2612  2613  [51] Simson L. Garfnkel. De-identifcation of personal information. 2015. DOI: 10 .  6028/NIST.IR.8053. URL: https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.  pdf.  2614  2615  2616  [52] Simson L. Garfnkel. Government Data De-Identifcation Stakeholder’s Meeting  June 29, 2016 Meeting Report. 2016. DOI: 10.6028/NIST.IR.8150. URL: https:  //nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf.  2617  2618  [53] Genetics Home Reference. What are single nucleotide polymorphisms (SNPs)?  Last access June 16, 2022. 2022. URL: https://ghr.nlm.nih.gov/primer/genomicresearch/  2619 snp.  2620  2621  [54] Genetics Home Reference. What is DNA. Last access June 16, 2022. 2022. URL:  https://ghr.nlm.nih.gov/primer/basics/dna.  2622  2623  [55] Craig Gentry. “A Fully Homomorphic Encryption Scheme”. AAI3382729. PhD  thesis. Stanford, CA, USA, 2009. ISBN: 9781109444506.  2624  2625  2626  [56] Ruobin Gong, Erica L. Groshen, and Salil Vadhan. “Harnessing the Known Un- knowns: Differential Privacy and the 2020 Census”. In: Harvard Data Science Re- view Special Issue 2 (June 2022). URL: https://hdsr.mitpress.mit.edu/pub/fgyf5cne.  2627  2628  [57] Melissa Gymrek et al. “Identifying Personal Genomes by Surname Inference”. In:  Science 339 (6117 Jan. 2013), pp. 321–329.  2629  2630  2631  [58] Michael B. Hawes. “Implementing Differential Privacy: Seven Lessons From the  2020 United States Census”. In: Harvard Data Science Review 2.2 (Apr. 2020).  URL: https://hdsr.mitpress.mit.edu/pub/dgg03vo6.  2632  2633  [59] TN Herzog, FJ Scheuren, and WE Winkler. Data Quality and Record Linkage Tech- niques. New York/London: Springer, 2007.  2634  2635  2636  [60] Vagelis Hristidis, ed. Information Discovery on Electronic Health Records. 1st.  Chapman and Hall/CRC, 2009. ISBN: 1420090380. URL: https://doi.org/10.1201/  9781420090413.  2637  2638  2639  2640  [61] IHE IT Infrastructure Technical Committee. IHE IT Infrastructure Handbook: De- Identifcation. Integrating the Healthcare Enterprise, Mar. 2014. URL: https://ihe.  net / uploadedFiles / Documents / ITI / IHE ITI Handbook De - Identifcation Rev1 .  0 2014-03-14.pdf.  2641  2642  2643  2644  [62] Clay Johnson III. OMB Memorandum M-07-16: Safeguarding Against and Re- sponding to the Breach of Personally Identifable Information. May 2007. URL:  https : / /georgewbush- whitehouse .archives .gov/omb/memoranda / fy2007/m07- 16.pdf.  73  http://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf https://doi.org/10.6028/NIST.IR.8053 https://doi.org/10.6028/NIST.IR.8053 https://doi.org/10.6028/NIST.IR.8053 https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf https://doi.org/10.6028/NIST.IR.8150 https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://ghr.nlm.nih.gov/primer/genomicresearch/snp https://ghr.nlm.nih.gov/primer/genomicresearch/snp https://ghr.nlm.nih.gov/primer/genomicresearch/snp https://ghr.nlm.nih.gov/primer/basics/dna https://hdsr.mitpress.mit.edu/pub/fgyf5cne https://hdsr.mitpress.mit.edu/pub/dgg03vo6 https://doi.org/10.1201/9781420090413 https://doi.org/10.1201/9781420090413 https://doi.org/10.1201/9781420090413 https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://georgewbush-whitehouse.archives.gov/omb/memoranda/fy2007/m07-16.pdf https://georgewbush-whitehouse.archives.gov/omb/memoranda/fy2007/m07-16.pdf https://georgewbush-whitehouse.archives.gov/omb/memoranda/fy2007/m07-16.pdf  NIST SP 800-188 3pd  November 2022  2645 [63] Information Commissioner’s Offce. Anonymisation: code of practice, managing  2646 data protection risk. 2012. URL: https://ico.org.uk/media/1061/anonymisation- 2647 code.pdf.  2648 [64] ISO 26324:2012, Information and documentation – Digital object identifer system.  2649 Geneva, Switzerland, 2012. URL: https://www.iso.org/standard/43506.html.  2650 [65] ISO/IEC 24760-1:2011, Information technology – Security techniques – A frame- 2651 work for identity management – Part 1: Terminology and concepts. 2011.  2652 [66] ISO/TS 25237:2008(E) Health Informatics — Pseudonymization. Geneva, Switzer- 2653 land, 2008.  2654 [67] Auguste Kerckhoffs. “II. Desiderata De La Cryptographie Militaire”. In: Journal  2655 des sciences militaires IX (Jan. 1883), pp. 5–38.  2656 [68] Shehab Khan. ““Human chimera”: Man fails paternity test because genes in his  2657 saliva are different to those in sperm”. In: The Independent (Oct. 2015).  2658 [69] Vladimir Kolesnikov et al. “Effcient Batched Oblivious PRF with Applications to  2659 Private Set Intersection”. In: Proceedings of the 2016 ACM SIGSAC Conference on  2660 Computer and Communications Security. CCS ’16. Vienna, Austria: Association  2661 for Computing Machinery, 2016, pp. 818–829. ISBN: 9781450341394. DOI: 10 .  2662 1145/2976749.2978381. URL: https://doi.org/10.1145/2976749.2978381.  2663 [70] Leah Krehling. De-Identifcation Guideline. Tech. rep. WL-2020-01. Department  2664 of Electrical and Computer Engineering, Western University, 2020, p. 45.  2665 [71] Sandra Lechner and Winfried Pohlmeier. “To Blank or Not to Blank? A Compari- 2666 son of the Effects of Disclosure Limitation Methods on Nonlinear Regression Es- 2667 timates”. In: Privacy in Statistical Databases, Lecture Notes in Computer Science  2668 3050 (2004), pp. 187–200.  2669 [72] Jaewoo Lee and Chris Clifton. “How Much Is Enough? Choosing ε for Differential  2670 Privacy”. In: Information Security. Ed. by Xuejia Lai, Jianying Zhou, and Hui Li.  2671 Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 325–340. ISBN: 978-3- 2672 642-24861-0.  2673 [73] Ninghui Li, Tiancheng Li, and Suresh Venkatasubramanian. “t-Closeness: Privacy  2674 Beyond k-Anonymity and l-Diversity”. In: 2007 IEEE 23rd International Confer- 2675 ence on Data Engineering. 2007, pp. 106–115. DOI: 10.1109/ICDE.2007.367856.  2676 [74] Yehuda Lindell. “Secure Multiparty Computation”. In: Commun. ACM 64.1 (Dec.  2677 2020), pp. 86–96. ISSN: 0001-0782. DOI: 10.1145/3387108. URL: https://doi.org/  2678 10.1145/3387108.  2679 [75] M. Altman M et al. “Towards a Modern Approach to Privacy-Aware Government  2680 Data Release”. In: Berkeley Journal of Technology Law Internet (2016). URL: https:  2681 //lawcat.berkeley.edu/record/1127405?ln=en.  74  https://ico.org.uk/media/1061/anonymisation-code.pdf https://ico.org.uk/media/1061/anonymisation-code.pdf https://ico.org.uk/media/1061/anonymisation-code.pdf https://www.iso.org/standard/43506.html https://doi.org/10.1145/2976749.2978381 https://doi.org/10.1145/2976749.2978381 https://doi.org/10.1145/2976749.2978381 https://doi.org/10.1145/2976749.2978381 https://doi.org/10.1109/ICDE.2007.367856 https://doi.org/10.1145/3387108 https://doi.org/10.1145/3387108 https://doi.org/10.1145/3387108 https://doi.org/10.1145/3387108 https://lawcat.berkeley.edu/record/1127405?ln=en https://lawcat.berkeley.edu/record/1127405?ln=en https://lawcat.berkeley.edu/record/1127405?ln=en  NIST SP 800-188 3pd  November 2022  2682 [76] Ashwin Machanavajjhala et al. “l-diversity: Privacy beyond k-anonymity”. In: Proc.  2683 22nd Intnl. Conf. Data Engg. (ICDE). 2006.  2684 [77] Sean Martin. When De-identifying Patient Information, Follow the HITRUST Frame- 2685 work. Sept. 2016. URL: https://hitrustalliance.net/de-identifying-patient-information- 2686 follow-hitrust-framework/.  2687 [78] Daniel A. Mayer et al. “Implementation and Performance Evaluation of Privacy- 2688 Preserving Fair Reconciliation Protocols on Ordered Sets”. In: Proceedings of the  2689 First ACM Conference on Data and Application Security and Privacy. CODASPY  2690 ’11. San Antonio, TX, USA: Association for Computing Machinery, 2011, pp. 109–  2691 120. ISBN: 9781450304665. DOI: 10.1145/1943513.1943529. URL: https://doi.org/  2692 10.1145/1943513.1943529.  2693 [79] E McCallister, T Grance, and K A Scarfone. Guide to protecting the confden- 2694 tiality of Personally Identifable Information (PII). Gaithersburg, MD, 2010. DOI:  2695 10.6028/NIST.SP.800-122. URL: https://nvlpubs.nist.gov/nistpubs/Legacy/SP/  2696 nistspecialpublication800-122.pdf.  2697 [80] William K. Michener et al. “Participatory design of DataONE—Enabling cyber- 2698 infrastructure for the biological and environmental sciences”. In: Ecological In- 2699 formatics 11 (2012). Data platforms in integrative biodiversity research, pp. 5–15.  2700 ISSN: 1574-9541. DOI: https : / / doi . org / 10 . 1016 / j . ecoinf . 2011 . 08 . 007. URL:  2701 https://www.sciencedirect.com/science/article/pii/S1574954111000768.  2702 [81] Yves-Alexandre de Montjoye et al. “Unique in the Crowd: The Privacy Bounds of  2703 Human Mobility”. In: Nature Scientifc Reports 3 (1376 2013).  2704 [82] Yves-Alexandre de Montjoye et al. “Unique in the Shopping Mall: On the Reiden- 2705 tifability of Credit Card Metadata”. In: Science 347 (536 2015).  2706 [83] Arvind Narayanan and Ed Felten. No silver bullet: De-identifcation still doesn’t  2707 work. Working Paper. July 2014. URL: http://randomwalker.info/publications/no- 2708 silver-bullet-de-identifcation.pdf.  2709 [84] Arvind Narayanan and Vitaly Shmatikov. “Robust De-anonymization of Large Sparse  2710 Datasets”. In: 2008 IEEE Symposium on Security and Privacy (sp 2008). 2008,  2711 pp. 111–125. DOI: 10.1109/SP.2008.33.  2712 [85] NIST Big Data Interoperability Framework: volume 1, defnitions, version 2. Gaithers- 2713 burg, MD, 2018. DOI: 10.6028/NIST.SP.1500-1r1. URL: https://nvlpubs.nist.gov/  2714 nistpubs/SpecialPublications/NIST.SP.1500-1r1.pdf.  2715 [86] NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk  2716 Management, Version 1.0. Gaithersburg, MD, 2022. DOI: 10.6028/NIST.CSWP.10.  2717 URL: https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.01162020.pdf.  75  https://hitrustalliance.net/de-identifying-patient-information-follow-hitrust-framework/ https://hitrustalliance.net/de-identifying-patient-information-follow-hitrust-framework/ https://hitrustalliance.net/de-identifying-patient-information-follow-hitrust-framework/ https://doi.org/10.1145/1943513.1943529 https://doi.org/10.1145/1943513.1943529 https://doi.org/10.1145/1943513.1943529 https://doi.org/10.1145/1943513.1943529 https://doi.org/10.6028/NIST.SP.800-122 https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf https://doi.org/https://doi.org/10.1016/j.ecoinf.2011.08.007 https://www.sciencedirect.com/science/article/pii/S1574954111000768 http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf https://doi.org/10.1109/SP.2008.33 https://doi.org/10.6028/NIST.SP.1500-1r1 https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-1r1.pdf https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-1r1.pdf https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-1r1.pdf https://doi.org/10.6028/NIST.CSWP.10 https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.01162020.pdf  NIST SP 800-188 3pd  November 2022  2718  2719  2720  2721  2722  [87] Christine M. O’Keefe and James O. Chipperfeld. “A Summary of Attack Meth- ods and Confdentiality Protection Measures for Fully Automated Remote Analysis  Systems”. In: International Statistical Review / Revue Internationale de Statistique  81.3 (2013), pp. 426–455. ISSN: 03067734, 17515823. URL: http://www.jstor.org/  stable/43299645 (visited on 06/28/2022).  2723  2724  2725  2726  [88] Barack Obama. Executive Order 13642—Making Open and Machine Readable the  New Default for Government Information. May 2013. URL: https://obamawhitehouse.  archives.gov/the- press- offce/2013/05/09/executive- order- making- open- and- machine-readable-new-default-government-.  2727  2728  2729  2730  2731  [89] Offce of Civil Rights, US Department of Health and Human Services. Guidance  Regarding Methods for De-identifcation of Protected Health Information in Ac- cordance with the Health Insurance Portability and Accountability Act (HIPAA)  Privacy Rule. Nov. 2012. URL: http : / / www. hhs . gov / hipaa / for - professionals /  privacy/special-topics/de-identifcation/.  2732  2733  2734  2735  [90] Offce of Civil Rights, US Department of Health and Human Services. Individuals’  Right under HIPAA to Access their Health Information 45 CFR § 164.524. Last  accessed June 17, 2022. 2022. URL: https://www.hhs.gov/hipaa/for-professionals/  privacy/guidance/access/index.html.  2736  2737  2738  [91] Offce of Management and Budget. Circular A110 Revised 11/19/93, as further  amended 9/30/99. URL: https : / / obamawhitehouse . archives . gov / omb / circulars  a110/.  2739  2740  2741  [92] Offce of Management and Budget. Statistical Programs and Standards. Last ac- cessed July 15, 2022. 2022. URL: https://www.whitehouse.gov/omb/information- regulatory-affairs/statistical-programs-standards/.  2742  2743  2744  [93] Offce of Safeguards, US Internal Revenue Service. Publication 1075: Tax Infor- mation Security Guidelines For Federal, State and Local Agencies. 2021. URL:  https://www.irs.gov/pub/irs-pdf/p1075.pdf.  2745  2746  [94] Paul Ohm. “Broken Promises of Privacy: Responding to the Surprising Failure of  Anonymization”. In: UCLA Law Review 57 (July 2012), pp. 1701–1778.  2747  2748  [95] OHRP-Guidance on Research Involving Private Information or Biological Speci- mens. Aug. 2008. URL: http://www.hhs.gov/ohrp/policy/cdebiol.html.  2749  2750  2751  [96] Joanne Pascale et al. Issue Paper on Disclosure Review for Information Products  with Qualitative Research Findings. Mar. 2020. URL: https : / /www.census .gov/  library/working-papers/2020/adrm/rsm2020-01.html.  2752  2753  2754  2755  2756  [97] Joanne Pascale et al. “Protecting the Identity of Participants in Qualitative Re- search”. In: Journal of Survey Statistics and Methodology 10.3 (Jan. 2022), pp. 549–  567. ISSN: 2325-0984. DOI: 10.1093/jssam/smab048. eprint: https://academic.oup.  com/jssam/article-pdf/10/3/549/44275508/smab048.pdf. URL: https://doi.org/10.  1093/jssam/smab048.  76  http://www.jstor.org/stable/43299645 http://www.jstor.org/stable/43299645 http://www.jstor.org/stable/43299645 https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- http://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/ http://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/ http://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/ https://www.hhs.gov/hipaa/for-professionals/privacy/guidance/access/index.html https://www.hhs.gov/hipaa/for-professionals/privacy/guidance/access/index.html https://www.hhs.gov/hipaa/for-professionals/privacy/guidance/access/index.html https://obamawhitehouse.archives.gov/omb/circulars_a110/ https://obamawhitehouse.archives.gov/omb/circulars_a110/ https://obamawhitehouse.archives.gov/omb/circulars_a110/ https://www.whitehouse.gov/omb/information-regulatory-affairs/statistical-programs-standards/ https://www.whitehouse.gov/omb/information-regulatory-affairs/statistical-programs-standards/ https://www.whitehouse.gov/omb/information-regulatory-affairs/statistical-programs-standards/ https://www.irs.gov/pub/irs-pdf/p1075.pdf http://www.hhs.gov/ohrp/policy/cdebiol.html https://www.census.gov/library/working-papers/2020/adrm/rsm2020-01.html https://www.census.gov/library/working-papers/2020/adrm/rsm2020-01.html https://www.census.gov/library/working-papers/2020/adrm/rsm2020-01.html https://doi.org/10.1093/jssam/smab048 https://academic.oup.com/jssam/article-pdf/10/3/549/44275508/smab048.pdf https://academic.oup.com/jssam/article-pdf/10/3/549/44275508/smab048.pdf https://academic.oup.com/jssam/article-pdf/10/3/549/44275508/smab048.pdf https://doi.org/10.1093/jssam/smab048 https://doi.org/10.1093/jssam/smab048 https://doi.org/10.1093/jssam/smab048  NIST SP 800-188 3pd  November 2022  2757  2758  2759  2760  2761  [98] Andrew Peterson. “Why the names of six people who complained of sexual assault  were published online by Dallas police”. In: The Washington Post (Apr. 2016).  URL: https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the- names-of-six-people-who-complained-of-sexual-assault-were-published-online- by-dallas-police/.  2762  2763  [99] Thomas Piketty and Emmanuel Saez. “Income Inequality in the United States 1913- 1998”. In: Quarterly Journal of Economics 118 (1 2003), pp. 1–41.  2764  2765  2766  [100] “Pillar Investigates: USCCB gen sec Burrill resigns after sexual misconduct alle- gations”. In: The Pillar (July 2021). URL: https://www.pillarcatholic.com/p/pillar- investigates-usccb-gen-sec.  2767  2768  2769  [101] Sandro Pinto and Nuno Santos. “Demystifying Arm TrustZone: A Comprehensive  Survey”. In: ACM Comput. Surv. 51.6 (Jan. 2019). ISSN: 0360-0300. DOI: 10.1145/  3291047. URL: https://doi.org/10.1145/3291047.  2770  2771  2772  [102] Private Lives and Public Policies: Confdentiality and Accessibility of Government  Statistics. Panel on Confdentiality and Data Access, National Research Council,  p. 288. ISBN: 0-309-57611-3. URL: http://www.nap.edu/catalog/2122/.  2773 [103] Public Law 93-579: The Privacy Act. 88 Stat. 1896, 5 U.S.C. § 552a.  2774  2775  [104] Balaji Raghunathan. The Complete Book of Data Anonymization: From Planning  to Implementation. USA: Auerbach Publications, 2013. ISBN: 1439877300.  2776  2777  [105] William H. Rehnquist. Department of State v. Washington Post Co., 456 U.S. 595  (1982). 1982. URL: https://www.loc.gov/item/usrep456595/.  2778  2779  2780  [106] Report 08-536, Privacy: Alternatives Exist for Enhancing Protection of Personally  Identifable Information. May 2008. URL: http://www.gao.gov/new.items/d08536.  pdf.  2781  2782  2783  [107] Diane Ridgeway et al. Challenge Design and Lessons Learned from the 2018 Dif- ferential Privacy Challenges. 2021. DOI: 10 . 6028 / NIST. TN . 2151. URL: https :  //nvlpubs.nist.gov/nistpubs/TechnicalNotes/NIST.TN.2151.pdf.  2784  2785  2786  2787  [108] Pierangela Samarati and Latanya Sweeney. “Protecting privacy when disclosing  information: k-anonymity and its enforcement through generalization and suppres- sion”. In: Proceedings of the IEEE Symposium on Research in Security and Privacy  (May 1998).  2788  2789  2790  [109] Pierangela Samarti. “Protecting Respondents’ Identities in Microdata Release”.  In: IEEE Transactions on Knowledge and Data Engineering 13 (6 Nov. 2001),  pp. 1010–1027.  2791  2792  2793  [110] Josep Sanz and Josep Domingo-Ferrer. “A Comparative Study of Microaggrega- tion Methods”. In: Questiio: Quaderns d’Estadistica, Sistemes, Informatica i In- vestigació Operativa 22 (3 Aug. 2000), pp. 511–526.  77  https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.pillarcatholic.com/p/pillar-investigates-usccb-gen-sec https://www.pillarcatholic.com/p/pillar-investigates-usccb-gen-sec https://www.pillarcatholic.com/p/pillar-investigates-usccb-gen-sec https://doi.org/10.1145/3291047 https://doi.org/10.1145/3291047 https://doi.org/10.1145/3291047 https://doi.org/10.1145/3291047 http://www.nap.edu/catalog/2122/ https://www.loc.gov/item/usrep456595/ http://www.gao.gov/new.items/d08536.pdf http://www.gao.gov/new.items/d08536.pdf http://www.gao.gov/new.items/d08536.pdf https://doi.org/10.6028/NIST.TN.2151 https://nvlpubs.nist.gov/nistpubs/TechnicalNotes/NIST.TN.2151.pdf https://nvlpubs.nist.gov/nistpubs/TechnicalNotes/NIST.TN.2151.pdf https://nvlpubs.nist.gov/nistpubs/TechnicalNotes/NIST.TN.2151.pdf  NIST SP 800-188 3pd  November 2022  2794  2795  2796  [111] M Scaiano et al. “unifed framework for evaluating the risk of re-identifcation of  text de-identifcation tools”. In: Journal of Biomedical Informatics 63 (Oct. 2016),  pp. 174–183.  2797  2798  2799  2800  2801  [112] Matthias Schunter. “Intel Software Guard Extensions: Introduction and Open Re- search Challenges”. In: Proceedings of the 2016 ACM Workshop on Software PRO- tection. SPRO ’16. Vienna, Austria: Association for Computing Machinery, 2016,  p. 1. ISBN: 9781450345767. DOI: 10.1145/2995306.2995307. URL: https://doi.org/  10.1145/2995306.2995307.  2802  2803  2804  [113] M Seastrom. “Licensing”. In: Confdentiality, Disclosure and Data Access: Theory  and Practical Application for Statistical Agencies. Ed. by P. Doyle et al. Elsevier  Science, 2001.  2805  2806  2807  2808  2809  [114] Jordi Soria-Comas and Josep Domingo-Ferrer. “Connecting privacy models: syn- ergies between k-anonymity, t-closeness, and differential privacy”. In: Working  Paper (English Only). Ottawa, Canada, Oct. 2013. URL: https : / / www . unece .  org / fleadmin / DAM / stats / documents / ece / ces / ge . 46 / 2013 / Topic 2 soria - comas domingo-ferrer.pdf.  2810  2811  [115] Philip Steel and Jon Sperling. The Impact of Multiple Geographies and Geographic  Detail on Disclosure Risk: Interactions between Census Tract and ZIP Code Tab-  2812  2813  ulation Geography. 2001. URL: https : / /www.census .gov/content /dam/Census/  library/working-papers/2001/adrm/steel-sperling-2001.pdf.  2814  2815  2816  [116] Jackson M. Steinkamp et al. “Evaluation of Automated Public De-Identifcation  Tools on a Corpus of Radiology Reports”. In: Radiol Artif Intell 2 (6 Oct. 2020).  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8082401/.  2817  2818  [117] John Paul Stevens. U.S. Dept. of Justice v. Reporters Committee For Freedom of  Press, 489 U.S. 749 (1989). 1988. URL: https://www.loc.gov/item/usrep489749/.  2819  2820  [118] John Paul Stevens. United States Department of State v. Ray et al., 502 U.S. 164  (1991). 1991. URL: https://www.loc.gov/item/usrep502164/.  2821  2822  2823  2824  [119] Kevin Stine et al. Volume I: guide for mapping types of information and infor- mation systems to security categories. Gaithersburg, MD, 2008. DOI: 10 . 6028 /  NIST. SP. 800 - 60v1r1. URL: https : / / nvlpubs . nist . gov / nistpubs / Legacy / SP /  nistspecialpublication800-60v1r1.pdf.  2825  2826  [120] Tim Stobierski. In: Business Insights (Feb. 2021). URL: https : / /online .hbs .edu/  blog/post/data-life-cycle.  2827  2828  2829  [121] Teresa A. Sullivan. “Coming to Our Census: How Social Statistics Underpin Our  Democracy (and Republic)”. In: Harvard Data Science Review 2.1 (Jan. 2020).  URL: https://hdsr.mitpress.mit.edu/pub/1g1cbvkv.  2830  2831  2832  [122] Latanya Sweeney. “k-anonymity: a model for protecting privacy”. In: International  Journal on Uncertainty, Fuzziness and Knowledge-based Systems 10 (5 2002),  pp. 557–570.  78  https://doi.org/10.1145/2995306.2995307 https://doi.org/10.1145/2995306.2995307 https://doi.org/10.1145/2995306.2995307 https://doi.org/10.1145/2995306.2995307 https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.census.gov/content/dam/Census/library/working-papers/2001/adrm/steel-sperling-2001.pdf https://www.census.gov/content/dam/Census/library/working-papers/2001/adrm/steel-sperling-2001.pdf https://www.census.gov/content/dam/Census/library/working-papers/2001/adrm/steel-sperling-2001.pdf https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8082401/ https://www.loc.gov/item/usrep489749/ https://www.loc.gov/item/usrep502164/ https://doi.org/10.6028/NIST.SP.800-60v1r1 https://doi.org/10.6028/NIST.SP.800-60v1r1 https://doi.org/10.6028/NIST.SP.800-60v1r1 https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://online.hbs.edu/blog/post/data-life-cycle https://online.hbs.edu/blog/post/data-life-cycle https://online.hbs.edu/blog/post/data-life-cycle https://hdsr.mitpress.mit.edu/pub/1g1cbvkv  NIST SP 800-188 3pd  November 2022  2833  2834  2835  [123] Latanya Sweeney. “k-anonymity: a model for protecting privacy”. In: Int. J. Un- certain. Fuzziness Knowl.-Based Syst. 10 (5 Oct. 2002), pp. 557–570. URL: http:  //dx.doi.org/10.1142/S0218488502001648.  2836  2837  [124] Latanya Sweeney. “Weaving Technology and Policy Together to Maintain Conf- dentiality”. In: Journal of Law, Medicine and Ethics 25 (1997), pp. 98–110.  2838  2839  2840  [125] “The Debate Over ‘Re-Identifcation’ Of Health Information: What Do We Risk?”  In: Health Affairs Blog (Aug. 2012). DOI: 10.1377/hblog20120810.021952. URL:  https://www.healthaffairs.org/do/10.1377/forefront.20120810.021952.  2841  2842  2843  [126] Title V of the E-Government Act of 2002: Confdential Information Protection and  Statistical Effciency Act (CIPSEA) PL 107–347,116 Stat. 2899, 44 USC § 101  Section 502(8).  2844  2845  [127] TransCelerate Biopharma, Inc. Data De-identifcation and Anonymization of Indi- vidual Patient Data in Clinical Studies—A Model Approach. 2013.  2846  2847  2848  [128] Michael Carl Tschantz and Jeannette M. Wing. Formal Methods for Privacy. Tech.  rep. CMU-CS-09-154. Pittsburg, PA: Carnegie Mellon University, Aug. 2009. URL:  http://reports-archive.adm.cs.cmu.edu/anon/2009/CMU-CS-09-154.pdf.  2849  2850  2851  [129] A. M. Turing. “On Computable Numbers, with an Application to the Entschei- dungsproblem”. In: Proceedings of the London Mathematical Society, Series 2 (42  1936–37), pp. 230–265.  2852  2853  [130] US Census Bureau. Census Confdentiality and Privacy: 1790-2002. 2003. URL:  https://www.census.gov/prod/2003pubs/conmono2.pdf.  2854  2855  [131] US Census Bureau. The “72-Year Rule”. Jan. 2022. URL: https://www.census.gov/  history/www/genealogy/decennial census records/the 72 year rule 1.html.  2856  2857  2858  [132] US Congress. Public Law 104-191: Health Insurance Portability and Accountabil- ity Act of 1996 (HIPAA). Aug. 1996. URL: https://www.congress.gov/bill/104th- congress/house-bill/3103.  2859  2860  [133] US Congress. Public Law 114-185: FOIA Improvement Act of 2016. 2016. URL:  https://www.congress.gov/114/plaws/publ185/PLAW-114publ185.pdf.  2861  2862  [134] US Congress. The Freedom Of Information Act, 5 U.S.C. § 552. 2022. URL: https:  //www.justice.gov/oip/freedom-information-act-5-usc-552.  2863  2864  2865  [135] US Department of Health and Human Services. 45 CFR Part 46: Federal Policy  for the Protection of Human Subjects. Jan. 2017. URL: https://www.govinfo.gov/  content/pkg/FR-2017-01-19/pdf/2017-01058.pdf.  2866  2867  [136] US Department of Health and Human Services. Guidance Regarding Methods for  De-identifcation of Protected Health Information in Accordance with the Health  2868  2869  2870  Insurance Portability and Accountability Act (HIPAA) Privacy Rule. 2012. URL:  https : / / www . hhs . gov / hipaa / for - professionals / privacy / special - topics / de - identifcation/index.html.  79  http://dx.doi.org/10.1142/S0218488502001648 http://dx.doi.org/10.1142/S0218488502001648 http://dx.doi.org/10.1142/S0218488502001648 https://doi.org/10.1377/hblog20120810.021952 https://www.healthaffairs.org/do/10.1377/forefront.20120810.021952 http://reports-archive.adm.cs.cmu.edu/anon/2009/CMU-CS-09-154.pdf https://www.census.gov/prod/2003pubs/conmono2.pdf https://www.census.gov/history/www/genealogy/decennial_census_records/the_72_year_rule_1.html https://www.census.gov/history/www/genealogy/decennial_census_records/the_72_year_rule_1.html https://www.census.gov/history/www/genealogy/decennial_census_records/the_72_year_rule_1.html https://www.congress.gov/bill/104th-congress/house-bill/3103 https://www.congress.gov/bill/104th-congress/house-bill/3103 https://www.congress.gov/bill/104th-congress/house-bill/3103 https://www.congress.gov/114/plaws/publ185/PLAW-114publ185.pdf https://www.justice.gov/oip/freedom-information-act-5-usc-552 https://www.justice.gov/oip/freedom-information-act-5-usc-552 https://www.justice.gov/oip/freedom-information-act-5-usc-552 https://www.govinfo.gov/content/pkg/FR-2017-01-19/pdf/2017-01058.pdf https://www.govinfo.gov/content/pkg/FR-2017-01-19/pdf/2017-01058.pdf https://www.govinfo.gov/content/pkg/FR-2017-01-19/pdf/2017-01058.pdf https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html  NIST SP 800-188 3pd  November 2022  2871  2872  2873  2874  2875  [137] Joel Havermann, plaintiff—Appellant v. Carolyn W. Colvin, Acting Commissioner  of the Social Security Administration, Defendant— Appellee, No. 12-2453, US Court  of Appeals for the Fourth Circuit, 537 Fed. Appx. 142; 2013 US App. Aug 1,  2013. Joel Havemann v. Carolyn W. Colvin, Civil No. JFM-12-1325. 2015 US Dist.  LEXIS 27560. Mar. 2015.  2876  2877  [138] “Utility”. In: Glossary of Statistical Terms (Aug. 2002). Last accessed June 23,  2022. URL: https://stats.oecd.org/glossary/detail.asp?ID=4884.  2878  2879  2880  [139] Russell T. Vought. Phase 1 Implementation of the Foundations for Evidence-Based  Policymaking Act of 2018: Learning Agendas, Personnel, and Planning Guidance.  URL: https://www.whitehouse.gov/wp-content/uploads/2019/07/M-19-23.pdf.  2881  2882  2883  [140] Charlie Warzel and Stuart A. Thompson. “How Your Phone Betrays Democracy”.  In: The New York Times (Dec. 2019). URL: https://www.nytimes.com/interactive/  2019/12/21/opinion/location-data-democracy-protests.html.  2884  2885  2886  [141] Cathy Wasserman and Eric Ossiander. Department of Health Agency Standards for  Reporting Data with Small Numbers. May 2018. URL: https://doh.wa.gov/sites/  default/fles/legacy/Documents/1500//SmallNumbers.pdf.  2887  2888  2889  [142] Leon Willenborg and Ton de Waal. “Chapter 3 Data Analytic Impact of SDC  Techniques on Microdata”. In: Elements of Statistical Disclosure Control (2012),  pp. 72–92.  2890  2891  2892  2893  [143] Li Xiong et al. “Privacy-Preserving Information Discovery on EHRs”. In: Informa- tion Discovery on Electronic Health Records. Ed. by Vagelis Hristidis. 1st. Chap- man and Hall/CRC, 2009. ISBN: 1420090380. URL: https : / / doi . org / 10 . 1201 /  9781420090413.  80  https://stats.oecd.org/glossary/detail.asp?ID=4884 https://www.whitehouse.gov/wp-content/uploads/2019/07/M-19-23.pdf https://www.nytimes.com/interactive/2019/12/21/opinion/location-data-democracy-protests.html https://www.nytimes.com/interactive/2019/12/21/opinion/location-data-democracy-protests.html https://www.nytimes.com/interactive/2019/12/21/opinion/location-data-democracy-protests.html https://doh.wa.gov/sites/default/files/legacy/Documents/1500//SmallNumbers.pdf https://doh.wa.gov/sites/default/files/legacy/Documents/1500//SmallNumbers.pdf https://doh.wa.gov/sites/default/files/legacy/Documents/1500//SmallNumbers.pdf https://doi.org/10.1201/9781420090413 https://doi.org/10.1201/9781420090413 https://doi.org/10.1201/9781420090413  NIST SP 800-188 3pd  November 2022  2894  2895  2896  2897  2898  2899  2900  2901  2902  2903  2904  2905  2906  2907  2908  2909  2910  2911  2912  2913  2914  2915  2916  2917  2918  2919  2920  2921  2922  2923  2924  2925  2926  2927  2928  Appendix A. Standards  • ASTM E1869-04(2014) Standard Guide for Confdentiality, Privacy, Access, and  Data Security Principles for Health Information Including Electronic Health Records.  • DICOM PS3.15 2016d – Security and System Management Profles Chapter E At- tribute Confdentiality Profles, DICOM Standards Committee, NEMA 2016. http:  //dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter E  • HITRUST De-Identifcation Working Group (2015, March). De-Identifcation Frame- work: A Consistent, Managed Methodology for the De-Identifcation of Personal  Data and the Sharing of Compliance and Risk Information. Frisco, TX: HITRUST.  Retrieved from https://hitrustalliance.net/de-identifcation-license-agreement/  • ISO 8000-2:2012(E) Data quality – Part 2: Vocabulary, 2012. ISO, Geneva, Switzer- land. 2012.  • ISO/IEC 27000:2014 Information technology -- Security techniques -- Information  security management systems -- Overview and vocabulary. ISO, Geneva, Switzer- land. 2012.  • ISO/IEC 24760-1:2011 Information technology -- Security techniques -- A frame- work for identity management -- Part 1: Terminology and concepts. ISO, Geneva,  Switzerland. 2011.  • ISO/TS 25237:2008(E) Health Informatics – Pseudonymization. ISO, Geneva, Switzer- land. 2008.  • ISO/IEC 20889 WORKING DRAFT 2016-05-30, Information technology – Secu- rity techniques – Privacy enhancing data de-identifcation techniques. ISO, Geneva,  Switzerland. 2016.  • IHE IT Infrastructure Handbook, De-Identifcation, Integrating the Healthcare Enter- prise, June 6, 2014. http://www.ihe.net/User Handbooks/  Appendix A.1. NIST Publications  • NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk  Management, Version 1.0. Gaithersburg, MD, 2022. DOI: 10.6028/NIST.CSWP.10.  URL: https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.01162020.pdf  • Kevin Stine et al. Volume I: guide for mapping types of information and information  systems to security categories. Gaithersburg, MD, 2008. DOI: 10.6028/NIST.SP.800- 60v1r1. URL: https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800- 60v1r1.pdf  • Simson L. Garfnkel. De-identifcation of personal information. 2015. DOI: 10.6028/  NIST.IR.8053. URL: https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf  81  http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E https://doi.org/10.6028/NIST.CSWP.10 https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.01162020.pdf https://doi.org/10.6028/NIST.SP.800-60v1r1 https://doi.org/10.6028/NIST.SP.800-60v1r1 https://doi.org/10.6028/NIST.SP.800-60v1r1 https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://doi.org/10.6028/NIST.IR.8053 https://doi.org/10.6028/NIST.IR.8053 https://doi.org/10.6028/NIST.IR.8053 https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf http://www.ihe.net/User https://hitrustalliance.net/de-identification-license-agreement  NIST SP 800-188 3pd  November 2022  2929  2930  2931  2932  2933  2934  2935  2936  2937  2938  2939  2940  2941  2942  2943  2944  2945  2946  2947  2948  2949  2950  2951  2952  2953  2954  2955  2956  2957  2958  2959  2960  2961  2962  • Simson L. Garfnkel. Government Data De-Identifcation Stakeholder’s Meeting  June 29, 2016 Meeting Report. 2016. DOI: 10.6028/NIST.IR.8150. URL: https:  //nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf  Appendix A.2. Other U.S. Government Publications  • Census Confdentiality and Privacy: 1790-2002, US Census Bureau, 2003. https:  //www.census.gov/prod/2003pubs/conmono2.pdf  • Data De-identifcation: An Overview of Basic Terms, Privacy Technical Assistance  Center, US Department of Education. May 2013. http://ptac.ed.gov/sites/default/  fles/data deidentifcation terms.pdf  • Data Disclosure Decision, Department of Education (ED) Disclosure Review Board  (DRB), A Product of the Federal CIO Council Innovation Committee. Version 1.0,  2015.  • Disclosure Avoidance Techniques at the US Census Bureau: Current Practices and  Research, Research Report Series (Disclosure Avoidance #2014-02), Amy Lauger,  Billy Wisniewski, and Laura McKenna, Center for Disclosure Avoidance Research,  US Census. Bureau, September 26, 2014. https://www.census.gov/srd/CDAR/cdar2014-02  Discl Avoid Techniques.pdf  • Frequently Asked Questions – Disclosure Avoidance, Privacy Technical Assistance  Center, U.S. Department of Education. October 2012 (revised July 2015). http:  //ptac.ed.gov/sites/default/fles/FAQ Disclosure Avoidance.pdf  • Guidance Regarding Methods for De-identifcation of Protected Health Information  in Accordance with the Health Insurance Portability and Accountability Act (HIPAA)  Privacy Rule, U.S. Department of Health & Human Services, Offce for Civil Rights,  November 26, 2012. http://www.hhs.gov/ocr/privacy/hipaa/understanding/coveredentities/  De-identifcation/hhs deid guidance.pdf  • http://www.hhs.gov/ohrp/policy/cdebiol.html  • http://ptac.ed.gov/sites/default/fles/data deidentifcation terms.pdf  • The Data Disclosure Decision, Department of Education (ED) Disclosure Review  Board (DRB), A Product of the Federal CIO Council Innovation Committee.  • https://www.cdc.gov/nchs/data/nchs microdata release policy 4-02a.pdf  • http://www.cdc.gov/nchs/nvss/dvs data release.htm  • Linking Data for Health Services Research: A Framework and Instructional Guide.,  Dusetzina SB, Tyree S, Meyer AM, Meyer A, Green L, Carpenter WR. (Prepared  by the University of North Carolina at Chapel Hill under Contract No. 290-2010-  82  https://doi.org/10.6028/NIST.IR.8150 https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://www.census.gov/prod/2003pubs/conmono2.pdf https://www.census.gov/prod/2003pubs/conmono2.pdf https://www.census.gov/prod/2003pubs/conmono2.pdf http://ptac.ed.gov/sites/default/files/data_deidentification_terms.pdf http://ptac.ed.gov/sites/default/files/data_deidentification_terms.pdf http://ptac.ed.gov/sites/default/files/data_deidentification_terms.pdf https://www.census.gov/srd/CDAR/cdar2014-02_Discl_Avoid_Techniques.pdf https://www.census.gov/srd/CDAR/cdar2014-02_Discl_Avoid_Techniques.pdf https://www.census.gov/srd/CDAR/cdar2014-02_Discl_Avoid_Techniques.pdf http://ptac.ed.gov/sites/default/files/FAQ_Disclosure_Avoidance.pdf http://ptac.ed.gov/sites/default/files/FAQ_Disclosure_Avoidance.pdf http://ptac.ed.gov/sites/default/files/FAQ_Disclosure_Avoidance.pdf http://www.hhs.gov/ocr/privacy/hipaa/understanding/coveredentities/De-identification/hhs_deid_guidance.pdf http://www.hhs.gov/ocr/privacy/hipaa/understanding/coveredentities/De-identification/hhs_deid_guidance.pdf http://www.hhs.gov/ocr/privacy/hipaa/understanding/coveredentities/De-identification/hhs_deid_guidance.pdf http://www.hhs.gov/ohrp/policy/cdebiol.html http://ptac.ed.gov/sites/default/files/data_deidentification_terms.pdf https://www.cdc.gov/nchs/data/nchs_microdata_release_policy_4-02a.pdf http://www.cdc.gov/nchs/nvss/dvs_data_release.htm  privacy/privacy-  NIST SP 800-188 3pd  November 2022  2963  2964  2965  2966  2967  2968  2969  2970  2971  2972  2973  2974  2975  2976  2977  2978  2979  2980  2981  2982  2983  2984  2985  2986  2987  2988  2989  2990  2991  2992  2993  2994  2995  2996  000141.) AHRQ Publication No. 14-EHC033-EF. Rockville, MD: Agency for Health- care Research and Quality; September 2014.  • National Center for Health Statistics Data Release and Access Policy for Micro-data  and Compressed Vital Statistics File, Centers for Disease Control, April 26, 2011.  http://www.cdc.gov/nchs/nvss/dvs data release.htm  • National Center for Health Statistics Policy on Micro-Data Dissemination, Centers  for Disease Control, July 2002. https://www.cdc.gov/nchs/data/nchs microdata release policy 4- 02a.pdf  • OHRP-Guidance on Research Involving Private Information or Biological Speci- mens (2008), Department of Health & Human Services, Offce of Human Research  Protections (OHRP), August 16, 2008. http://www.hhs.gov/ohrp/policy/cdebiol.html  • OMB Circular A-130, Managing Information as a Strategic Resource, July 2016.  • Privacy and Confdentiality Research and the U.S. Census Bureau, Recommenda- tions Based on a Review of the Literature, Thomas S. Mayer, Statistical Research Di- vision, US Bureau of the Census. February 7, 2002. https://www.census.gov/srd/papers/pdf/rsm2002- 01.pdf  • Statistical Policy Working Paper 22 (Second version, 2005), Report on Statistical  Disclosure Limitation Methodology, Federal Committee on Statistical Methodology,  December 2005.  Selected Publications by Other Governments  • Privacy business resource 4: De-identifcation of data and information, Offce of the  Australian Information Commissioner, Australian Government, April 2014. http://www.oaic.gov.au/images/documents/  resources/privacy-business-resources/privacy business resource 4.pdf  • Opinion 05/2014 on Anonymisation Techniques, Article 29 Data Protection Working  Party, 0829/14/EN WP216, Adopted on 10 April 2014.  • Anonymisation: Managing data protection risk, Code of Practice 2012, Information  Commissioner’s Offce. https://ico.org.uk/media/for-organisations/documents/1061/anonymisation- code.pdf. 108 pages.  • The Anonymisation Decision-Making Framework, Mark Elliot, Elaine Mackey, Kieron  O’Hara and Caroline Tudor, UKAN, University of Manchester, July 2016. http:  //ukanon.net/ukan-resources/ukan-decision-making-framework/  Reports and Books  • Private Lives and Public Policies: Confdentiality and Accessibility of Government  Statistics (1993), George T. Duncan, Thomas B. Jabine, and Virginia A. de Wolf,  83  http://www.cdc.gov/nchs/nvss/dvs_data_release.htm https://www.cdc.gov/nchs/data/nchs_microdata_release_policy_4-02a.pdf https://www.cdc.gov/nchs/data/nchs_microdata_release_policy_4-02a.pdf https://www.cdc.gov/nchs/data/nchs_microdata_release_policy_4-02a.pdf http://www.hhs.gov/ohrp/policy/cdebiol.html http://www.oaic.gov.au/images/documents/privacy/privacy-resources/privacy-business-resources/privacy_business_resource_4.pdf http://www.oaic.gov.au/images/documents/privacy/privacy-resources/privacy-business-resources/privacy_business_resource_4.pdf http://www.oaic.gov.au/images/documents/privacy/privacy-resources/privacy-business-resources/privacy_business_resource_4.pdf https://ico.org.uk/media/for-organisations/documents/1061/anonymisation-code.pdf https://ico.org.uk/media/for-organisations/documents/1061/anonymisation-code.pdf https://ico.org.uk/media/for-organisations/documents/1061/anonymisation-code.pdf http://ukanon.net/ukan-resources/ukan-decision-making-framework/ http://ukanon.net/ukan-resources/ukan-decision-making-framework/ http://ukanon.net/ukan-resources/ukan-decision-making-framework/ https://www.census.gov/srd/papers/pdf/rsm2002  NIST SP 800-188 3pd  November 2022  2997  2998  2999  3000  3001  3002  3003  3004  3005  3006  3007  3008  3009  3010  3011  3012  3013  3014  3015  3016  3017  3018  3019  3020  3021  3022  3023  3024  3025  3026  3027  3028  3029  3030  3031  Editors; Panel on Confdentiality and Data Access; Commission on Behavioral and  Social Sciences and Education; Division of Behavioral and Social Sciences and Ed- ucation; National Research Council, 1993. http://dx.doi.org/10.17226/2122  • Sharing Clinical Trial Data: Maximizing Benefts, Minimizing Risk, Committee on  Strategies for Responsible Sharing of Clinical Trial Data, Board on Health Sciences  Policy, Institute of Medicine of the National Academies, The National Academies  Press, Washington, DC. 2015.  • P. Doyle and J. Lane, Confdentiality, Disclosure and Data Access: Theory and Prac- tical Applications for Statistical Agencies, North-Holland Publishing, Dec 31, 2001.  • George T. Duncan, Mark Elliot, Juan-José Salazar-Gonzalez, Statistical Confden- tiality: Principles and Practice, Springer, 2011.  • Cynthia Dwork and Aaron Roth, The Algorithmic Foundations of Differential Pri- vacy (Foundations and Trends in Theoretical Computer Science). Now Publishers,  August 11, 2014. http://www.cis.upenn.edu/˜aaroth/privacybook.html  • Khaled El Emam, Guide to the De-Identifcation of Personal Health Information,  CRC Press, 2013.  • Khaled El Emam and Luk Arbuckle, Anonymizing Health Data, O’Reilly, Cam- bridge, MA. 2013.  • K El Emam and B Malin, “Appendix B: Concepts and Methods for De-Identifying  Clinical Trial Data,” in Sharing Clinical Trial Data: Maximizing Benefts, Minimiz- ing Risk, Institute of Medicine of the National Academies, The National Academies  Press, Washington, DC. 2015.  • Anco Hundepool, Josep Domingo-Ferrer, Luisa Franconi, Sarah Giessing, Eric Schulte  Nordholt, Keith Spicer, Peter-Paul de Wolf, Statistical Disclosure Control, Wiley,  September 2012.  How-To Articles  • Leah Krehling, De-Identifcation Guideline, WHISPERLAB, Technical Report WL- 2020-01, Department of Electrical and Computer Engineering, Western University,  2020.  • Olivia Angiuli, Joe Blitstein, and Jim Waldo, How to De-Identify Your Data, Com- munications of the ACM, December 2015.  • Jörg Drechsler, Stefan Bender, Susanne Rässler, Comparing fully and partially syn- thetic datasets for statistical disclosure control in the German IAB Establishment  Panel. 2007, United Nations, Economic Commission for Europe. Working paper,  11, New York, 8 p. http://fdz.iab.de/342/section.aspx/Publikation/k080530j05  84  http://www.nap.edu/author/CBASSE http://www.nap.edu/author/CBASSE http://www.nap.edu/author/CBASSE http://www.nap.edu/author/DBASSE http://www.nap.edu/author/DBASSE http://www.nap.edu/author/DBASSE http://fdz.iab.de/342/section.aspx/Publikation/k080530j05 http://www.cis.upenn.edu/�aaroth/privacybook.html http://dx.doi.org/10.17226/2122  NIST SP 800-188 3pd  November 2022  3032  3033  3034  3035  3036  3037  3038  3039  3040  3041  3042  3043  3044  3045  • Ebaa Fayyoumi and B. John Oommen, A survey on statistical disclosure control and  micro-aggregation techniques for secure statistical databases. 2010, Software Prac- tice and Experience. 40, 12 (November 2010), 1161-1188. DOI=10.1002/spe.v40:12  http://dx.doi.org/10.1002/spe.v40:12http://dx.doi.org/10.1002/spe.v40:12  • Jingchen Hu, Jerome P. Reiter, and Quanli Wang, Disclosure Risk Evaluation for  Fully Synthetic Categorical Data, Privacy in Statistical Databases, pp. 185-199,  2014. https://link.springer.com/chapter/10.1007/978-3-319-11257-2 15  • Matthias Templ, Bernhard Meindl, Alexander Kowarik and Shuang Chen, Introduc- tion to Statistical Disclosure Control (SDC), IHSN Working Paper No. 007, Inter- national Household Survey Network, August 2014. http://www.ihsn.org/home/sites/  default/fles/resources/ihsn-working-paper-007-Oct27.pdf  • Natalie Shlomo, Statistical Disclosure Control Methods for Census Frequency Ta- bles, International Statistical Review (2007), 75, 2, 199-217. https://www.jstor.org/  stable/41508461  85  http://dx.doi.org/10.1002/spe.v40:12 https://link.springer.com/chapter/10.1007/978-3-319-11257-2_15 http://www.ihsn.org/home/sites/default/files/resources/ihsn-working-paper-007-Oct27.pdf http://www.ihsn.org/home/sites/default/files/resources/ihsn-working-paper-007-Oct27.pdf http://www.ihsn.org/home/sites/default/files/resources/ihsn-working-paper-007-Oct27.pdf https://www.jstor.org/stable/41508461 https://www.jstor.org/stable/41508461 https://www.jstor.org/stable/41508461  NIST SP 800-188 3pd  November 2022  3046  3047  3048  3049  3050  3051  3052  3053  3054  3055  3056  3057  3058  3059  3060  3061  3062  3063  3064  3065  3066  3067  3068  3069  3070  3071  3072  3073  Appendix B. List of Symbols, Abbreviations, and Acronyms  Selected acronyms and abbreviations used in this paper are defned below.  ACM Association for Computing Machinery  AHRQ Agency for Healthcare Research and Quality  AMD Advanced Micro Devices  ARM Advanced RISC Machines (formerly Acron RISC Machine)  ARMP average record matching probability  ASTM ASTM (formerly the American Society for Testing and Materials)  CED-DA Center for Enterprise Dissemination-Disclosure Avoidance  CFR Code of Federal Regulations  CIO chief information offcer  CIPSEA The Confdential Information Protection and Statistical Effciency Act of 2002  CNSS Committee on National Security Systems  CNSSI Committee on National Security Systems instruction  CPU central processing unit  CRC (formerly the Chemical Rubber Company)  DC District of Columbia  DCMA Defense Contract Management Agency  DICOM Digital Imaging and Communications in Medicine  DNA deoxyribonucleic acid  DOI digital object identifer  DRB disclosure review board  DUA data use agreement  EDDRB Department of Education disclosure review board  FCSM Federal Committee on Statistical Methodology  FHE Fully-homomorphic encryption  FISMA Federal Information Security Modernization Act  FOIA Freedom of Information Act  86  NIST SP 800-188 3pd  November 2022  3074  3075  3076  3077  3078  3079  3080  3081  3082  3083  3084  3085  3086  3087  3088  3089  3090  3091  3092  3093  3094  3095  3096  3097  3098  3099  3100  3101  3102  HHS Health and Human Services  HIPAA Health Insurance Portability and Accountability Act  HITRUST (formerly the Health Industry Trust Alliance)  IAB Institut fur¨ Arbeitsmarkt-und Berufsforschung (Germany’s Institute for Employment  and Research)  ICSP Interagency Council on Statistical Policy  ID Identifcation number  IEC International Electrotechnical Commission  IHE Integrating the Healthcare Enterprise  IHSN International Household Survey Network  IP internet protocol  IR inter-agency report  IRB institutional review board  IRS Internal Revenue Service  ISO (formerly International Organization for Standardization)  ISO/TS ISO Technical Standard  IT information technology  ITL Information Technology Laboratory  KIRP Known inclusion re-identifcation probability  MA Massachusetts  MCC Millennium Challenge Corporation  MD Maryland  MIT Massachusetts Institute of Technology  MPC multi-party computation  NEMA National Electrical Manufacturers Association  NIST National Institute of Standards and Technology  NISTIR National Institute of Standards and Technology interagency report  OECD Organisation for Economic Co-operation and Development  OHRP Offce for Human Research Protections  87  NIST SP 800-188 3pd  November 2022  3103  3104  3105  3106  3107  3108  3109  3110  3111  3112  3113  3114  3115  3116  3117  3118  3119  3120  3121  3122  3123  3124  3125  OMB Offce of Management and Budget  OPRE Offce of Planning, Research and Evaluation  PDF portable document fle  PEC privacy enhancing cryptography  PHI protected health information  PII personally identifable information  PL public law  PUF public use fle  RMP record matching probability  SDC statistical disclosure control  SDL statistical disclosure limitation  SHA secure hash algorithm  SLA service-level agreement  SP special publication  TEE trusted execution environments  TX Texas  UIRP Unknown inclusion re-identifcation probability  UK United kingdom  UKAN United Kingdom Advocacy Network  US United States  USC United States Code  WHISPERLAB Western Information Security and Privacy Research Laboratory  WP working paper  88  NIST SP 800-188 3pd  November 2022  3126  3127  3128  3129  3130  3131  3132  3133  3134  3135  3136  3137  3138  3139  3140  3141  3142  3143  3144  3145  3146  3147  3148  3149  3150  3151  3152  3153  3154  3155  3156  3157  3158  Appendix C. Glossary  Selected terms used in the publication are defned below. Where noted, the defnition is  sourced from another publication.  anonymization A process that removes the association between the identifying dataset  and the data subject. (ISO 25237-2008)  attribute An inherent characteristic. (ISO 9241-302:2008)  attribute disclosure Re-identifcation event in which an entity learns confdential infor- mation about a data principal, without necessarily identifying the data principal.  (ISO/IEC 20889 WORKING DRAFT 2 2016-05-27)  anonymity Condition in identifcation whereby an entity can be recognized as distinct,  without suffcient identity information to establish a link to a known identity. (ISO/IEC  24760-1:2011)  anticipated re-identifcation rate When an organization contemplates performing re-identifcation,  the re-identifcation rate that the resulting de-identifed data are likely to have.  attacker A person who seeks to exploit potential vulnerabilities of a system.  attribute Characteristic or property of an entity that can be used to describe its state, ap- pearance, or other aspect. (ISO/IEC 24760-1:2011)[65]  brute force attack In cryptography, an attack that involves trying all possible combina- tions to fnd a match.  characteristic Distinguishing feature. (ISO 8000-2:2012(E))  coded 1. Identifying information (such as name or social security number) that would  enable the investigator to readily ascertain the identity of the individual to whom  the private information or specimens pertain has been replaced with a number, let- ter, symbol, or combination thereof (i.e., the code); 2. A key to decipher the code  exists, enabling linkage of the identifying information to the private information or  specimens. [95]  control Measure that is modifying risk. Note: controls include any process, policy, device,  practice, or other actions which modify risk. (ISO/IEC 27000:2014)  covered entity Under HIPAA, a health plan, a health care clearinghouse, or a health care  provider that conducts certain health care transactions electronically (e.g., billing).  (HIPAA Privacy Rule)  data Re-interpretable representation of information in a formalized manner suitable for  communication, interpretation, or processing. (ISO 8000-2:2012(E))  89  NIST SP 800-188 3pd  November 2022  3159  3160  3161  3162  3163  3164  3165  3166  3167  3168  3169  3170  3171  3172  3173  3174  3175  3176  3177  3178  3179  3180  3181  3182  3183  3184  3185  3186  3187  3188  3189  data accuracy Closeness of agreement between a property value and the true value. (ISO  8000-2:2012(E)  data dictionary collection of data dictionary entries that allows lookup by entity identifer.  (ISO 8000-2:2012(E))  data dictionary entry Description of an entity type containing, at a minimum, an unam- biguous identifer, a term, and a defnition. (ISO 8000-2:2012(E))  data intruder A data user who attempts to disclose information about a population through  identifcation or attribution. (OECD Glossary of Statistical Terms)  data life cycle The set of processes in an application that transform raw data into action- able knowledge. (NIST SP 1500-1)  data subjects Persons to whom data refer. (ISO/TS 25237:2008)  data use agreement Executed agreement between a data provider and a data recipient that  specifes the terms under which the data can be used.  data universe All possible data within a specifed domain.  dataset A collection of data.  dataset with identifers A dataset that contains information that directly identifes indi- viduals.  dataset without identifers A dataset that does not contain direct identifers.  de-identifcation A process that is applied to a dataset with the goal of preventing or lim- iting informational risks to individuals, protected groups, and establishments, while  still allowing for the production of aggregate statistics.28  de-identifcation model An approach to the application of data de-identifcation tech- niques that enables the calculation of re-identifcation risk. (ISO/IEC 20889 WORK- ING DRAFT 2 2016-05-27)  de-identifcation process A general term for any process of removing the association be- tween a set of identifying data and the data principal. (ISO/TS 25237:2008)  de-identifed information Records that have had enough PII removed or obscured such  that the remaining information does not identify an individual, and there is no rea- sonable basis to believe that the information can be used to identify an individual.  (SP800-122)  direct identifying data Data that directly identify a single individual. (ISO/TS 25237:2008)  28ISO/TS 25237:2008 defnes de-identifcation as the “general term for any process of removing the associ- ation between a set of identifying data and the data subject” [66, p.3]. This document intentionally adopts  a broader defnition for de-identifcation that allows for noise-introducing techniques, such as differential  privacy and the creation of synthetic datasets that are based on privacy-preserving models.  90  NIST SP 800-188 3pd  November 2022  3190  3191  3192  3193  3194  3195  3196  3197  3198  3199  3200  3201  3202  3203  3204  3205  3206  3207  3208  3209  3210  3211  3212  3213  3214  3215  3216  3217  3218  3219  3220  3221  3222  3223  3224  3225  disclosure Divulging of, or provision of access to, data. (ISO/TS 25237:2008)  disclosure limitation Statistical methods used to hinder anyone from identifying an indi- vidual respondent or establishment by analyzing published data, especially by ma- nipulating mathematical and arithmetical relationships among the data. [p.21][130]  effectiveness The extent to which planned activities are realized and planned results achieved.  (ISO/IEC 27000:2014)  entity An item inside or outside an information and communication technology system,  such as a person, an organization, a device, a subsystem, or a group of such items  that has recognizably distinct existence. (ISO/IEC 24760-1:2011)  expert determination Within the context of de-identifcation, refers to the Expert Deter- mination method for de-identifying protected health information in accordance with  the HIPAA Privacy Rule de-identifcation standard.  Federal Committee on Statistical Methodology (FCSM) An interagency committee ded- icated to improving the quality of Federal statistics. The FCSM was created by the  Offce of Management and Budget (OMB) to inform and advise OMB and the Inter- agency Council on Statistical Policy (ICSP) on methodological and statistical issues  that affect the quality of Federal data. (fscm.sites.usa.gov)  genomic information Information based on an individual’s genome, such as a sequence  of DNA or the results of genetic testing.  harm Any adverse effects that would be experienced by an individual (i.e., that may be  socially, physically, or fnancially damaging) or an organization if the confdentiality  of PII were breached. (SP 800-122)  Health Insurance Portability and Accountability Act of 1996 (HIPAA) A federal statute  that called on the federal Department of Health and Human Services to establish reg- ulatory standards to protect the privacy and security of individually identifable health  information. See https://www.hhs.gov/hipaa/for-professionals/index.html.  HIPAA See Health Insurance Portability and Accountability Act of 1996.  HIPAA Privacy Rule Establishes national standards to protect individuals’ medical records  and other personal health information and applies to health plans, health care clear- inghouses, and those health care providers that conduct certain health care transac- tions electronically. (HIPAA Privacy Rule, 45 CFR 160, 162, 164). See https://www.hhs.gov/hipaa/for- professionals/privacy/index.html.  identifcation The process of using claimed or observed attributes of an entity to single  out the entity among other entities in a set of identities. (ISO/TS 25237:2008)  identifying information Information that can be used to distinguish or trace an individ- ual’s identity (e.g., their name, social security number, biometric records, etc.) alone  91  https://www.hhs.gov/hipaa/for https://www.hhs.gov/hipaa/for-professionals/index.html https://fscm.sites.usa.gov  NIST SP 800-188 3pd  November 2022  3226  3227  3228  3229  3230  3231  3232  3233  3234  3235  3236  3237  3238  3239  3240  3241  3242  3243  3244  3245  3246  3247  3248  3249  3250  3251  3252  3253  3254  3255  3256  3257  3258  3259  3260  3261  3262  or when combined with other personal or identifying information that is linked or  linkable to a specifc individual (e.g., date and place of birth, mother’s maiden name,  etc.). (OMB M-07-16)  identifer Information used to claim an identity, before a potential corroboration by a cor- responding authenticator. (ISO/TS 25237:2008)  imputation A procedure for entering a value for a specifc data item where the response is  missing or unusable. (OECD Glossary of Statistical Terms)  inference Refers to the ability to deduce the identity of a person associated with a set of  data through “clues” contained in that information. This analysis permits determi- nation of the individual’s identity based on a combination of facts associated with  that person even though specifc identifers have been removed, like name and social  security number. (ASTM E1869-04)[15]  information Knowledge concerning objects, such as facts, events, things, processes, or  ideas, including concepts, that within a certain context has a particular meaning.  (ISO 8000-2:2012(E))  k-anonymity A technique “to release person-specifc data such that the ability to link to  other information using the quasi-identifer is limited” [122]. k-anonymity achieves  this through suppression of identifers and output perturbation.  l-diversity A refnement to the k-anonymity approach that assures that groups of records  specifed by the same identifers have suffcient diversity to prevent inferential dis- closure. [76]  masking The process of systematically removing a feld or replacing it with a value in a  way that does not preserve the analytic utility of the value, such as replacing a phone  number with asterisks or a randomly generated pseudonym. [45]  motivated intruder test The ‘motivated intruder’ is taken to be a person who starts with- out any prior knowledge but who wishes to identify the individual from whose per- sonal data the anonymised data has been derived. This test is meant to assess whether  the motivated intruder would be successful. [63]  noise A convenient term for a series of random disturbances borrowed through communi- cation engineering, from the theory of sound. In communication theory, noise results  in the possibility of a signal sent, x, being different from the signal received, y, and  the latter has a probability distribution conditional upon x. If the disturbances con- sist of impulses at random intervals, it is sometimes known as “shot noise.” (OECD  Glossary of Statistical Terms)  non-deterministic noise A random value that cannot be predicted.  non-ignorable bias A bias introduced into data or an analytics procedure that results in a  change that cannot be ignored.  92  NIST SP 800-188 3pd  November 2022  3263  3264  3265  3266  3267  3268  3269  3270  3271  3272  3273  3274  3275  3276  3277  3278  3279  3280  3281  3282  3283  3284  3285  3286  3287  3288  3289  3290  3291  3292  3293  3294  3295  3296  3297  3298  3299  non-public personal information Information about a person that is not publicly known;  called “private information” in some other publications.  personal identifer Information with the purpose of uniquely identifying a person within  a given context. (ISO/TS 25237:2008)  personal data Any information relating to an identifed or identifable natural person (data  subject). (ISO/TS 25237:2008)  personal information See personal data.  personally identifable information (PII) Any information about an individual maintained  by an agency, including (1) any information that can be used to distinguish or trace  an individual’s identity, such as name, social security number, date and place of birth,  mother’s maiden name, or biometric records; and (2) any other information that is  linked or linkable to an individual, such as medical, educational, fnancial, and em- ployment information. [106](SP 800-122)  perturbation-based methods Perturbation-based methods falsify the data before publica- tion by introducing an element of error purposely for confdentiality reasons. This  error can be inserted in the cell values after the table is created, which means the  error is introduced to the output of the data and will therefore be referred to as output  perturbation, or the error can be inserted in the original data on the microdata level,  which is the input of the tables one wants to create; the method with then be referred  to as data perturbation—input perturbation being the better but uncommonly used  expression. Possible methods are: rounding; random perturbation; [and] disclosure  control methods for microstatistics applied to macrostatistics. (OECD Glossary of  Statistical Terms)  privacy Freedom from intrusion into the private life or affairs of an individual when that  intrusion results from undue or illegal gathering and use of data about that individual.  (ISO/IEC 2382-8:1998, defnition 08-01-23)  privacy risk  privacy loss A measure of the extent to which a data release may reveal information that  is specifc to an individual.  privacy loss budget An upper bound on the cumulative total privacy loss for individuals.  property value Instance of a specifc value together with an identifer for a data dictionary  entry that defnes a property. (ISO 8000-2:2012(E))  protected health information (PHI) Individually identifable health information: (1) Ex- cept as provided in paragraph (2) of this defnition, that is: (i) Transmitted by elec- tronic media; (ii) Maintained in electronic media; or (iii) Transmitted or maintained  in any other form or medium. (2) Protected health information excludes individu- ally identifable health information in: (i) Education records covered by the Fam-  93  NIST SP 800-188 3pd  November 2022  3300  3301  3302  3303  3304  3305  3306  3307  3308  3309  3310  3311  3312  3313  3314  3315  3316  3317  3318  3319  3320  3321  3322  3323  3324  3325  3326  3327  3328  3329  3330  3331  3332  ily Educational Rights and Privacy Act, as amended, 20 USC. 1232g; (ii) Records  described at 20 USC. 1232g(a)(4)(B)(iv); and (iii) Employment records held by a  covered entity in its role as employer. (HIPAA Privacy Rule, 45 CFR 160.103). See  https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html.  pseudonymization A particular type of de-identifcation that both removes the association  with a data subject and adds an association between a particular set of characteristics  related to the data subject and one or more pseudonyms.29 Typically, pseudonymiza- tion is implemented by replacing direct identifers with a pseudonym, such as a ran- domly generated value.  pseudonym Personal identifer that is different from the normally used personal identifer.  (ISO/TS 25237:2008)  quality Degree to which a set of inherent characteristics fulfls requirements. (ISO 8000- 2:2012(E))  quasi-identifer A variable that can be used to identify an individual through association  with another variable.  recipient Natural or legal person, public authority, agency, or any other body to whom  data are disclosed. (ISO/TS25237:2008)  redaction The removal of information from a document or dataset for legal or security  purposes.  re-identifcation A general term for any process that restores the association between a  set of de-identifed data and a data subject.  re-identifcation risk The likelihood that a third party can re-identify data subjects in a  de-identifed dataset.  re-identifcation rate The percentage of records in a dataset that can be re-identifed.  re-identifcaiton probability TBD  requirement A need or expectation that is stated, generally implied or obligatory. (ISO  8000-2:2012(E))  risk A measure of the extent to which an entity is threatened by a potential circumstance  or event, and typically a function of: (i) the adverse impacts that would arise if the  circumstance or event occurs; and (ii) the likelihood of occurrence. (CNSSI No.  4009)  risk assessment The process of identifying, estimating, and prioritizing risks to organi- zational operations (including mission, functions, image, reputation), organizational  29This defnition is the same as the defnition in ISO/TS 25237:2008, except that the word “anonymization”  is replaced with the word “de-identifcation.”  94  https://www.law.cornell.edu/uscode/text/20/1232g https://www.law.cornell.edu/uscode/text/20/1232g#a_4_B_iv https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html  NIST SP 800-188 3pd  November 2022  3333  3334  3335  3336  3337  3338  3339  3340  3341  3342  3343  3344  3345  3346  3347  3348  3349  3350  3351  3352  assets, individuals, other organizations, and the Nation, resulting from the operation  of an information system. Part of risk management, incorporates threat and vulner- ability analyses, and considers mitigations provided by security controls planned or  in place. Synonymous with risk analysis. (NIST SP 800-39)  safe harbor Within the context of de-identifcation, refers to the Safe Harbor method for  de-identifying protected health information in accordance with the Health Insurance  Portability and Accountability Act (HIPAA) Privacy Rule. See https://www.hhs.gov/hipaa/f professionals/privacy/special-topics/de-identifcation/index.html.  statistical disclosure control The set of methods to reduce the risk of disclosing informa- tion on individuals, businesses or other organizations. Such methods are only related  to the dissemination step and are usually based on restricting the amount of or modi- fying the data released. (OECD Glossary of Statistical Terms)  suppression One of the most commonly used ways of protecting sensitive cells in a table is  via suppression. It is obvious that in a row or column with a suppressed sensitive cell,  at least one additional cell must be suppressed, or the value in the sensitive cell could  be calculated exactly by subtraction from the marginal total. For this reason, certain  other cells must also be suppressed. These are referred to as secondary suppressions.  (OECD Glossary of Statistical Terms)  synthetic data generation A process in which seed data are used to create artifcial data  that have some of the statistical characteristics as the seed data.  or-  95  https://www.hhs.gov/hipaa/for  Executive Summary  Introduction  Document Purpose and Scope  Intended Audience  Organization  Introducing De-Identification  Historical Context  Terminology  Governance and Management of Data De-Identification  Identifying Goals and Intended Uses of De-Identification  Evaluating Risks that Arise from De-Identified Data Releases  Probability of Re-Identification  Adverse Impacts of Re-Identification  Impacts Other Than Re-Identification  Remediation  Data Life Cycle  Data-Sharing Models  The Five Safes  Disclosure Review Boards  De-Identification Standards  Benefits of Standards  Prescriptive De-Identification Standards  Performance-Based De-Identification Standards  Education, Training, and Research  Defense in Depth  Encryption and Access Control  Secure Computation  Trusted Execution Environments  Physical Enclaves  Technical Steps for Data De-Identification  Determine the Privacy, Data Usability, and Access Objectives  Conducting a Data Survey  De-Identification by Removing Identifiers and Transforming Quasi-Identifiers  Removing or Transforming of Direct Identifiers  Special Security Note Regarding the Encryption or Hashing of Direct Identifiers  De-Identifying Numeric Quasi-Identifiers  De-Identifying Dates  De-Identifying Geographical Locations and Geolocation Data  De-Identifying Genomic Information  De-Identifying Text Narratives and Qualitative Information  Challenges Posed by Aggregation Techniques  Challenges Posed by High-Dimensional Data  Challenges Posed by Linked Data  Challenges Posed by Composition  Potential Failures of De-Identification  Post-Release Monitoring  Synthetic Data  Partially Synthetic Data  Test Data  Fully Synthetic Data  Synthetic Data with Validation  Synthetic Data and Open Data Policy  Creating a Synthetic Dataset with Differential Privacy  De-Identifying with an Interactive Query Interface  Validating a De-Identified Dataset  Validating Data Usefulness  Validating Privacy Protection  Re-Identification Studies  Software Requirements, Evaluation, and Validation  Evaluating Privacy-Preserving Techniques  De-Identification Tools  De-Identification Tool Features  Data Provenance and File Formats  Data Masking Tools  Evaluating De-Identification Software  Evaluating Data Accuracy  Conclusion  References  Appendix Standards  NIST Publications  Other U.S. Government Publications  Selected Publications by Other Governments  Reports and Books  How-To Articles  Appendix List of Symbols, Abbreviations, and Acronyms  Appendix Glossary",
    "original document": "NIST SP 800-188 3pd (third public draft), De-Identifying Government Data Sets  1  2  3  4  5  6  7  8  9  10  11  12  13  NIST Special Publication  NIST SP 800-188 3pd  De-Identifying Government Data Sets  Third Public Draft  Simson Garfnkel  Phyllis Singer  Joseph Near  Aref N. Dajani  Barbara Guttman  This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.800-188.3pd  https://doi.org/10.6028/NIST.SP.800-188.3pd https://crossmark.crossref.org/dialog/?doi=10.6028/NIST.SP.800-188.3pd  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  NIST Special Publication  NIST SP 800-188 3pd  De-Identifying Government Data Sets  Third Public Draft  Simson Garfnkel  Barbara Guttman  Software Quality Group  Software and Systems Division  Joseph Near  Department of Computer Science  University of Vermont  Aref N. Dajani  Phyllis Singer  Center for Enterprise Dissemination  US Census Bureau  This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.800-188.3pd  November 2022  US Department of Commerce  Gina M. Raimondo, Secretary  National Institute of Standards and Technology  Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology  https://doi.org/10.6028/NIST.SP.800-188.3pd  40  45  50  55  60  65  70  75  80  38  39  41  42  43  44  46  47  48  49  51  52  53  54  56  57  58  59  61  62  63  64  66  67  68  69  71  72  73  74  76  77  78  79  81  82  Certain commercial entities, equipment, or materials may be identifed in this document in order to describe  an experimental procedure or concept adequately. Such identifcation is not intended to imply  recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to  imply that the entities, materials, or equipment are necessarily the best available for the purpose.  There may be references in this publication to other publications currently under development by NIST in  accordance with its assigned statutory responsibilities. The information in this publication, including  concepts and methodologies, may be used by federal agencies even before the completion of such  companion publications. Thus, until each publication is completed, current requirements, guidelines, and  procedures, where they exist, remain operative. For planning and transition purposes, federal agencies may  wish to closely follow the development of these new publications by NIST.  Organizations are encouraged to review all draft publications during public comment periods and provide  feedback to NIST. Many NIST cybersecurity publications, other than the ones noted above, are available at  https://csrc.nist.gov/publications.  This document is presented with the hope that its content may be of interest to the general privacy  community. The views in this document are those of the authors, and do not represent those of the US  Census Bureau.  Authority  This publication has been developed by NIST in accordance with its statutory responsibilities under the  Federal Information Security Modernization Act (FISMA) of 2014, 44 U.S.C. § 3551 et seq., Public Law  (P.L.) 113-283. NIST is responsible for developing information security standards and guidelines, including  minimum requirements for federal information systems, but such standards and guidelines shall not apply to  national security systems without the express approval of appropriate federal offcials exercising policy  authority over such systems. This guideline is consistent with the requirements of the Offce of Management  and Budget (OMB) Circular A-130.  Nothing in this publication should be taken to contradict the standards and guidelines made mandatory and  binding on federal agencies by the Secretary of Commerce under statutory authority. Nor should these  guidelines be interpreted as altering or superseding the existing authorities of the Secretary of Commerce,  Director of the OMB, or any other federal offcial. This publication may be used by nongovernmental  organizations on a voluntary basis and is not subject to copyright in the United States. Attribution would,  however, be appreciated by NIST.  NIST Technical Series Policies  Copyright, Fair Use, and Licensing Statements  NIST Technical Series Publication Identifer Syntax  Publication History  Approved by the NIST Editorial Review Board on YYYY-MM-DD [will be added upon fnal publication]  How to cite this NIST Technical Series Publication:  Garfnkel S, Guttman B, Near J, Dajani AN, Singer P (2022) De-Identifying Government Data  Sets. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Special Publication (SP)  NIST SP 800-188 3pd. https://doi.org/10.6028/NIST.SP.800-188.3pd  Author ORCID iDs  Simson Garfnkel: 0000-0003-1294-2831  Joseph Near: 0000-0002-3203-3742  Aref N. Dajani: 0000-0003-0361-5409  Phyllis Singer: 0000-0002-8885-7273  Public Comment Period  https://doi.org/10.6028/NIST.SP.800-188.3pd  83  84  85  86  87  88  89  November 15, 2022 – January 15, 2023  Submit Comments  sp800-188-draft@nist.gov  National Institute of Standards and Technology  Attn: Software and Systems Division, Information Technology Laboratory  100 Bureau Drive (Mail Stop 8970) Gaithersburg, MD 20899-8970  All comments are subject to release under the Freedom of Information Act (FOIA).  NIST SP 800-188 3pd  November 2022  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  Abstract  De-identifcation is a process that is applied to a dataset with the goal of preventing or  limiting informational risks to individuals, protected groups, and establishments while still  allowing for meaningful statistical analysis. Government agencies can use de-identifcation  to reduce the privacy risk associated with collecting, processing, archiving, distributing,  or publishing government data. Previously, NISTIR 8053, De-Identifcation of Personal  Information [51], provided a survey of de-identifcation and re-identifcation techniques.  This document provides specifc guidance to government agencies that wish to use de- identifcation. Before using de-identifcation, agencies should evaluate their goals for us- ing de-identifcation and the potential risks that de-identifcation might create. Agencies  should decide upon a de-identifcation release model, such as publishing de-identifed data,  publishing synthetic data based on identifed data, or providing a query interface that incor- porates de-identifcation. Agencies can create a Disclosure Review Board to oversee the  process of de-identifcation. They can also adopt a de-identifcation standard with measur- able performance levels and perform re-identifcation studies to gauge the risk associated  with de-identifcation. Several specifc techniques for de-identifcation are available, in- cluding de-identifcation by removing identifers and transforming quasi-identifers and the  use of formal privacy models. People performing de-identifcation generally use special- purpose software tools to perform the data manipulation and calculate the likely risk of  re-identifcation. However, not all tools that merely mask personal information provide  suffcient functionality for performing de-identifcation. This document also includes an  extensive list of references, a glossary, and a list of specifc de-identifcation tools, which is  only included to convey the range of tools currently available and is not intended to imply  a recommendation or endorsement by NIST.  Keywords  data life cycle; de-identifcation; differential privacy; direct identifers; Disclosure Re- view Board; the fve safes; k-anonymity; privacy; pseudonymization; quasi-identifers;  re-identifcation; synthetic data.  Reports on Computer Systems Technology  The Information Technology Laboratory (ITL) at the National Institute of Standards and  Technology (NIST) promotes the U.S. economy and public welfare by providing technical  leadership for the Nation’s measurement and standards infrastructure. ITL develops tests,  test methods, reference data, proof of concept implementations, and technical analyses to  advance the development and productive use of information technology. ITL’s responsi- bilities include the development of management, administrative, technical, and physical  standards and guidelines for the cost-effective security and privacy of other than national  security-related information in federal information systems. The Special Publication 800-  i  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  series reports on ITL’s research, guidelines, and outreach efforts in information system  security, and its collaborative activities with industry, government, and academic organiza- tions.  Call for Patent Claims  This public review includes a call for information on essential patent claims (claims whose  use would be required for compliance with the guidance or requirements in this Information  Technology Laboratory (ITL) draft publication). Such guidance and/or requirements may  be directly stated in this ITL Publication or by reference to another publication. This call  also includes disclosure, where known, of the existence of pending U.S. or foreign patent  applications relating to this ITL draft publication and of any relevant unexpired U.S. or  foreign patents.  ITL may require from the patent holder, or a party authorized to make assurances on its  behalf, in written or electronic form, either:  1. assurance in the form of a general disclaimer to the effect that such party does not  hold and does not currently intend holding any essential patent claim(s); or  2. assurance that a license to such essential patent claim(s) will be made available to ap- plicants desiring to utilize the license for the purpose of complying with the guidance  or requirements in this ITL draft publication either:  (a) under reasonable terms and conditions that are demonstrably free of any unfair  discrimination; or  (b) without compensation and under reasonable terms and conditions that are demon- strably free of any unfair discrimination.  Such assurance shall indicate that the patent holder (or third party authorized to make assur- ances on its behalf) will include in any documents transferring ownership of patents subject  to the assurance, provisions suffcient to ensure that the commitments in the assurance are  binding on the transferee, and that the transferee will similarly include appropriate provi- sions in the event of future transfers with the goal of binding each successor-in-interest.  The assurance shall also indicate that it is intended to be binding on successors-in-interest  regardless of whether such provisions are included in the relevant transfer documents.  Such statements should be addressed to: sp800-188-draft@nist.gov  ii  157  158  NIST SP 800-188 3pd  November 2022  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  Table of Contents  Executive Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1  1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3  1.1. Document Purpose and Scope . . . . . . . . . . . . . . . . . . . . . . . . . . 7  1.2. Intended Audience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7  1.3. Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7  2. Introducing De-Identifcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8  2.1. Historical Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8  2.2. Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10  3. Governance and Management of Data De-Identifcation . . . . . . . . . . . . . . 17  3.1. Identifying Goals and Intended Uses of De-Identifcation . . . . . . . . . . . 17  3.2. Evaluating Risks that Arise from De-Identifed Data Releases . . . . . . . . 18  3.2.1. Probability of Re-Identifcation . . . . . . . . . . . . . . . . . . . . . 19  3.2.2. Adverse Impacts of Re-Identifcation . . . . . . . . . . . . . . . . . . 22  3.2.3. Impacts Other Than Re-Identifcation . . . . . . . . . . . . . . . . . 23  3.2.4. Remediation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24  3.3. Data Life Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24  3.4. Data-Sharing Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29  3.5. The Five Safes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30  3.6. Disclosure Review Boards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31  3.7. De-Identifcation Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36  3.7.1. Benefts of Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . 36  3.7.2. Prescriptive De-Identifcation Standards . . . . . . . . . . . . . . . . 36  3.7.3. Performance-Based De-Identifcation Standards . . . . . . . . . . . 37  3.8. Education, Training, and Research . . . . . . . . . . . . . . . . . . . . . . . . 38  3.9. Defense in Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38  3.9.1. Encryption and Access Control . . . . . . . . . . . . . . . . . . . . . 38  3.9.2. Secure Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . 38  3.9.3. Trusted Execution Environments . . . . . . . . . . . . . . . . . . . . 39  3.9.4. Physical Enclaves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39  4. Technical Steps for Data De-Identifcation . . . . . . . . . . . . . . . . . . . . . . 40  iii  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  4.1. Determine the Privacy, Data Usability, and Access Objectives . . . . . . . . 40  4.2. Conducting a Data Survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41  4.3. De-Identifcation by Removing Identifers and Transforming Quasi-Identifers 43  4.3.1.  4.3.2.  4.3.3.  4.3.4.  4.3.5.  4.3.6.  4.3.7.  4.3.8.  4.3.9.  Removing or Transforming of Direct Identifers . . . . . . . . . . . . 44  Special Security Note Regarding the Encryption or Hashing of Di- rect Identifers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46  De-Identifying Numeric Quasi-Identifers . . . . . . . . . . . . . . . . 46  De-Identifying Dates . . . . . . . . . . . . . . . . . . . . . . . . . . . 48  De-Identifying Geographical Locations and Geolocation Data . . . 49  De-Identifying Genomic Information . . . . . . . . . . . . . . . . . . 49  De-Identifying Text Narratives and Qualitative Information . . . . . 51  Challenges Posed by Aggregation Techniques . . . . . . . . . . . . . 51  Challenges Posed by High-Dimensional Data . . . . . . . . . . . . . 52  4.3.10. Challenges Posed by Linked Data . . . . . . . . . . . . . . . . . . . . 52  4.3.11. Challenges Posed by Composition . . . . . . . . . . . . . . . . . . . 52  4.3.12. Potential Failures of De-Identifcation . . . . . . . . . . . . . . . . . 53  4.3.13. Post-Release Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . 54  4.4. Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54  4.4.1.  4.4.2.  4.4.3.  4.4.4.  4.4.5.  4.4.6.  Partially Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . 55  Test Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56  Fully Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 56  Synthetic Data with Validation . . . . . . . . . . . . . . . . . . . . . 58  Synthetic Data and Open Data Policy . . . . . . . . . . . . . . . . 58  Creating a Synthetic Dataset with Diferential Privacy . . . . . . . 58  4.5. De-Identifying with an Interactive Query Interface . . . . . . . . . . . . . . 59  4.6. Validating a De-Identifed Dataset . . . . . . . . . . . . . . . . . . . . . . . . 60  4.6.1. Validating Data Usefulness . . . . . . . . . . . . . . . . . . . . . . . 60  4.6.2. Validating Privacy Protection . . . . . . . . . . . . . . . . . . . . . . 60  4.6.3. Re-Identifcation Studies . . . . . . . . . . . . . . . . . . . . . . . . . 61  5. Software Requirements, Evaluation, and Validation . . . . . . . . . . . . . . . . . 63  5.1. Evaluating Privacy-Preserving Techniques . . . . . . . . . . . . . . . . . . . 63  5.2. De-Identifcation Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64  iv  222  223  224  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  5.2.1. De-Identifcation Tool Features . . . . . . . . . . . . . . . . . . . . . 64  5.2.2. Data Provenance and File Formats . . . . . . . . . . . . . . . . . . . 64  5.2.3. Data Masking Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . 64  5.3. Evaluating De-Identifcation Software . . . . . . . . . . . . . . . . . . . . . . 65  5.4. Evaluating Data Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65  6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66  References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80  Appendix A. Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81  A.1. NIST Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81  A.2. Other U.S. Government Publications . . . . . . . . . . . . . . . . . . . . . . 82  Selected Publications by Other Governments . . . . . . . . . . . . . . . . . . . . . 83  Reports and Books . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83  How-To Articles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84  Appendix B. List of Symbols, Abbreviations, and Acronyms . . . . . . . . . . . . . . 86  Appendix C. Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89  List of Tables  Table 1. Reading levels at a hypothetical school, as measured by entrance exami- nations, reported at the start of the school year on October 1. . . . . . . . 51  Table 2. Reading levels at a hypothetical school, as measured by entrance exami- nations, reported one month into the school year on November 1 after a  new student has transferred to the school. . . . . . . . . . . . . . . . . . . . 51  Table 3. Adjectives used for describing data in data releases. . . . . . . . . . . . . . 55  List of Figures  Fig. 1. The data life cycle as described by Michener et al. [80] . . . . . . . . . . . 25  Fig. 2. Chisholm’s view of the data life cycle is a linear process with a branching  point after data usage [25] . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25  Fig. 3. Altman’s “modern approach to privacy-aware government data releases” [75] 26  Fig. 4. Altman’s conceptual diagram of the relationship between post-transformation  identifability, level of expected harm, and suitability of selected privacy  controls for a data release [75] . . . . . . . . . . . . . . . . . . . . . . . . . . 27  Fig. 5. Advice for Practitioners: A Summary . . . . . . . . . . . . . . . . . . . . . . 68  v  NIST SP 800-188 3pd  November 2022  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  Acknowledgments  The authors wish to thank the U.S. Census Bureau for its help in researching and preparing this  publication, with specifc thanks to John Abowd, Ron Jarmin, Christa Jones, and Laura McKenna.  The authors would also like to thank Luk Arbuckle, Andrew Baker, Daniel Barth-Jones, Christi  Dant, Khaled El Emam, Robert Gellman, Tom Krenzke, Bradley Malin, Kevin Mangold, John  Moehrke, Linda Sanchez, Denise Sturdy, and Chris Traver for providing comments on previous  drafts and their valuable insights, all of which were helpful in creating this publication.  The authors also wish to thank several organizations that provided useful comments on previous  drafts of this publication: the Defense Contract Management Agency (DCMA) Information As- surance Directorate; the Offce of Chief Privacy Offcer within the U.S. Department of Education;  the Offce of Planning, Research, and Evaluation (OPRE) within the Administration for Children  and Families at the U.S. Department of Health and Human Services; the Millennium Challenge  Corporation (MCC) Department of Policy and Evaluation; Integrating the Healthcare Enterprise  (IHE), an ANSI-accredited standards organization focused on healthcare standards; and the Privacy  Tools project at Harvard University (including Micah Altman, Stephen Chong, Kobbi Nissim, David  O’Brien, Salil Vadhan, and Alexandra Wood).  Author Contributions  Simson Garfnkel: Conceptualization, Supervision, Writing (original draft preparation); Joseph  Near: Writing (original draft preparation); Aref N. Dajani: Writing (original draft preparation of  the section on reidentifcation studies); Phyllis Singer: Writing (original draft preparation of the  section on reidentifcation studies).  vi  NIST SP 800-188 3pd  November 2022  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304  305  306  307  308  309  310  311  312  Executive Summary  Every federal agency creates and maintains internal datasets that are vital for fulflling its  mission. The Foundation for Evidence-based Policymaking Act of 2018 [2] mandates that  agencies also collect and publish their government data in open, machine-readable formats,  when it is appropriate to do so. Agencies can use de-identifcation to make government  datasets available while protecting the privacy of the individuals whose data are contained  within those datasets.  Many Government documents use the phrase personally identifable information (PII) to  describe private information that can be linked to an individual [62, 79], although there are  a variety of defnitions for PII. As a result, it is possible to have information that singles  out individuals but that does not meet a specifc defnition of PII. This document therefore  presents ways of removing or altering information that can identify individuals that go  beyond merely removing PII.  For decades, de-identifcation based on simply removing of identifying information was  thought to be suffcient to prevent the re-identifcation of individuals in large datasets. Since  the mid 1990s, a growing body of research has demonstrated the reverse, resulting in new  privacy attacks capable of re-identifying individuals in “de-identifed” data releases. For  several years the goals of such attacks appeared to be the embarrassment of the publishing  agency and achieving academic distinction for the privacy researcher [50]. More recently,  as high-resolution de-identifed geolocation data has become commercially available, re- identifcation techniques have been used by journalists and activists [100, 140, 70] with the  goal of learning confdential information.  These attacks have become more sophisticated in recent years with the availability of ge- olocation data, highlighting the defciencies in traditional  Formal models of privacy, like k-anonymity [122] and differential privacy, [39] use math- ematically rigorous approaches that are designed to allow for the controlled use of conf- dential data while minimizing the privacy loss suffered by the data subjects. Because there  is an inherent trade-off between the accuracy of published data and the amount of privacy  protection afforded to data subjects, most formal methods have some kind of parameter  that can be adjusted to control the “privacy cost” of a particular data release. Informally, a  data release with a low privacy cost causes little additional privacy risk to the participants,  while a higher privacy cost results in more privacy risk. When they are available, formal  privacy methods shoudl be preferred over informal, ad hoc methods.  Decisions and practices regarding the de-identifcation and release of government data can  be integral to the mission and proper functioning of a government agency. As such, an  agency’s leadership should manage these activities in a way that assures performance and  results in a manner that is consistent with the agency’s mission and legal authority. One way  that agencies can manage this risk is by creating a formal Disclosure Review Board (DRB)  that consists of legal and technical privacy experts, stakeholders within the organization,  1  NIST SP 800-188 3pd  November 2022  313  314  315  316  317  318  319  320  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  and representatives of the organization’s leadership. The DRB evaluated applications for  data release that describe the confdential data, the techniques that will be used to mini- mize the risk of disclosure, the resulting protected data, and how the effectiveness of those  techniques will be evaluated.  Establishing a DRB may seem like an expensive and complicated administrative under- taking for some agencies. However, a properly constituted DRB and the development of  consistent procedures regarding data release should enable agencies to lower the risks as- sociated with each data release, which is likely to save agency resources in the long term.  Agencies can create or adopt standards to guide those performing de-identifcation, and  regarding regarding the accuracy of de-identifed data. If accuracy goals exist, then tech- niques such as differential privacy can be used to make the data suffciently accurate for the  intended purpose but not unnecessarily more accurate, which can limit the amount of pri- vacy loss. However, agencies must carefully choose and implement accuracy requirements.  If data accuracy and privacy goals cannot be well-maintained, then releases of data that are  not suffciently accurate can result in incorrect scientifc conclusions and policy decisions.  Agencies should consider performing de-identifcation with trained individuals using soft- ware specifcally designed for the purpose. While it is possible to perform de-identifcation  with off-the-shelf software like a commercial spreadsheet or fnancial planning program,  such programs typically lack the key functions required for proper de-identifcation. As a  result, they may encourage the use of simplistic de-identifcation methods, such as deleting  sensitive columns and manually searching and removing data that appears sensitive. This  may result in a dataset that appears de-identifed but that still contain signifcant disclosure  risks.  Finally, different countries have different standards and policies regarding the defnition and  use of de-identifed data. Information that is regarded as de-identifed in one jurisdiction  may be regarded as being identifable in another.  2  NIST SP 800-188 3pd  November 2022  339  340  341  342  343  344  345  346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368  369  370  371  372  373  374  375  376  1. Introduction  The U.S. Government collects, maintains, and uses many kinds of datasets. Every federal  agency creates and maintains internal datasets that are vital for fulflling its mission, such  as delivering services to taxpayers or ensuring regulatory compliance. There are also 13  principal federal statistical agencies, three recognized statistical units, and over 100 other  federal statistical programs that collect, compile, process, analyze, and distribute informa- tion for statistical purposes [126, 92].  Government programs collect information from individuals and organizations for taxation,  public benefts, public health, licensing, employment, censuses, and the production of of- fcial statistics. While privacy is integral, many individuals and organizations that provide  information to the Government do not typically have the right to opt-out of such requests.  For example, people and establishments in the United States are required by law to respond  to mandatory U.S. Census Bureau surveys.  Agencies make many of their datasets available to the public. The U.S. Government  publishes data to promote commerce, scientifc research, and public transparency. Many  datasets contain some data elements that should not be made public, and it is necessary to  remove such information before making the rest of the dataset available. Some datasets  are so sensitive that they cannot be made publicly available at all but can be available on a  limited basis to qualifed, vetted researchers in protected enclaves. In some cases, agencies  may also elect to release summary statistics of sensitive data or create synthetic datasets  that resemble the original data but that have a lower disclosure risk [8].  There is frequent tension between the goals of privacy protection and the release of useful  data to the public. One way that the Government attempts to resolve this tension is with  an offcial promise of confdentiality to individuals and organizations regarding the infor- mation that they provide [102]. A bedrock principle of offcial statistical programs is that  data provided to the Government should generally remain confdential and not be used in a  way that could harm the individual or the organization providing the data. One justifcation  for this principle is that it helps to ensure high data accuracy. If data providers did not feel  that the information they provide would remain confdential, they might not be willing to  provide information that is accurate.  Other information is created by the Government as a consequence of providing government  services. This information – sometimes called administrative data – is also increasingly  being used and made available for statistical purposes and must be protected.  In 2018, the U.S. Congress passed three laws that signifcantly increased the need for ex- pertise regarding privacy-preserving data analysis and data publishing techniques, such as  de-identifcation:  1. The Foundations for Evidence-Based Policymaking Act of 2018 [2], commonly  called the Evidence Act, requires federal agencies to track all of their data in data  3  NIST SP 800-188 3pd  November 2022  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  inventories, report public datasets to https://data.gov, perform systematic evidence- making and evaluation activities, and engage in capacity-building so that the federal  workforce can meet the requirements of data-centric, evidence-based operations. The  Evidence Act is based on the fndings of the U.S. Commission on Evidence-Based  Policymaking [27] and is implemented in part by OMB Memorandum M-19-23  [139].  The Evidence Act contains specifc guidance requiring that agencies publishing data  take into account “(A) risks and restrictions related to the disclosure of personally  identifable information, including the risk that an individual data asset in isolation  does not pose a privacy or confdentiality risk but when combined with other available  information may pose such a risk;” and “(B) security considerations, including the  risk that information in an individual data asset in isolation does not pose a security  risk but when combined with other available information may pose such a risk” [2].  2. The Open Government Data Act, which was passed as part of the Evidence Act,  requires that the U.S. Government publish data in machine-readable, open, non- proprietary formats when possible. This act largely codifed presidential Executive  Order 13642 of May 9, 2013, “Making Open and Machine Readable the New Default  for Government Information” [88] and its implementation in OMB Memorandum M- 13-13 [18].  3. The Geospatial Data Act of 2018, which requires that government agencies make  inventories of their geospatial data and that public geospatial data be registered on  the U.S. Government’s public geospatial platform, https://www.geoplatform.gov/.  Other laws, regulations, and policies that govern the release of statistics and data to the  public enshrine this principle of confdentiality. For example:  • The Confdential Information Protection and Statistical Effciency Act of 2002  states, “data or information acquired by an agency under a pledge of confdentiality  for exclusively statistical purposes shall not be disclosed by an agency in identif- able form for any use other than an exclusively statistical purpose, except with the  informed consent of the respondent.” [126, §512 (b)(1)] Commonly called CIPSEA,  the act further requires that federal statistical agencies “establish appropriate admin- istrative, technical, and physical safeguards to ensure the security and confdentiality  of records and to protect against any anticipated threats or hazards to their security  or integrity which could result in substantial harm, embarrassment, inconvenience,  or unfairness to any individual on whom information is maintained.”  • US Code Title 13, Section 9 governs the confdentiality of information provided to  the Census Bureau and prohibits “any publication whereby the data furnished by any  particular establishment or individual under this title can be identifed” [130].  • US Code Title 26, Section 6103 governs the confdentiality of information provided  to the U.S. Government on tax returns and other return information. These rules are  4  https://data.gov https://www.geoplatform.gov/  NIST SP 800-188 3pd  November 2022  416  417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448  449  450  now spelled out in IRS Publication 1075, “Tax Information Security Guidelines for  Federal, State and Local Agencies,” published by the IRS Offce of Safeguards [93].  • The Privacy Act of 1974 covers the release of personal information of U.S. citizens  and Lawful Permanent Residents by the Government. The Act recognizes that the  disclosure of records for statistical purposes is acceptable if the data are not “indi- vidually identifable” [103, at a(b)(5)].  Minimizing privacy risk is not an absolute goal of federal laws and regulations. Guidance  from the U.S. Department of Health and Human Services (HHS) on the Health Insurance  Portability and Accountability Act (HIPAA) de-identifcation standards notes that ”[b]oth  methods [the safe harbor and expert determination methods for de-identifcation], even  when properly applied, yield de-identifed data that retains some risk of identifcation. Al- though the risk is very small, it is not zero, and there is a possibility that de-identifed data  could be linked back to the identity of the patient to which it corresponds” [136].  U.S. law also balances privacy risk with other factors, such as transparency, accountabil- ity, and the opportunity for public good. An example of this balance is the handling of  personally identifable information collected by the Census Bureau as part of the decennial  census: this information remains confdential for 72 years and is then transferred to the  National Archives and Records Administration where it is released to the public [131, 5].  De-identifcation is a process that is applied to a dataset with the goal of preventing or lim- iting privacy risks to individuals, protected groups, and establishments while still allowing  for the production of aggregate statistics.1 De-identifcation is not a single technique, but  a collection of approaches, algorithms, and tools that can be applied to different kinds of  data with differing levels of effectiveness. In general, the potential risk to privacy posed by  a dataset’s release decreases as more aggressive de-identifcation techniques are employed,  but data accuracy and – in some cases – the ultimate utility of the de-identifed dataset  decreases as well.  Accuracy is traditionally defned as the “closeness of computations or estimates to the exact  or true values that the statistics were intended to measure” [9]. The data accuracy of  de-identifed data, therefore, refers to the degree to which inferences drawn on the de- identifed data will be consistent with inferences drawn on the original data. Data accuracy  can be measured by the ratio of a value computed with de-identifed data to the same value  computed using the underlying true confdential value.  In economics, Utility is traditionally defned as “the satisfaction derived from consumption  of a good or service”[138]. Data utility therefore refers to the value that data users can de- rive from data in general. When speaking of de-identifed data, utility comes from two pub-  1In Europe, the term data anonymization is frequently used as a synonym for de-identifcation, but the terms  may have subtly different defnitions in some contexts. For a more complete discussion of de-identifcation  and data anonymization, see NISTIR 8053, De-Identifcation of Personal Data [51].  5  NIST SP 800-188 3pd  November 2022  451  452  453  454  455  456  457  458  459  460  461  462  463  464  465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480  481  482  483  484  485  486  lic goods: the uses of the data and the privacy protection afforded by the de-identifcation  process.  This document uses the phrase data accuracy to refer to the abstract characteristic of the  data as determined by a specifc, measurable statistic, whereas data utility refers to the ben- eft derived from the application of the data to a specifc use. Although there has previously  been a tendency within offcial statistical organizations to confate these two terms, it is im- portant to keep them distinct because they are not necessary correlated. Data may have low  accuracy because they contain errors or substantial noise, yet users may nevertheless derive  high value from the data, giving the data high utility. Likewise, data that are very close to  the reality of the thing being measured may have high accuracy but may be fundamentally  worthless and, thus, have low utility.  In general, data accuracy decreases as more aggressive de-identifcation techniques are  employed. Therefore, any effort that involves the release of data that contain personal  information typically involves making a trade-off between identifability and data accuracy.  However, increased privacy protections do not necessarily result in decreased data utility.  Some users of de-identifed data may be able to use the data to make inferences about  private facts regarding the data subjects. They may even be able to re-identify the data  subjects. Both of these uses undo the privacy goals of de-identifcation. Agencies that  release data should understand what data they are releasing, what other data may already  be publicly or privately available, and the risk of re-identifcation. Agencies should aim to  make an informed decision about the fdelity of the data that they release by systematically  evaluating the risks and benefts and choosing de-identifcation techniques and data sharing  models that are tailored to their requirements. In addition, when telling individuals that  their de-identifed information will be released, agencies should disclose that privacy risks  may remain despite de-identifcation.  Planning is essential for successful de-identifcation and data release. In a research envi- ronment, this planning should include the research design, data collection, protection of  identifers, disclosure analysis, and data-sharing strategy. In an operational environment,  this planning includes a comprehensive analysis of the purpose of the data release and the  expected use of the released data, the privacy-related risks, and the privacy protecting con- trols. Both cases should review the appropriateness of various privacy controls given the  risks, intended uses, and the ways that those controls could fail.  De-identifcation can have signifcant costs, including time, labor, and data processing  costs. However, when properly executed, this effort can result in data that have high value  for a research community and the general public while still adequately protecting individual  privacy.  6  NIST SP 800-188 3pd  November 2022  487  488  489  490  491  492  493  494  495  496  497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512  513  514  515  516  517  518  1.1. Document Purpose and Scope  This document provides guidance on the selection, use, and evaluation of de-identifcation  techniques for U.S. Government datasets. It also provides a framework that can be adapted  by federal agencies to shape the governance of de-identifcation processes. The ultimate  goal of this document is to reduce disclosure risks that might result from an intentional data  release.  1.2. Intended Audience  This document is intended for use by government engineers, data scientists, privacy off- cers, disclosure review boards, and other offcials. It is also designed to be generally infor- mative to researchers and academics involved in the technical aspects of the de-identifcation  of government data. While this document assumes a high-level understanding of informa- tion system security technologies, it is intended to be accessible to a wide audience.  1.3. Organization  The remainder of this publication is organized as follows:  • Section 2, “Introducing De-Identifcation,” presents a background on the science  and terminology of de-identifcation.  • Section 3, “Governance and Management of Data De-Identifcation,” provides  guidance to agencies on the establishment of or improvement to a program that makes  privacy-sensitive data available to researchers and the public.  • Section 4, “Technical Steps for Data De-Identifcation,” provides specifc tech- nical guidance for performing de-identifcation using a variety of mathematical ap- proaches.  • Section 5, “Software Requirements, Evaluation, and Validation,” provides a rec- ommended set of features that should be in de-identifcation tools, which may be use- ful for potential purchasers or developers of such software. This section also provides  information for evaluating both de-identifcation tools and de-identifed datasets.  • Section 6, “Conclusion,” Section 6 is the conclusion.  Following the conclusion, this document provides a list of all publications referenced  in this document, as well as an Appendix that includes standards, related NIST pub- lications, other selected publications by the US and other governments, reports and  books, and a few articles of interest. A second appendix provides a list of symbols,  abbreviations and acronyms. The third appendix contains a glossary.  7  NIST SP 800-188 3pd  November 2022  519  520  521  522  523  524  525  526  527  528  529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544  545  546  547  548  549  550  551  552  553  554  2. Introducing De-Identifcation  This document presents recommendations for de-identifying government datasets.  If the information derived from personal data remains in a de-identifed dataset, the dataset  might inadvertently reveal attributes related to specifc individuals, specifc de-identifed  records could be linked back to specifc individuals. When this happens, the privacy pro- tection provided by de-identifcation is compromised. Even if a specifc individual cannot  be matched to a specifc data record, de-identifed data can be used to improve the accu- racy of inferences regarding individuals whose de-identifed data are in the dataset. This  so-called inference risk cannot be eliminated if there is any information in the de-identifed  data, but it can be minimized. Thus, the decision of how or whether to de-identify data  should be made in conjunction with decisions over how the de-identifed data will be used,  shared, or released.  De-identifcation is especially important for government agencies, businesses, and other or- ganizations that seek to make data available to outsiders. For example, signifcant medical  research resulting in societal beneft is made possible by the sharing of de-identifed patient  information under the framework established by the HIPAA Privacy Rule, the primary U.S.  regulation that provides for the privacy of medical records; billing records; enrollment, pay- ment, and claims records; and “other records that are used, in whole or in part, by or for  the covered entity to make decisions about individuals” [90]. The HIPAA Privacy Rule de- identifcation framework applies to both government organizations charged with protecting  government datasets as well as to private sector organizations, such as health plans and  health care providers.  Agencies may also be required to de-identify records when responding to a Freedom of  Information Act (FOIA) [134, 133] request in a manner that is consistent with Exemption  6, which protects information about individuals in “personnel and medical fles and similar  fles” when the disclosure of such information “would constitute a clearly unwarranted in- vasion of personal privacy,” and Exemption 7(C), which is limited to information compiled  for law enforcement purposes and protects personal information when disclosure “could  reasonably be expected to constitute an unwarranted invasion of personal privacy.” The  meaning of these exemptions has been clarifed by multiple cases before the US Supreme  Court [105, 117, 118].  2.1. Historical Context  The modern practice of de-identifcation comes from three overlapping intellectual tradi- tions.  1. For four decades, offcial statistical agencies have researched and investigated meth- ods broadly termed Statistical Disclosure Limitation (SDL) or Statistical Disclosure  8  NIST SP 800-188 3pd  November 2022  555  556  557  558  559  560  561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576  577  578  579  580  581  582  583  584  585  586  587  588  589  590  Control [29, 36].2 Statistical agencies created these methods so that they could re- lease statistical tables and public use fles (PUF) to allow users to learn information  and perform original research while protecting the privacy of the individuals in the  dataset. SDL is widely used in contemporary statistical reporting.  2. In the 1990s, there was a signifcant increase in the release of microdata fles for  public use in the form of both individual responses from surveys and administrative  records. Initially, these releases merely stripped obviously identifying information,  such as names and social security numbers (what are now called direct identifers).  Following some releases, researchers discovered that it was possible to re-identify  individuals’ data by triangulating with some of the remaining data (now called quasi- identifers or indirect identifers [28]). The research resulted in the invention of the  k-anonymity model for protecting privacy [124, 108, 109, 123] , which is refected  in the Offce of Civil Rights guidance on how to apply de-identifcation in a manner  consistent with the HIPAA Privacy Rule [89]. Today, variants of k-anonymity are  commonly used to allow for the sharing of medical microdata. This intellectual tra- dition is typically called de-identifcation, although this document uses that term to  describe all three intellectual traditions.  3. In the 2000s, research in theoretical computer science and cryptography developed  the theory of differential privacy [40], which is based on a mathematical defnition  of the privacy loss to an individual that results from queries on a database containing  that individual’s personal information. Differential privacy is termed a formal model  for privacy protection because its defnitions for privacy and privacy loss are based on  mathematical proofs.3 This does not mean that algorithms that implement differen- tial privacy cannot result in increased privacy risk. Rather, it means that the amount  of privacy risk that results from the use of these algorithms can be mathematically  bounded. These mathematical limits on privacy risk have created considerable inter- est in differential privacy in academia, commerce, and business. To date, however,  only a few systems that utilize differential privacy have been operationally deployed.  uring the frst decade of the 21st century, there was a growing awareness within the U.S.  overnment about the risks that could result from the improper handling and inadvertent  elease of personal identifying and fnancial information. This realization, combined with  growing number of inadvertent data disclosures within the U.S. Government, resulted  n President George Bush signing Executive Order 13402, which established an Identity  heft Task Force on May 10, 2006 [19]. One year later, the Offce of Management and  udget issued Memorandum M-07-16 [62], which required federal agencies to develop  nd implement breach notifcation policies. As part of this effort, NIST issued Special  D G r a i T B a  2A summary of the history of Statistical Disclosure Limitation can be found in Private Lives and Public  Policies: Confdentiality and Accessibility of Government Statistics [102].  3Other formal methods for privacy include cryptographic algorithms and techniques with provably secure  properties, privacy-preserving data mining, Shamir’s secret sharing, and advanced database techniques. A  summary of such techniques appears in [128].  9  NIST SP 800-188 3pd  November 2022  591  592  593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624  625  Publication (SP) 800-122, Guide to Protecting the Confdentiality of Personally Identifable  Information (PII) [79]. These policies and documents had the specifc goal of limiting the  accessibility of information that could be directly used for identity theft but did not create  a framework for processing government datasets so that they could be released without  impacting the privacy of the data subjects.  In 2015, NIST published NISTIR 8053, De-Identifcation of Personal Information [51],  which provided an overview of de-identifcation issues and terminology. It also sum- marized signifcant publications involving de-identifcation and re-identifcation. How- ever, NISTIR 8053 did not make recommendations regarding the appropriateness of de- identifcation or specifc de-identifcation algorithms. The following year, NIST convened  a Government Data De-Identifcation Stakeholder’s Meeting [52].  De-identifcation is one of several models for allowing the controlled sharing of personal  data and other kinds of sensitive data.4 Other models include the use of data processing en- claves, where computations are performed with confdential data using computers that are  physically isolated from the outside world. That isolation might be performed with locked  doors and guards, or it might be performed using silicon and encryption, as is the case  with enclaves implemented on some modern microprocessors. Another approach is to use  mathematical techniques – such as secure multiparty computation – so that computations  can be carried out on confdential data held by multiple parties without ever bringing all of  the confdential data together in a single location.  Techniques for privacy-preserving data-sharing and analysis can be layered to provide  stronger protection than any single technique would provide in isolation. Such comple- mentary models are discussed in Section 3.4. For a more complete description of data- sharing models, privacy-preserving data publishing, and privacy-preserving data mining,  see NISTIR 8053.  Many of the techniques discussed in this publication (e.g., fully synthetic data and differen- tial privacy) have limited use within the Federal Government due to cost, time constraints,  and the sophistication required of practitioners. However, these techniques are likely to  see increased use as agencies seek to make datasets that include identifying information  available.  2.2. Terminology  While each of the de-identifcation traditions has developed its own terminology and math- ematical models, they share many underlying goals and concepts. Where terminology  differs, this document relies on the terminology developed in previous documents by the  U.S. Government and standards organizations.  4For information on characterizing the sensitivity of information, see NIST SP 800 Volume I, Revision  1 [119].  10  NIST SP 800-188 3pd  November 2022  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640  641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656  657  De-identifcation is a process that is applied to a dataset with the goal of preventing or  limiting informational risks to individuals, protected groups, and establishments while still  allowing for the production of aggregate statistics.5 De-identifcation takes an original  dataset and produces de-identifed data.  Re-identifcation is the general term for any process that restores the association between a  set of de-identifed data and the data subject. Re-identifcation is not the only way that de- identifcation techniques can fail to protect privacy. Improperly de-identifed information  can also be used to infer private facts about individuals that were thought to have been  protected.  Re-identifcation risk is the likelihood that a third party can re-identify data subjects in a  de-identifed dataset. Re-identifcation risk is typically a function of the adverse impacts  that would arise if the re-identifcation were to occur and the likelihood of occurrence.  Re-identifcation risk is a specifc form of privacy risk.  Redaction is the removal of information from a document or dataset for legal or security  purposes. Also known as suppression, redaction is a kind of de-identifying technique that  relies on the removal of information. In general, redaction alone is not suffcient to provide  formal privacy guarantees, such as differential privacy. Redaction may also reduce the data  accuracy of the dataset since the use of selective redaction may result in the introduction of  non-ignorable bias.  Anonymization is a “process that removes the association between the identifying dataset  and the data subject” [66]. This term is reserved for de-identifcaiton processes that cannot  be reversed.  Some authors use the terms de-identifcation and anonymization interchangeably. In some  contexts, the term anonymization is used to describe the destruction of a table that maps  pseudonyms to real identifers.6 Both of these uses are potentially misleading, as many  de-identifcation procedures can be readily reversed if a dataset is discovered that maps a  unique attribute or combination of attributes to identities. For example, a medical dataset  may contain a list of names, medical identifers, the rooms where a patient was seen, the  time that the patient was seen, and the results of a medical test. Such a dataset could  be de-identifed by removing the name and medical identifcation numbers. However, the  dataset of medical test results should not be considered anonymized because the tests can  be re-identifed if the dataset is joined with a second dataset of room numbers, times, and  5ISO/TS 25237:2008 defnes de-identifcation as the “general term for any process of removing the asso- ciation between a set of identifying data and the data subject.” [66]. This document intentionally adopts  a broader defnition for de-identifcation that allows for noise-introducing techniques, such as differential  privacy and the creation of synthetic datasets that are based on privacy-preserving models.  6For example, “Anonymization is a step subsequent to de-identifcation that involves destroying all links  between the de-identifed datasets and the original datasets. The key code that was used to generate the new  identifcation code number from the original is irreversibly destroyed (i.e., destroying the link between the  two code numbers)” [127].  11  NIST SP 800-188 3pd  November 2022  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672  673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688  689  690  691  692  names. Since it is not possible to know whether such an auxiliary dataset exists, this publi- cation recommends avoiding the word anonymization and using the word de-identifcation  instead.  Because of the inconsistencies in the use and defnitions of the word “anonymization,” this  document avoids the term except in this section and in the titles of some references. Instead,  it uses the term “de-identifcation” with the understanding that sometimes de-identifed  information can be re-identifed, and sometimes it cannot.7  Pseudonymization is a “particular type of [de-identifcation]8 that both removes the asso- ciation with a data subject and adds an association between a particular set of character- istics relating to the data subject and one or more pseudonyms” [66]. The term coded  is frequently used in healthcare settings to describe data that has been pseudonymized.  Pseudonymization is commonly used so that multiple observations of an individual over  time can be matched and so that an individual can be re-identifed if there is a policy reason  to do so. Although pseudonymous data are typically re-identifed by consulting a key that  may be highly protected, the existence of the pseudonym identifers frequently increases  the risk of re-identifcation through other means.  Many U.S. Government documents use the phrase personally identifable information (PII)  to describe private information that can be linked to an individual [62, 79], although there  are a variety of defnitions for PII in various laws, regulations, and agency guidance docu- ments. Because of these differing defnitions, it is possible to have information that singles  out individuals but that does not meet a specifc defnition of PII. An added complication  is that some documents use the term PII to denote any information that is attributable to  individuals or information that is uniquely attributable to a specifc individual, while others  use the term strictly for data that are directly identifying.  This document avoids the term personally identifable information. Instead, it uses the  phrases personal data or personal information to denote information related to individu- als and identifying information for “information that can be used to distinguish or trace an  individual’s identity, such as their name, social security number, biometric records, etc.,  alone, or when combined with other personal or identifying information which is linked or  linkable to a specifc individual, such as date and place of birth, mother’s maiden name,  etc.” [62]. Under this defnition, identifying information is personal information, but per- sonal information is not necessarily identifying information.  Non-public personal information is used to describe personal information that is in a dataset  that is not publicly available. Non-public personal information is not necessarily identify- ing.  7Thus, where other references (e.g. [104]) might use the term anonymized fle or anonymized dataset to  describe a dataset that has been de-identifed, this publication will use the terms de-identifed fle and de- identifed dataset since the term de-identifed is descriptive while the term anonymized is aspirational.  8Here, the word anonymization in the ISO 25237 defnition is replaced with the more accurate and descriptive  term de-identifcation.  12  NIST SP 800-188 3pd  November 2022  693  694  695  696  697  698  699  700  701  702  703  704  705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720  721  722  723  724  725  726  727  728  729  730  The defnition of identifying information above suggests that it is easy – or at least possible  – to distinguish personal information from identifying information. Indeed, many tech- niques for de-identifcation require an expert to make this distinction and protect only the  identifying information. However, as understanding of privacy risk develops, it is increas- ingly apparent that all information is potentially identifying information.  This document envisions a de-identifcation process in which an original dataset that con- tains personal information is algorithmically processed to produce de-identifed data. The  result may be a de-identifed dataset, aggregate statistics such as summary tables, or a  synthetic dataset, in which the data are created by a model. This kind of de-identifcation  is envisioned as a batch process. Alternatively, the de-identifcation process may be a  system that accepts queries and returns responses that do not leak more identifying infor- mation than is allowable by policy. De-identifed results may be corrected or updated and  re-released on a periodic basis. The accumulated leakage of information from multiple  releases may be signifcant, even if the leakage from a single release is small. Issues that  arise from multiple releases are discussed in Section 3.4, “Data-Sharing Models.”  Disclosure is generally the exposure of data beyond the original collection use case. How- ever, when the goal of de-identifcation is to protect privacy, disclosure  ...relates to inappropriate attribution of information to a data subject, whether  an individual or an organization. Disclosure occurs when a specifc individual  can be associated with a corresponding record(s) in the released dataset with  high probability (identity disclosure), when an attribute described in a dataset is  held by a specifc individual, even if the record(s) associated with that individ- ual is (are) not identifed (attribute disclosure), or when it is possible to make  an inference about an individual, even if the individual was not in the dataset  prior to de-identifcation (inferential disclosure). [47, emphasis in original]  More information about disclosure can be found in Section 3.2.1, “Probability of Re- Identifcation.”  Disclosure limitation is a general term for the practice of allowing summary information  or queries on data within a dataset to be released without revealing information about spe- cifc individuals whose personal information is contained within the dataset. Thus, de- identifcation is a kind of disclosure limitation technique. Every disclosure limitation pro- cess introduces inaccuracy into the results [14, 11].  A primary goal of disclosure limitation is to protect the privacy of individuals while avoid- ing the introduction of non-ignorable biases [7] (e.g., bias that might lead a social scientist  to come to the wrong conclusion) into the de-identifed dataset. One way to measure the  amount of bias that has been introduced by the de-identifcation process is to compare  statistics or models generated by analyzing the original dataset with those that are gener- ated by analyzing the de-identifed datasets. Such biases introduced by the de-identifcation  13  NIST SP 800-188 3pd  November 2022  731  732  733  734  735  736  737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752  753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768  769  770  process are typically unrelated to any statistical biases that may also exist in the original  data.  Formal models of privacy can quantify the amount of privacy protection offered by a de- identifcation process. With methods based on differential privacy, this measurement takes  the form of a number called privacy loss, which quantifes the additional risk that an ad- versary might learn something new about an individual as a result of a de-identifed data  release. When a de-identifcation process is associated with low privacy loss, releasing the  data it produces results in little additional risk for individuals in the input dataset. Some  formal models, such as differential privacy, allow composing the privacy losses of multiple  data releases to quantify the total risk to individuals of the combined releases, while others  – such as k-anonymity – do not have this capability.  An upper bound on the total acceptable privacy loss of many data releases is often called  a privacy loss budget or simply a privacy budget. This number quantifes the total privacy  risk to an individual who participates in all of the releases.  Differential privacy [40] is a model based on a mathematical defnition of privacy that con- siders the risk to an individual from the release of a query on a dataset containing their  personal information. Statisticians, mathematicians, and other kinds of privacy engineers  then develop mathematical algorithms, called mechanisms, that process data in a way that  is consistent with the defnition. Differential privacy limits both identity and attribute dis- closure by adding non-deterministic noise (random values) to the results of mathematical  operations before the results are reported. Unlike k-anonymity and other de-identifcation  frameworks, differential privacy is based on information theory and makes no distinction  between what is private data and what is not. Differential privacy does not require that val- ues be classifed as direct identifers, quasi-identifers, and non-identifying values. Instead,  differential privacy assumes that all values in a record might be identifying and therefore  all must be de-identifed.  Differential privacy’s mathematical defnition requires that the result of an analysis of a  dataset should be roughly the same with or without the data of any single individual. The  defnition is usually satisfed by adding random noise to the result of a query, ensuring  that the added noise masks the contribution of any individual. The degree of sameness  is defned by the parameter ε (epsilon). The smaller the parameter ε , the more noise is  added, and the more diffcult it is to distinguish the contribution of a single individual. The  result is increased privacy for all individuals – both those in the sample and those in the  population from which the sample is drawn who are not present in the dataset. The research  literature describes differential privacy being used to solve a variety of tasks, including  statistical analysis, machine learning, and data sanitization [38]. Differential privacy can  be implemented in an online query system or in a batch mode in which an entire dataset  is de-identifed at one time. In common usage, the phrase “differential privacy” is used  to describe both the formal mathematical framework for evaluating privacy loss and for  algorithms that provably provide those privacy guarantees.  14  NIST SP 800-188 3pd  November 2022  771  772  773  774  775  776  777  778  779  780  781  782  783  784  785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800  801  802  803  804  805  806  807  808  The use of differential privacy algorithms does not guarantee that privacy will be preserved.  Instead, the algorithms guarantee that the amount of privacy risk introduced by data pro- cessing or data release will reside within specifc mathematical bounds. It is also important  to remember that the impact on privacy risk is limited to reducing the risk of identity and  attribute disclosures (see §3.2.1, “Probability of Re-Identifcation”) and not inferential dis- closure.  K-anonymity [108, 123] is a framework for quantifying the amount of manipulation re- quired of the quasi-identifers to achieve a desired level of privacy. The technique is based  on the concept of an equivalence class – the set of records that have the same values on the  quasi-identifers9. A dataset is said to be k-anonymous if there are no fewer than k match- ing records for every specifc combination of quasi-identifers. For example, if a dataset  that has the quasi-identifers (birth year) and (state) has k=4 anonymity, then there must  be at least four records for every combination of (birth year, state). Subsequent work has  refned k-anonymity by adding requirements for diversity of the sensitive attributes within  each equivalence class (known as l-diversity [76]) and requiring that the resulting data be  statistically close to the original data (known as t-closeness [73]).  K-anonymity and its subsequent refnements defne formal privacy models but come with  two important drawbacks. First, they require an expert to determine the set of quasi- identifers by distinguishing between identifying and non-identifying information. As de- scribed earlier, this task can be diffcult or impossible in some contexts. If identifying  information is not marked as a quasi-identifer, then the resulting k-anonymous dataset will  not prevent the re-identifcation of data subjects. Second, k-anonymity and related tech- niques are not compositional – they do not quantify the cumulative privacy loss of multiple  data releases, and multiple releases can result in a catastrophic loss of privacy.  When data releases containing information about the same individual accumulate, then  privacy loss accumulates. This accumulation of privacy loss is not refected in k-anonymity,  nor is it refected in HIPAA privacy rule guidance [136]. Nevertheless, the accumulation  is real. In 2003, Dinur and Nissim discovered that it was possible to reconstruct private  microdata from a query interface even if the results of each query were systematically  infused with small amount of noise [33]. The researchers showed that the amount of  noise added to prevent an accurate reconstruction increases as the amount of queries on the  dataset increase. If a query interface allows for an ulimited number of queries, no amount  of noise is suffcient. Organizations should keep this in mind and try to assess the overall  accumulated risk. The discovery in this paper led directly to the invention of differential  privacy.  Some agencies (notably those that publish data for accountability and enforcement pur- poses) view perturbative Statistical Disclosure Limitation methods (e.g., those that add  noise, such as differential privacy) as being inherently unacceptable, since the noise intro-  9A quasi-identifer is a variable that can be used to identify an individual through association with other  information.  15  NIST SP 800-188 3pd  November 2022  809  810  811  812  813  814  815  816  817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832  833  duced by the methods can void their ability to be used for accountability. For example, if  school would lose funding if the promotion rate for any class fell below a certain thresh- ld, then a method that protects the privacy of students within each class by introducing  oise could mask whether the school did or did not make that target. Thus, despite their  eaknesses and faws, program agencies often prefer to use suppression as the preferred  rotection method for these purposes because the data are either reported as is or sup- ressed, eliminating the uncertainty. Agencies should realize that suppression alone is not  uffcient to protect privacy, and if a large enough number of queries is released based on  he same confdential dataset, it is frequently possible to reconstruct even data that have  een suppressed.  raditional disclosure limitation and k-anonymity start with specifc disclosure limitation  echanisms that were designed to hide information while allowing for useful data analysis  nd attempting to reach the goal of privacy protection. In contrast, differential privacy starts  ith an information-theoretic defnition of privacy and has attempted to evolve mechanisms  hat produce useful (but privacy-preserving) results. These techniques are currently the  ubject of academic research, so it is reasonable to expect new techniques to be developed  n the coming years that simultaneously increase privacy protection while providing for the  igh accuracy of resulting de-identifed data. Indeed, some authors have shown that the  odels can be viewed synergistically [114] under some circumstances.  inally, privacy harms are not the only kinds of harms that can result from the release of  e-identifed data. Analysts working with de-identifed data often have no way of knowing  ow inaccurate their statistical results are due to statistical distortions introduced by the  e-identifcation process. Thus, de-identifcation operations intended to shield individuals  rom harm could result in inaccurate research fndings. Such research might also cause  arm if it is used to support harmful policies.  a o n w p p s t b  T m a w t s i h m  F d h d f h  16  NIST SP 800-188 3pd  November 2022  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848  849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864  865  866  867  868  869  870  3. Governance and Management of Data De-Identifcation  The decisions and practices regarding the de-identifcation and release of government data  can be integral to the mission and proper functioning of a government agency. As such,  these activities should be managed by an agency’s leadership in a way that assures that  performance and results that are consistent with the agency’s mission and legal authority.  As discussed above, the need for attention arises because of the conficting goals of data  transparency and privacy protection. Although many agencies once assumed that it was  relatively straightforward to remove privacy-sensitive data from a dataset so that the re- mainder could be released without restriction, history shows that this is not the case [51,  §2.4, §3.6].  Given this history, there may be a tendency for government agencies to either over-protect  data or to simply avoid its release. Limiting the release of data clearly limits the privacy risk  that might result from a data release. However, limiting the release of data also creates costs  and risks for other government agencies (which will then not have access to the identifed  data), external organizations, and society. For example, absent the data release, external  organizations will suffer the cost of recollecting the data (if it is possible to do so) or the  risk of incorrect decisions that might result from having insuffcient information.  This section begins with a discussion of why agencies might wish to de-identify data and  how agencies should balance the benefts of data release with risks to the data subjects. It  then discusses where de-identifcation fts within the data life cycle. Finally, it discusses  options that agencies have for adopting de-identifcation standards.  3.1. Identifying Goals and Intended Uses of De-Identifcation  Before engaging in de-identifcation, agencies should clearly articulate their goals regard- ing transparency and disclosure limitation in making a data release. They should then  develop a written plan that explains how de-identifcation will be used to accomplish those  goals.  For example:  • Federal Statistical Agencies collect, process, and publish data for use by researchers,  business planners, and other well-established purposes. These agencies are likely to  have established standards and methodologies for de-identifcation. As these agen- cies evaluate new approaches for de-identifcation, they should document their ra- tionale for adopting legacy versus new approaches, evaluate how successful their  approaches have been over time, and address inconsistencies between data releases.  • Federal Awarding Agencies are allowed under OMB Circular A-110 to require that  institutions of higher education, hospitals, and other non-proft organizations that  receive federal grants provide the U.S. Government with “the right to (1) obtain,  reproduce, publish or otherwise use the data frst produced under an award; and  17  NIST SP 800-188 3pd  November 2022  871  872  873  874  875  876  877  878  879  880  881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896  897  898  899  900  901  902  903  904  905  906  907  908  (2) authorize others to receive, reproduce, publish, or otherwise use such data for  Federal Purposes” [91, see §36 (c) (1) and (2)]. To realize this policy, awarding  agencies can require that awardees establish data management plans for making re- search data publicly available. Such data are used for a variety of purposes, including  transparency and reproducibility. In general, research data that contain personal in- formation should be de-identifed by the awardee prior to public release. Awarding  agencies may establish de-identifcation standards to ensure the protection of per- sonal information and may consider audits to assure that awardees have performed  de-identifcation in an appropriate manner.  • Federal Research Agencies may wish to make de-identifed data available to the  public to further the objectives of research transparency and allow others to reproduce  and build upon their results. These agencies are generally prohibited from publishing  research data that contain personal information, requiring the use of de-identifcation.  • All Federal Agencies that wish to make administrative or operational data available  for transparency, accountability, or program oversight or to enable academic research  may wish to employ de-identifcation to avoid sharing sensitive personally identif- able information of employees, customers, or others. These agencies may wish to  evaluate the effectiveness of simple feld suppression, de-identifcation that involves  aggregation, and the creation and release of synthetic data as alternatives for realizing  their commitment to open data.  3.2. Evaluating Risks that Arise from De-Identifed Data Releases  Once the purpose of the data release is understood, agencies should identify the risks that  might result from the data release. As part of this risk analysis, agencies should specifcally  evaluate the anticipated negative actions that might result from re-identifcation, as well as  strategies for remediation. NIST provides detailed information on how to conduct risk  assessments in NIST SP 800-30 [23].  Risk assessments should be based on objective scientifc factors and consider the best inter- ests of the individuals in the dataset, the responsibilities of the agency holding the data, and  the anticipated benefts to society. The goal of a risk evaluation is not to eliminate risk but  to identify which risks can be reduced while still meeting the objectives of the data release  and then deciding whether the residual risk is justifed by the goals of the data release. An  agency decision-making process may choose to accept or reject the risk that might result  from a release of de-identifed data, but participants in the risk assessment should not be  empowered to prevent risk from being documented and discussed. Centralized processes  also allow for standardization of the risk assessment and the amount of “acceptable risk”  across different programs’ releases.  It is diffcult to measure re-identifcation risk in ways that are both general and meaningful.  For example, it is possible to measure the similarity between individuals in the dataset  18  NIST SP 800-188 3pd  November 2022  909  910  911  912  913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928  929  930  931  932  933  934  935  936  937  938  939  940  941  under a variety of different parameters and to model how that similarity is impacted when  the larger population is considered. However, such calculations may result in different  levels of risk for different groups. There may be some individuals in a dataset who would  be signifcantly adversely impacted by re-identifcation and for whom the likelihood of  re-identifcation might be quite high, but these individuals might represent a tiny fraction  of the entire dataset. This represents an important area for research in the feld of risk  communication.  3.2.1. Probability of Re-Identifcation  As discussed in Section 2.2, “Terminology,” the potential impacts on individuals from the  release and use of de-identifed data include [143]:  Identity disclosures: Associating a specifc individual with the corresponding record(s)  in the dataset with high probability. Identity disclosure can result from insuffcient  de-identifcation, re-identifcation by linking, or pseudonym reversal.  Attribute disclosure: Determining that an attribute described in the dataset is held by a  specifc individual with high probability, even if the records associated with that indi- vidual are not identifed. Attribute disclosure can occur without identity disclosure if  the de-identifed dataset contains data from a signifcant number of relatively homo- geneous individuals [51, p.13]. In these cases, traditional de-identifcation does not  protect against attribute disclosure, although differential privacy can. Membership  inference is an example of attribute disclosure.  Inferential disclosure: Being able to make an inference about an individual (typically a  member of a group) with high probability, even if the individual was not in the dataset  prior to de-identifcation. “Inferential disclosure is of less concern in most cases  as inferences are designed to predict aggregate behavior, not individual attributes,  and thus are often poor predictors of individual data values” [60]. Traditional de- identifcation does not protect against inferential disclosure. Such disclosures can  never be eliminated; they can only be controlled.  Re-identifcation probability10 is the estimated probability that an outside party will be able  to use information contained in a de-identifed dataset to make identity-related inferences  about individuals. This outside party was originally termed a data intruder, although the  terms adversary and attacker are also used, borrowing from the colorful language of infor- mation security. Different kinds of re-identifcation probabilities for this data intruder can  be calculated.  10Previous publications described identifcation probability as “re-identifcation risk” and used scenarios such  as a journalist seeking to discredit a national statistics agency or a prosecutor seeking to fnd information  about a suspect as the bases for probability calculations. That terminology is not presented in this document  because of the possible unwanted connotations of those terms and in the interest of bringing the terminology  of de-identifcation into agreement with the terminology used in contemporary risk analysis processes [42].  19  NIST SP 800-188 3pd  November 2022  942  943  944  945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960  961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  Here are several kinds of probabilities, as well as proposals for new, declarative, self- describing names:  Known inclusion re-identifcation probability (KIRP) is the probability of fnding the  record that matches a specifc individual known to be in the sample corresponding to  a specifc record. KIRP can be expressed as the probability for a specifc individual  or the probability averaged over the entire dataset (AKIRP).11  Unknown inclusion re-identifcation probability (UIRP) is the probability of fnding the  record that matches a specifc individual without frst knowing whether the individual  is in the dataset. UIRP can be expressed as a probability for an individual record in  the dataset averaged over the entire population (AUIRP).12  Record matching probability (RMP) is the probability of fnding the record that matches  a specifc individual chosen from the population. RMP can be expressed as the prob- ability for a specifc record (RMP), the probability averaged over the entire dataset  (ARMP), or the maximum probability over the entire dataset.  Inclusion probability (IP) is the probability that a specifc individual’s presence in the  dataset can be inferred.  Whether it is necessary to quantitatively estimate these probabilities depends on the specifcs  of each intended data release. For example, many cities publicly disclose whether taxes  have been paid on a property. Given that this information is already a matter of public  record, it may not be necessary to consider inclusion probability when a dataset of property  taxpayers for a specifc dataset is released. Likewise, there may be some attributes in a  dataset that are already public and may not need to be protected with disclosure limitation  techniques. However, the existence of such attributes may pose a re-identifcation risk for  other information in the dataset or in other de-identifed datasets. The fact that information  is public may not negate the responsibility of an agency to provide protection for that in- formation, as the aggregation and distribution of information may cause privacy risk that  was not otherwise present. Agencies may also be legally prohibited from releasing copies  of information that is similar to information that is already in the public domain.  Although disclosures are commonly thought to be discrete events involving the release of  specifc data, such as an individual’s name matched to a record, disclosures can result from  the release of data that merely changes a data intruder’s probabilistic belief. For example,  a disclosure might change an intruder’s estimate that a specifc individual is present in a  dataset from a 50% probability to 90%. The intruder still does not know if the individual  is in the dataset or not (and the individual might not, in fact, be in the dataset), but a  11Some texts refer to KIRP as “prosecutor risk.” The scenario is that a prosecutor is looking for records that  belong to a specifc, named individual.  12Some texts refer to UIRP as “journalist risk.” The scenario is that a journalist has obtained a de-identifed  fle and is trying to identify one of the data subjects, but the journalist fundamentally does not care who is  identifed.  20  NIST SP 800-188 3pd  November 2022  976  977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992  993  994  995  996  997  998  999  1000  1001  1002  1003  1004  1005  1006  1007  1008  1009  1010  1011  1012  1013  1014  probabilistic disclosure has still occurred because the intruder’s estimate of the individual  has been changed by the data release.  It may be diffcult to estimate specifc re-identifcation probabilities, as the ability to re- identify depends on the original dataset, the de-identifcation technique, the technical skill  of the data intruder, the intruder’s available resources, and the availability of additional  data (publicly available or privately held) that can be linked with the de-identifed data.  It is likely that the true probability of re-identifcation increases over time as techniques  improve and more contextual information becomes available to potential data intruders.  Indeed, some researchers have claimed that computing these probabilities “is a fundamen- tally meaningless exercise” because the calculations are based on assumptions that cannot  be validated (e.g., the lack of a database that could link specifc quasi-identifers or sensi- tive, non-identifying values to identities) [83].  De-identifcation practitioners have traditionally quantifed re-identifcation probability, in  part, based on the skills and abilities of a potential data intruder. Datasets that were thought  to have little possibility for exploitation were deemed to have a lower re-identifcation  probability than datasets containing sensitive or otherwise valuable information. Such ap- proaches are not appropriate when attempting to evaluate the re-identifcation probability  of government datasets that will be publicly released.  • Although a specifc de-identifed dataset may not be recognized as sensitive, re- identifying that dataset may be an important step in re-identifying another dataset  that is sensitive. Alternatively, the data intruder may merely wish to embarrass the  government agency. Thus, adversaries may have a strong incentive to re-identify  datasets that are seemingly innocuous.  • Although the public may not generally be skilled in re-identifcation, many resources  on the internet make it easy to acquire specialized datasets, tools, and experts for  specifc re-identifcation challenges. Family members, friends, colleagues, and oth- ers may also possess substantial personal knowledge about individuals in the data  that can be used for re-identifcation.  Instead, de-identifcation practitioners should assume that de-identifed government datasets  could be subjected to sustained, worldwide re-identifcation attempts, and they should  gauge their de-identifcation requirements accordingly. Of course, it is unrealistic to as- sume that all of the world’s resources will be used to attempt to re-identify every publicly  released fle. Therefore, de-identifcation requirements should be gauged using a risk as- sessment [75]. More information on conducting risk assessments can be found in NIST SP  800-30, Guide for Conducting Risk Assessments [23].  Members of vulnerable populations (e.g., prisoners, children, people with disabilities) may  be more susceptible to having their identities disclosed by de-identifed data than non- vulnerable populations because the thing that makes these individuals vulnerable may also  make them stand out in the dataset. Likewise, residents of areas with small populations  21  NIST SP 800-188 3pd  November 2022  1015  1016  1017  1018  1019  1020  1021  1022  1023  1024  1025  1026  1027  1028  1029  1030  1031  1032  1033  1034  1035  1036  1037  1038  1039  1040  1041  1042  1043  1044  1045  1046  1047  may be more susceptible to having their identities disclosed than residents of urban areas.  Individuals with multiple traits will generally be more identifable if the individual’s loca- tion is geographically restricted. For example, data belonging to a person who is labeled as  a pregnant, unemployed female veteran will be more identifable if restricted to Baltimore  County, Maryland, than to all of North America.  If agencies determine that the potential for harm is large in a contemplated data release, one  way to manage the risk is by increasing the level of de-identifcation and accepting a lower  data accuracy level. Other options include data controls, such as restricting the availability  of data to qualifed researchers in a data enclave.  3.2.2. Adverse Impacts of Re-Identifcation  As part of a risk analysis, agencies should attempt to enumerate specifc kinds of adverse  impacts that can result from the re-identifcation of de-identifed information. These can  include potential impacts on individuals, the agency, and society.  Potential adverse impacts on individuals include:  • Increased availability of personal information that leads to an increased risk of fraud,  identity theft, discrimination, or abuse  • Increased availability of an individual’s location that puts that person at risk for bur- glary, property crime, assault, or other kinds of violence  • Increased availability of an individual’s non-public personal information that causes  psychological harm by exposing potentially embarrassing information or information  that the individual may not otherwise choose to reveal to the public or to family  members and that potential affects opportunities in the economic marketplace (e.g.,  employment, housing, college admission)  Potential adverse impacts on agencies include:  • Mandatory reporting under breach reporting laws, regulations, or policies  • Embarrassment or reputational damage  • Harm to agency operations if some aspect of those operations required that the de- identifed data remain confdential (e.g., an agency that is forced to discontinue a  scientifc experiment because the data release may have biased the study participants)  • Financial impacts that result from the harm to the individuals (e.g., lawsuits)  • Civil or criminal sanctions against employees or contractors that result from a data  release contrary to U.S. law  Potential adverse impacts on society include:  22  NIST SP 800-188 3pd  November 2022  1048  1049  1050  1051  1052  1053  1054  1055  1056  1057  1058  1059  1060  1061  1062  1063  1064  1065  1066  1067  1068  1069  1070  1071  1072  1073  1074  1075  1076  1077  1078  • Undermining the reputation of researchers in general and the willingness of the pub- lic to support/tolerate research and provide accurate information to government agen- cies and researchers  • Engendering a lack of trust in government – individuals may stop consenting to the  use of their data, may stop providing data, or may provide false data  • Damaging the practice of using de-identifed information – de-identifcation is an  important tool for promoting research and accountability, and poorly executed de- identifcation efforts may negatively impact the public’s view of this technique and  limit its use  One way to calculate an upper bound on impact to an individual or the agency is to es- timate the impact that would result from the inadvertent release of the original dataset.  This approach will not calculate the upper bound on the societal impact, however, since  that impact includes reputational damage to the practice of de-identifcation itself. That is,  every time data are compromised because of a poorly executed de-identifcation effort, it  becomes harder to justify the use of de-identifcation in future data releases.  As part of a risk analysis process, organizations should enumerate specifc measures that  they will take to minimize the risk of successful re-identifcation. Organizations may wish  to consider both the actual risk and the perceived risk to those in the dataset and in the  broader community.  As part of the risk assessment, an organization may determine that there is no way to  achieve the de-identifcation goal in terms of data accuracy and identifability. In these  cases, the organization will need to decide whether it should adopt additional measures to  protect privacy (e.g., administrative controls or data use agreements), accept a higher level  of risk, or choose not to proceed with the project.  3.2.3. Impacts Other Than Re-Identifcation  The use of de-identifed data can lead to adverse impacts other than those that might re- sult from re-identifcation. Risk assessments that evaluate the risks of re-identifcation can  address these other risks as well. Such risks might include:  • The risk of excessive inferential disclosures  • The risk that the de-identifcation process might introduce bias or inaccuracies into  the dataset that result in incorrect decisions13  13For example, a personalized warfarin dosing model created with data that had been modifed in a manner  consistent with the differential privacy de-identifcation model produced higher mortality rates in simulation  than a model created from unaltered data [49]. Educational data de-identifed with the k-anonymity model  can also result in the introduction of bias that leads to spurious results [14, 125].  23  NIST SP 800-188 3pd  November 2022  1079  1080  1081  1082  1083  1084  1085  1086  1087  1088  1089  1090  1091  1092  1093  1094  1095  1096  1097  1098  1099  1100  1101  1102  1103  1104  1105  1106  1107  1108  1109  1110  1111  1112  1113  • The risk that releasing a de-identifed dataset might reveal non-public information  about an agency’s policies or practices  It is preferable to use de-identifcation processes that include assessments of accuracy (e.g.,  confdence intervals) with respect to the bias and precision of statistical properties of the  data. Where it does not provide information that may aid data intruders, it is also useful to  reveal the de-identifcation process itself so that analysts can understand any potential in- accuracies that might be introduced by the de-identifcation. This is consistent with Kerck- hoffs’ principle [67], a widely accepted system design principle that holds that the security  of a system should not rely on the secrecy of the methods that it employs.  3.2.4. Remediation  As part of a risk analysis process, agencies should attempt to enumerate techniques that  could be used to mitigate or remediate harm that would result from a successful re-identifcation  of de-identifed information. Remediation could include victim education, the procurement  of monitoring or security services, the issuance of new identifers, or other measures.  3.3. Data Life Cycle  The NIST Big Data Interoperability Framework defnes the data life cycle as “the set of  processes in an application that transform raw data into actionable knowledge” [85]. The  data life cycle can be used in the de-identifcation process to help analyze the expected  benefts, intended uses, privacy threats, and vulnerabilities of de-identifed data. As such,  the data life cycle concept can be used to select appropriate privacy controls based on a  reasoned analysis of the threats. For example, privacy-by-design concepts [22] can be  employed to decrease the number of identifers collected, minimizing requirements for de- identifcation prior to data release. The data life cycle can also be used to design a tiered  access mechanism based on this analysis [12].  Several data life cycles have been proposed, but none are widely accepted as a standard.  Michener et al. [80] (Figure 1) describe the data life cycle as a true cycle:  → Assure → Describe → Deposit → Preserve → Discover → Integrate → Analyze →  Collect  Stobierski [120] also describes the data life cycle as a cycle with different steps:  Generation → Collection → Processing → Storage → Management → Analysis →  Visualization → Interpretation → Generation  De-identifcation does not ft into a circular data life cycle model, as the data owner typ- ically retains access to the identifed data. However, if the organization employs de- identifcation, it could be performed during Collect or between Collect and Assure if iden- tifed data were collected but the identifying information was not actually needed. Alter-  24  NIST SP 800-188 3pd  November 2022  NIST SP 800-188  DE-IDENTIFYING GOVERNMENT DATASETS  30  Figure 1 Michener et al.’s view of the data life cycle is a true cycle, with analysis guiding future collection.  It is unclear how de-identification fits into a circular life cycle model, as the data owner typically  retains access to the identified data. However, if the organization employs de-identification, it  could be performed during the Collect, or between Collect and Assure if identified data were  collected but the identifying information was not actually needed. Alternatively, de-identification  could be applied after Describe and prior to Deposit, to avoid archiving identifying information.  Chisholm and others describe the data life cycle as a linear process that involves Data Capture →  Data Maintenance → Data Synthesis → Data Usage → {Data Publication & Data Archival} →  Data Purging:99  Figure 2 Chisholm's view of the data life cycle is a linear process with a branching point after data usage.  99 Malcolm Chisholm, 7 Phases of a Data Life Cycle, Information Management, July 9, 2015. http://www.information- management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html  Fig. 1. The data life cycle as described by Michener et al. [80]  NIST SP 800-188  DE-IDENTIFYING GOVERNMENT DATASETS  30  Figure 1 Michener et al.’s view of the data life cycle is a true cycle, with analysis guiding future collection.  It is unclear how de-identification fits into a circular life cycle model, as the data owner typically  retains access to the identified data. However, if the organization employs de-identification, it  could be performed during the Collect, or between Collect and Assure if identified data were  collected but the identifying information was not actually needed. Alternatively, de-identification  could be applied after Describe and prior to Deposit, to avoid archiving identifying information.  Chisholm and others describe the data life cycle as a linear process that involves Data Capture →  Data Maintenance → Data Synthesis → Data Usage → {Data Publication & Data Archival} →  Data Purging:99  Figure 2 Chisholm's view of the data life cycle is a linear process with a branching point after data usage.  99 Malcolm Chisholm, 7 Phases of a Data Life Cycle, Information Management, July 9, 2015. http://www.information- management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html  Fig. 2. Chisholm’s view of the data life cycle is a linear process with a branching point after  data usage [25]  1114 natively, de-identifcation could be applied after Describe and prior to Deposit to avoid  1115 archiving identifying information.  1116 Chisholm and others [25] (Figure 2) describe the data life cycle as a linear process with a  1117 fork for data publication:  1118 Data Capture → Data Maintenance → Data Synthesis → Data Usage →  1119 {Data Publication & Data Archival → Data Purging}  1120 Using this formulation, de-identifcation can take place either during Data Capture or fol- 1121 lowing Data Usage. However, agencies should consider data release requirements from  1122 the very beginning of the planning process for each new data collection. By knowing in  1123 advance how they intend to publish and for what purposes and by having a plan for how  1124 disclosure limitation will be applied, agencies can tailor information collection accordingly.  1125 For example, if specifc identifers are not needed for maintenance, synthesis, and usage,  1126 then those identifers should not be collected. If fully identifed data are needed within the  25  NIST SP 800-188 3pd  November 2022  NIST SP 800-188  DE-IDENTIFYING GOVERNMENT DATASETS  32  Figure 3 Life cycle model for government data releases, from Altman et al. Fig. 3. Altman’s “modern approach to privacy-aware government data releases” [75]  1127  1128  1129  1130  1131  1132  1133  1134  1135  1136  organization, the identifying information can be removed prior to the data being published,  shared, or archived. Applying de-identifcation throughout the data life cycle minimizes  privacy risk and signifcantly eases the process of public release. However, agencies should  be cognizant of the potential loss of future utility if identifers are permanently removed.  For this reason, agencies may wish to retain an identifed dataset or data linking informa- tion, as it may be diffcult to predict future needs.  Altman et al. [75] (Figures 3 and 4) propose a “modern approach to privacy-aware gov- ernment data releases” that incorporates progressive levels of de-identifcation as well as  different kinds of access and administrative controls in line with the sensitivity of the data.  Agencies that perform de-identifcation should document that:  26  NIST SP 800-188 3pd  November 2022  NIST SP 800-188  DE-IDENTIFYING GOVERNMENT DATASETS  33  Figure 4 Conceptual diagram of the relationship between post-transformation identifiability, level of expected  harm, and suitability of selected privacy controls for a data release. From Altman et al.  Agencies performing de-identification should document that:  • Techniques used to perform the de-identification are theoretically sound and generally  accepted;100  • Software used to perform the de-identification is reliable for the intended task;  100 Specifically, agencies may wish to mirror the language of the HIPAA Privacy Rule’s expert determination method, which  states: “The second way to de-identify PHI is to have a qualified statistician determine, using(1) A person with appropriate  knowledge of and experience with generally accepted statistical and scientific principles and methods, for rendering  information not individually identifiable: (i) Applying such principles and methods, determines that the risk is very small  that the information could be used, alone or in combination with other reasonably available information, by thean  anticipated recipient to identify thean individual who is a subject of the information. The qualified statistician must  document; and  (ii) Documents the methods and results of the analysis that justify such a determination.” ;  See  https://www.hhs.gov/hipaa/for- professionals/privacy/special-topics/de-identification/index.html#guidancedetermination.  Fig. 4. Altman’s conceptual diagram of the relationship between post-transformation  identifability, level of expected harm, and suitability of selected privacy controls for a data  release [75]  27  NIST SP 800-188 3pd  November 2022  1137  1138  1139  1140  1141  1142  1143  1144  1145  1146  1147  1148  1149  1150  1151  1152  1153  1154  1155  1156  1157  • The techniques used to perform the de-identifcation are theoretically sound and gen- erally accepted.14  • The software used to perform the de-identifcation is reliable for the intended task.  • The individuals who performed the de-identifcation were suitably qualifed.  • The tests that were used to evaluate the effectiveness of the de-identifcation were  validated for that purpose.  • Ongoing monitoring is in place to ensure the continued effectiveness of the de- identifcation strategy.  No matter where de-identifcation is applied in the data life cycle, agencies should docu- ment the answers to the following questions for each de-identifed dataset:  • Are direct identifers collected with the dataset?  • Even if direct identifers are not collected, is it still possible to identify the data  subjects through the presence of quasi-identifers?  • Where in the data life cycle is de-identifcation performed? Is it performed in only  one place or in multiple places?  • Is the original dataset retained after de-identifcation?  • Is there a key or map retained so that specifc data elements can be re-identifed later?  • How are decisions made regarding de-identifcation and re-identifcation?  • Are there specifc datasets that can be used to re-identify the de-identifed data? If so,  what controls are in place to prevent intentional or unintentional re-identifcation?  • Is it a problem if some records in a dataset are re-identifed?  14To determine that a technique is theoretically sound and generally accepted, agencies that wish to adopt  guidance that mirrors the language that the HHS November 26, 2012 Guidance Regarding Methods for  De-identifcation of Protected Health Information in Accordance with the Health Insurance Portability  and Accountability Act (HIPAA) Privacy Rule [136]. uses in its discvussion of the Privacy Rule’s “expert  determination method,” which states on page 7:  “A covered entity may determine that health information is not individually identifable health information  only if:  (1) A person with appropriate knowledge of and experience with generally accepted statistical and sci- entifc principles and methods for rendering information not individually identifable:  (i) Applying such principles and methods, determines that the risk is very small that the informa- tion could be used, alone or in combination with other reasonably available information, by an  anticipated recipient to identify an individual who is a subject of the information; and  (ii) Documents the methods and results of the analysis that justify such determination;”  28  NIST SP 800-188 3pd  November 2022  1158  1159  1160  1161  1162  1163  1164  1165  1166  1167  1168  1169  1170  1171  1172  1173  1174  1175  1176  1177  1178  1179  1180  1181  1182  1183  1184  1185  1186  1187  1188  1189  1190  1191  1192  1193  1194  1195  • Is there a mechanism that will inform the de-identifying agency if there is an attempt  to re-identify the de-identifed dataset? Is there a mechanism that will inform the  agency if the attempt is successful?  3.4. Data-Sharing Models  Agencies should decide on the data-sharing model that will be used to make the data avail- able outside of the agency after the data have been de-identifed [51, p.14]. Specifc models  combine security and privacy techniques to reduce privacy risks to individuals. Security  refers to techniques that limit who can view the data. Encryption is an example of a secu- rity technique – it allows only the party holding the encryption key to view the data. Pri- vacy refers to techniques that limit what information the data contains. The two concepts  can be considered orthogonally. In practice, however, who has access to the data makes  a signifcant difference in the expected risk of disclosure and therefore infuences the ex- tent to which privacy techniques must be used to limit the presence of sensitive personally  identifable information in the data.  A number of possible models exist at different points in the spectrum of security and pri- vacy protections. Figure 4 summarizes this spectrum: its x-axis describes various privacy  techniques that can limit the informational content of the data; its y-axis describes how  much harm would occur if the underlying information were disclosed; and the regions of  the graph are labeled with suggested security techniques. Some common combinations of  security and privacy techniques include:  The Release and Forget Model [94]. The de-identifed data may be released to the pub- lic, typically by being published on the internet. It can be diffcult or impossible for  an organization to recall the data once released in this fashion and may limit infor- mation for future releases.  The Data Use Agreement (DUA) Model. The de-identifed data may be made available  under a legally binding data use agreement that details what can and cannot be done  with the data. Typically, data use agreements may prohibit attempted re-identifcation,  linking to other data, and redistribution of the data without a similarly binding DUA.  A DUA will typically be negotiated between the data holder and qualifed researchers  (the “qualifed investigator model” [44]) or members of the general public (e.g., cit- izen scientists or the media), although they may be simply posted on the internet  with a click-through license agreement that must be agreed to before the data can be  downloaded (the “click-through model” [44]).  The Synthetic Data with Verifcation Model. Statistical disclosure limitation techniques  are applied to the original dataset and used to create a synthetic dataset that contains  many of the aspects of the original dataset but does not contain disclosing infor- mation. The synthetic dataset is released, either publicly or to vetted researchers.  The synthetic dataset can then be used as a proxy for the original dataset, and if  29  NIST SP 800-188 3pd  November 2022  1196  1197  1198  1199  1200  1201  1202  1203  1204  1205  1206  1207  1208  1209  1210  1211  1212  1213  1214  1215  1216  1217  1218  1219  1220  1221  1222  1223  1224  1225  1226  1227  1228  1229  1230  1231  1232  1233  constructed well, the results of statistical analyses should be similar. If used in con- junction with an enclave model as below, researchers may use the synthetic dataset  to develop queries and/or analytic software. These queries and/or software can then  be taken to the enclave or provided to the agency and be applied on the original data.  The Enclave Model [44, 87, 113]. The de-identifed data may be kept in a segregated en- clave that restricts the export of the original data and instead accepts queries from  qualifed researchers, runs the queries on the de-identifed data, and responds with  results. Enclaves can be physical or virtual and can operate under a variety of dif- ferent models. For example, vetted researchers may travel to the enclave to perform  their research, as is done with the Federal Statistical Research Data Centers operated  by the U.S. Census Bureau. Enclaves may be used to implement the verifcation step  of the Synthetic Data with Verifcation Model. Queries made in the enclave model  may be vetted automatically or manually (e.g., by the DRB). Vetting can try to screen  for queries that might violate privacy or are inconsistent with the stated purpose of  the research.  Sharing models should consider the possibility of multiple or periodic releases. Just as  repeated queries to the same dataset may leak personal data from the dataset, repeated de- identifed releases (whether from the same dataset or from different datasets containing  some of the same individuals) by an agency may result in compromising the privacy of in- dividuals unless each subsequent release is viewed in light of the previous release. Even if  a contemplated release of a de-identifed dataset does not directly reveal identifying infor- mation, federal agencies should ensure that the release – combined with previous releases  – will also not reveal identifying information [137].  Instead of sharing an entire dataset, the data owner may choose to release a sample. If only  a sample is released, the probability of re-identifcation decreases because a data intruder  will not know if a specifc individual from the data universe is present in the de-identifed  dataset [43]. However, releasing only a sample may decrease the statistical power of tests  on the data, may cause users to draw incorrect inferences if proper statistical sampling  methods are not used, and may not align with agency goals regarding transparency and  accountability.  3.5. The Five Safes  Agencies that make data available to outsiders should use a repeatable methodology for  evaluating the terms under which that data will be made available. The Five Safes [31] is  such a framework.  The Five Safes was created in the United Kingdom to assist a national statistical agency in  evaluating proposed collaborative projects with the larger research community. The frame- work is designed to assist in “designing, describing and evaluating” data access systems.  Here, the term “data access system” is viewed broadly as any mechanism that allows out-  30  NIST SP 800-188 3pd  November 2022  1234  1235  1236  1237  1238  1239  1240  1241  1242  1243  1244  1245  1246  1247  1248  1249  1250  1251  1252  1253  1254  1255  1256  1257  1258  1259  1260  1261  1262  1263  1264  1265  1266  1267  1268  1269  siders to gain access to the agency’s confdential data. That is, a data access system might  include setting up an enclave for academic researchers who undergo extensive background  checks, but it also includes publishing data on the internet.  The Five Safes framework gets its name from the use of fve categories (called “risk” or  “access” dimensions) that are used in the evaluation. They are:  1. Safe projects Is this use of the data appropriate?  2. Safe people Can the researchers be trusted to use it in an appropriate manner?  3. Safe data Is there a disclosure risk in the data itself?  4. Safe settings Does the access facility limit unauthorized use?  5. Safe outputs Are the statistical results non-disclosive?  Each of these dimensions is independent. That is, the legal, moral, and ethical review of  each dimension is independent of the others. In practice, this might mean that the project  is safe (the proposed use of the data is appropriate), the people are safe (the researchers are  noted academics with respected histories of collaborative work), the data are safe (there is  no disclosure risk in the data), and the output is safe (it will not disclose personal infor- mation). However, because the setting is not safe (perhaps the facility has poor internal  security), the project should not go forward. In this example, the Five Safes framework  would provide a decision-maker with the tools to separate each of these dimensions and  resolve the problems so that the project could proceed.  One of the positive aspects of the Five Safes framework is that it forces data controllers  to consider many different aspects of data release when evaluating data access proposals.  Frequently, the authors write, it is common for data owners to “focus on one, and only  one, particular issue (such as the legal framework surrounding access to their data or IT  solutions).” With the Five Safes, people who may be specialists in one area are forced to  consider (or to explicitly not consider) aspects of privacy protection with which they may  not be familiar and might otherwise overlook.  The Five Safes framework can be used as a tool for designing access systems, for evaluating  existing systems, for communication, and for training. Agencies should consider using a  framework such as The Five Safes for organizing risk analyses of data release efforts.  3.6. Disclosure Review Boards  Disclosure Review Boards (DRBs), also known as Data Release Boards, are administrative  bodies created within an organization that are charged with ensuring that intended dis- closures meet the policy and procedural requirements of that organization. DRBs should  be governed by a written mission statement and charter (or equivalent document) that  are ideally approved by the same mechanisms that the organization uses to approve other  organization-wide policies.  31  NIST SP 800-188 3pd  November 2022  1270  1271  1272  1273  1274  1275  1276  1277  1278  1279  1280  1281  1282  1283  1284  1285  1286  1287  1288  1289  1290  1291  1292  1293  1294  1295  1296  1297  1298  1299  1300  1301  1302  1303  1304  1305  1306  The DRB should have a mission statement that guides its activities. For example, the U.S.  Department of Education’s DRB has the mission statement:  The Mission of the Department of Education Disclosure Review Board (ED- DRB) is to review proposed data releases by the Department’s principal offces  (POs) through a collaborate technical assistance, aiding the Department to re- lease as much useful data as possible, while protecting the privacy of individ- uals and the confdentiality of their data, as required by law. [41]  The DRB charter specifes the mechanics of how the mission is implemented. A formal,  written charter promotes transparency in the decision-making process and ensures consis- tency in the applications of its policies.  Most DRBs will be established to weigh the interests of data release against those of in- dividual privacy protection. However, a DRB may also be chartered to consider group  harms [51, p.13] that can result from the release of a dataset. Such harms go beyond the  harm to the privacy interests of a specifc individual.  The DRB charter should frame the DRB’s responsibilities in reference to existing orga- nizational policies, regulations, and laws. Some agencies may balance these concerns by  employing data use models other than de-identifcation (e.g., by establishing data enclaves  where a limited number of vetted researchers can access sensitive datasets in a way that  provides data value while minimizing the possibility for harm or by authorizing the use  of secure multi-party computation, homomorphic encryption, or other Privacy Preserving  Data Analytics to compute various statistics). In those agencies, a DRB would be empow- ered to approve the use of such mechanisms.  Certain agencies may engage in data disclosure on a routine basis (such as research and  evaluation agencies), in which case it may be benefcial for the DRB to establish policies  and procedures for de-identifcation rather than being responsible for every review. In  these cases, the DRB charter should clearly specify how the group will provide oversight  and ensure organizational accountability to the agreed-upon policies.  The DRB charter should specify the DRB’s composition. To be effective, the DRB should  include representatives from multiple groups and experts in both technology and privacy  policy. Specifcally, DRBs may wish to have as members:  • Individuals who represent the interests of potential users (such individuals need not  come from outside of the organization)  • Representation from among the public, specifcally from groups represented in the  datasets if they have a limited scope  • Representation from the organization’s leadership team, such as a representation of  the Senior Agency Offcial for Privacy [4, Appendix II, section 4] (such representa- tion helps to establish the DRB’s credibility with the rest of the organization)  32  NIST SP 800-188 3pd  November 2022  1307  1308  1309  1310  1311  1312  1313  1314  1315  1316  1317  1318  1319  1320  1321  1322  1323  1324  1325  1326  1327  1328  1329  1330  1331  1332  1333  1334  1335  1336  1337  1338  1339  1340  • A representative of the organization’s senior privacy offcial  • Subject matter experts  • Outside experts  The charter should establish rules for ensuring a quorum and specify whether members can  designate alternates on a standing or meeting-by-meeting basis. The DRB should specify  the mechanism by which members are nominated and approved, their tenure, conditions  for removal, and removal procedures.15  The charter should set policy expectations for record keeping and reporting, including  whether records and reports are considered public or restricted. For example, the char- ter could specify that a DRB issue an annual report with a list of every dataset that was  approved for release. The charter should indicate whether it is possible to exclude sensitive  decisions from these reporting requirements and the mechanism for doing so. Ideally, the  charter should be a public document to promote transparency.  To meet its requirement of evaluating data releases, the DRB should require that writ- ten applications be submitted to the DRB that specify the nature of the dataset, the de- identifcation methodology, and the result. An application may require that the proposer  present the re-identifcation risk, the risk to individuals if the dataset is re-identifed, and  a proposed plan for detecting and mitigating successful re-identifcation. In addition, the  DRB should require that when individuals are informed that their information will be de- identifed, they also be informed that privacy risks may remain despite de-identifcation.  The DRB should keep accurate records of its request memos, their associated documen- tation, the DRB decision, and the actual fles released. These records should be appropri- ately archived and curated so that they can be recovered. In the case of large data releases,  the defnitive version of the released data should be curated using an externally validated  procedure, such as a recorded cryptographic hash value or signature, and a digital object  identifer (DOI) [64].  DRBs may wish to institute a two-step process in which the applicant frst proposes and  receives approval for a specifc de-identifcation process that will be applied to a specifc  dataset and then submits and receives approval for the release of the dataset that has been  de-identifed according to the proposal. However, because it is theoretically impossible  to predict the results of applying an arbitrary process to an arbitrary dataset [26, 129],  the DRB should be empowered to reject a proposed release of a dataset even if it has  been de-identifed in accordance with an approved procedure because performing the de- identifcation may demonstrate that the procedure was insuffcient to protect privacy. The  15For example, in 2022, the Census Bureau’s DRB had 12 voting members: two technical co-chairs, a repre- sentative from the Policy Coordination Offce, a representative from the Associate Director for Communica- tions, two representatives from the Center for Enterprise Dissemination-Disclosure Avoidance (CED-DA),  two representatives from the Economic Programs Directorate, two representatives from the Demographic  Programs Directorate, and two representatives from the Decennial Programs Directorate [24].  33  NIST SP 800-188 3pd  November 2022  1341  1342  1343  1344  1345  1346  1347  1348  1349  1350  1351  1352  1353  1354  1355  1356  1357  1358  1359  1360  1361  1362  1363  1364  1365  1366  1367  1368  1369  1370  1371  1372  1373  1374  DRB should be able to delegate the responsibility of reviewing the de-identifed dataset,  but such responsibility should not be delegated to the individual or group that performed  the de-identifcation.  The DRB charter should specify whether the DRB needs to approve each data release by  the organization or if it may grant blanket approval for all data of a specifc type that is de- identifed according to a specifc methodology. The charter should specify the duration of  the approval. Given advances in the science and technology of de-identifcation, it is inad- visable that a Board be empowered to grant release authority for an indefnite or unlimited  amount of time.  In most cases, a single privacy protection methodology will be insuffcient to protect the  varied datasets that an agency may wish to release. That is, different techniques might best  optimize the trade-off between re-identifcation risk and data usability, depending on the  specifcs of each kind of dataset. Nevertheless, the DRB may wish to develop guidance, rec- ommendations, and training materials regarding specifc de-identifcation techniques that  are to be used. Agencies that standardize on a small number of de-identifcation techniques  will gain familiarity with these techniques and are likely to have results with a higher level  of consistency and success than those that have no such guidance or standardization.  Although it is envisioned that DRBs will work in a cooperative, collaborative, and conge- nial manner with those inside an agency seeking to release de-identifed data, there will  at times be a disagreement of opinion. For this reason, the DRB’s charter should state  whether the DRB has the fnal say over disclosure matters or if the DRB’s decisions can be  overruled, by whom, and by what procedure. For example, an agency might give the DRB  fnal say over disclosure matters but allow the agency’s leadership to replace members of  the DRB as necessary. Alternatively, the DRB’s rulings might merely be advisory, with all  data releases being individually approved by agency leadership or its delegates.16  Finally, agencies should decide whether the DRB charter will include any kind of perfor- mance timetables or be bound by a service-level agreement (SLA) that defnes a level of  service to which the DRB commits.  The key elements of a Disclosure Review Board include:  • A written mission statement and charter  • Members represent different groups within the organization, including leadership  • The Board receives written applications to release de-identifed data  • The Board reviews both the proposed methodology and the results of applying the  methodology  16At the Census Bureau, “staff members [who] are not satisfed with the DRB’s decision . . . may appeal to a  steering committee consisting of several Census Bureau Associate Directors. Thus far, there have been few  appeals, and the Steering Committee has never reversed a decision made by the Board” [130, p.35].  34  NIST SP 800-188 3pd  November 2022  1375  1376  1377  1378  1379  1380  1381  1382  1383  1384  1385  1386  1387  1388  1389  1390  1391  1392  1393  1394  1395  1396  1397  1398  1399  1400  1401  1402  1403  1404  1405  1406  • Applications should identify the risks associated with data release, including re- identifcation probability, potentially adverse events that would result if individuals  are re-identifed, and a mitigation strategy if re-identifcation takes place  • Approvals may be valid for multiple releases but should not be valid indefnitely  • Reliable records management for applications, approvals, and released data  • Mechanisms for dispute resolution  • Timetable or service-level agreement (SLA)  • Legal and technical understanding of privacy  Example outputs of a DRB include specifying access methods for different kinds of data  releases, establishing acceptable levels of re-identifcation risk, and maintaining detailed  records of previous data releases that ideally include the dataset that was released and the  privacy-preserving methodology that was employed.  There is some similarity between DRBs as envisioned here and the Institutional Review  Board (IRBs) system created by the Common Rule17 for regulating human subject research  in the United States. However, there are also important differences:  • While the purpose of IRBs is to protect human subjects involved in human subject  research, DRBs are charged with protecting data subjects, institutions, and – poten- tially – society as a whole.  • Whereas IRBs are required to have “at least one member whose primary concerns  are in nonscientifc areas” and “at least one member who is not otherwise affliated  with the institution and who is not part of the immediate family of a person who is  affliated with the institution,” there does not appear to be a requirement for such  members on a DRB.  • Whereas IRBs give approval for research and then typically receive reports only dur- ing an annual review or when a research project terminates, DRBs may be involved  at multiple points during the process.  • Whereas approval of an IRB is required before research with human subjects can  commence, DRBs are typically involved after research has taken place and prior to  data or other research fndings being released.  • Whereas service on an IRB requires knowledge of the Common Rule and an under- standing of ethics, service on a DRB requires knowledge of statistics, computation,  public policy, and some familiarity with the data being considered for release.  17The Federal Policy for the Protection of Human Subjects or the “Common Rule” was published in 1991  and codifed in separate regulations by 15 federal departments and agencies. The Revised Common Rule  was published in the Federal Register (FR) on January 19, 2017, and was amended to delay the effective  and compliance dates on January 22, 2018, and June 19, 2018 [135].  35  NIST SP 800-188 3pd  November 2022  1407  1408  1409  1410  1411  1412  1413  1414  1415  1416  1417  1418  1419  1420  1421  1422  1423  1424  1425  1426  1427  1428  1429  1430  1431  1432  1433  1434  1435  1436  1437  1438  1439  1440  1441  1442  3.7. De-Identifcation Standards  Agencies can rely on de-identifcation standards to provide standardized terminology, pro- cedures, and performance criteria for de-identifcation efforts. Agencies can adopt existing  de-identifcation standards or create their own. De-identifcation standards can be prescrip- tive or performance-based.  3.7.1. Benefts of Standards  De-identifcation standards assist agencies with the process of de-identifying data prior to  public release. Without standards, data owners may be unwilling to share data, as they may  be unable to assess whether a procedure for de-identifying data is suffcient to minimize  privacy risk.  Standards can increase the availability of individuals with appropriate training by iden- tifying a specifc body of knowledge and practice that training should address. Absent  standards, agencies may forego opportunities to share data. De-identifcation standards can  help practitioners develop a community, as well as certifcation and accreditation processes.  Standards decrease uncertainty and provide data owners and custodians with best practices  to follow. Courts can consider standards as acceptable practices that should generally be  followed. In the event of litigation, an agency can point to the standard and say that it  followed good data practice.  3.7.2. Prescriptive De-Identifcation Standards  A prescriptive de-identifcation standard specifes an algorithmic procedure that – if fol- lowed – results in data that are de-identifed to an established benchmark.  The “Safe Harbor” method of the HIPAA Privacy Rule [3] is an example of a prescriptive  de-identifcation standard. The intent of the Safe Harbor method is to “provide covered en- tities with a simple method to determine if the information is adequately de-identifed” [89].  It does this by specifying that health information is considered to be de-identifed through  the removal of 18 kinds of identifers and the assurance that the entity does not have actual  knowledge that the remaining information can be used to identify an individual who is the  subject of the information. Once de-identifed, the dataset is no longer subject to HIPAA  privacy, security, and breach notifcation regulations. Nevertheless, “a covered entity may  require the recipient of de-identifed information to enter into a data use agreement to ac- cess fles with known disclosure risk” [89].  The Privacy Rule states that a covered entity that employs the Safe Harbor method must  have no “actual knowledge” that the information – once de-identifed – could still be used  to re-identify individuals. However, covered entities are not obligated to employ experts  or mount re-identifcation attacks against datasets to verify that the use of the Safe Harbor  method has in fact resulted in data that cannot be re-identifed.  36  NIST SP 800-188 3pd  November 2022  1443  1444  1445  1446  1447  1448  1449  1450  1451  1452  1453  1454  1455  1456  1457  1458  1459  1460  1461  1462  1463  1464  1465  1466  1467  1468  1469  1470  1471  1472  1473  1474  1475  1476  1477  Prescriptive standards have the advantage of being relatively easy for users to follow, but  developing, testing, and validating such standards can be burdensome. Because prescrip- tive de-identifcation standards do not depend on the particulars of a specifc case, there  is a tendency for them to be more conservative than is necessary, resulting in an unneces- sary decrease in data for corresponding levels of risk. Even so, there is no assurance that  following a prescriptive standard actually produces the intended outcome.  Agencies that create prescriptive de-identifcation standards should ensure that data de- identifed according to the standards have a suffciently small risk of being re-identifed  consistent with the intended level of privacy protection. Such assurances frequently can- not be made unless formal privacy techniques, such as differential privacy, are employed.  However, agencies may determine that public policy goals furthered by having an easy- to-use prescriptive standard outweighs the risk of a standard that does not have provable  privacy guarantees.  Prescriptive de-identifcation standards carry the risk that the standard may not suffciently  de-identify to avoid the risk of re-identifcation, especially as methodology advances and  more data sources become available.  A second risk when adopting prescriptive standards is that different agencies (or govern- ments) may adopt inconsistent rules. In such a case, information that is legally de-identifed  for one purpose or in one jurisdiction may not be legally de-identifed in another.  3.7.3. Performance-Based De-Identifcation Standards  Performance-based de-identifcation standards specify the properties that de-identifed data  must have. For example, under the “Expert Determination” method of the HIPAA Privacy  Rule, a technique for de-identifying data is suffcient if an appropriate expert applying  generally accepted statistical and scientifc principles and methods “determines that the  risk is very small that the information could be used, alone or in combination with other  reasonably available information, by an anticipated recipient to identify an individual who  is a subject of the information” [89]. The rule requires that experts document their methods  and the results of their analyses.  Performance-based standards have the advantage of allowing users many different ways to  solve a problem by leaving room for innovation. Another advantage is that they can require  the desired outcome rather than specifying an aspirational mechanism.  Performance-based standards should be suffciently detailed to perform in a manner that is  reliable and repeatable. For example, standards that call for the use of experts can specify  how an expert’s expertise should be determined. Standards that call for the reduction of  risk to an acceptable level should provide a procedure for determining that level.  37  NIST SP 800-188 3pd  November 2022  1478  1479  1480  1481  1482  1483  1484  1485  1486  1487  1488  1489  1490  1491  1492  1493  1494  1495  1496  1497  1498  1499  1500  1501  1502  1503  1504  1505  1506  1507  1508  1509  1510  1511  3.8. Education, Training, and Research  De-identifying data in a manner that preserves privacy can be a complex mathematical,  statistical, administrative, and data-driven process. Frequently, the opportunities for iden- tity disclosure will vary from dataset to dataset. Privacy-protecting mechanisms developed  for one dataset may not be appropriate for others. For these reasons, agencies that engage  in de-identifcation should ensure that their workers have adequate education and training  in the subject domain. Agencies may wish to establish education or certifcation require- ments for those who work directly with the datasets or to adopt industry standards such  as the HITrust De-Identifcation Framework [77]. Because de-identifcation techniques are  modality-dependent, agencies using de-identifcation may need to institute research efforts  to develop and test appropriate data release methodologies.  3.9. Defense in Depth  In addition to de-identifcation, there are other technologies and methodologies that can  secure sensitive data. Many of these approaches can complement de-identifcation and  further reduce privacy risk to data subjects. Combining techniques is an example of defense  in depth and should be considered whenever possible.  3.9.1. Encryption and Access Control  Encrypting sensitive data at rest can prevent attackers from obtaining the data directly  (e.g., by compromising the server that stores it). Encryption can also serve as a form of  access control (i.e., it can control who can access the data) because examining the data  requires access to the encryption keys. If the original data (with identities) are retained,  they should be stored encrypted, and access should be limited. Even after de-identifcation,  more sensitive data not intended for public release can be provided to select individuals by  limiting access via encryption.  3.9.2. Secure Computation  Two technologies enable computing on encrypted data without decrypting it:  1. Fully-homomorphic encryption (FHE) [55] allows a server to compute a function  f (x) on an encrypted value x without decrypting it. The result is a new encrypted  value that can only be decrypted by someone who holds the original encryption key.  2. Secure multi-party computation (MPC) [74] allows multiple servers to jointly  compute a function f (x1, . . . ,xk), where each server provides one of the inputs xi,  and no server learns any of the others’ inputs.  Both of these approaches are general-purpose in that they can be used to compute any  function, and both are considerably slower than performing the equivalent computation  38  NIST SP 800-188 3pd  November 2022  1512  1513  1514  1515  1516  1517  1518  1519  1520  1521  1522  1523  1524  1525  1526  1527  1528  1529  1530  1531  1532  1533  1534  1535  1536  1537  1538  1539  1540  1541  1542  1543  1544  1545  1546  with unencrypted data on a single computer. Nevertheless, both approaches are now suf- fciently performant that they can be used for many practical kinds of privacy-preserving  data analysis.18  3.9.3. Trusted Execution Environments  Trusted Execution Environments (TEEs) (also called trusted hardware enclaves or secure  hardware enclaves) are another approach for computing on encrypted data. TEEs are im- plemented in computer hardware, typically within the silicon of a modern CPU, and pro- tect programs that run on that CPU from the surrounding environment. For example, a  TEE can cause data from a computer’s CPU to be automatically encrypted when written  to main memory and decrypted when read back to the CPU. In this way, data in memory  are protected from other devices that can access memory, such as a network interface card.  In addition to encryption, TEEs typically support attestation so that a program running on  a TEE can attest to a remote system that the program is a true, legitimate, and faithful  execution of the program.  Traditional cloud services require trusting the cloud provider, who may have a compro- mised environment (e.g., an operating system that records encryption keys). A TEE de- creases the need for trust because it allows a user to validate that they are communicating  with the remote program and offers assurance that no other program running in the cloud  provider can access the program’s data. Secure enclaves can thus be used to allow untrusted  infrastructure to operate on sensitive data in much the same way as technologies like FHE  and MPC.  Intel’s Software Guard Extensions (SGX) [112], ARM’s TrustZone [101], and AMD’s Se- cure Encrypted Virtualization (SEV) [13] are all examples of secure hardware enclaves.  All of these products are designed to provide similar security to cryptographic techniques  while also providing performance similar to a single CPU operating on unencrypted data.  These secure hardware products are necessarily complex, and various implementation er- rors have been discovered that can allow attackers to defeat their security protections. Se- cure hardware enclaves certainly offer increased security for data compared to plaintext  computation, but agencies should carefully consider the trade-off between performance  and security when choosing between secure hardware and cryptographic techniques.  3.9.4. Physical Enclaves  For extremely sensitive data, a physical enclave (see Section 3.4) may provide additional  security. In this model, data are stored on a computer not connected to any network and  are accessible only via physical access to a particular room. Access to the data is then  controlled by limiting access to the room. This approach can be quite cumbersome.  18More information about these and other kinds of secure computation can be found on the NIST Privacy- Enhancing Cryptography (PEC) project website at https://csrc.nist.gov/projects/pec.  39  https://csrc.nist.gov/projects/pec  NIST SP 800-188 3pd  November 2022  1547  1548  1549  1550  1551  1552  1553  1554  1555  1556  1557  1558  1559  1560  1561  1562  1563  1564  1565  1566  1567  1568  1569  1570  1571  1572  1573  1574  1575  1576  1577  s  r  4. Technical Steps for Data De-Identifcation  The goal of de-identifcation is to transform data in a way that protects privacy while pre- erving the validity of inferences drawn on that data within the context of a target use-case.  This section discusses technical options for performing de-identifcation and verifying the  esult of a de-identifcation procedure.  Agencies should adopt a detailed, written process for de-identifying data prior to com- mencing work on a de-identifcation project. The details of the process will depend on the  particular de-identifcation approach that is pursued. In developing technical steps for data  de-identifcation, agencies may wish to consider existing de-identifcation standards, such  as the HIPAA Privacy Rule, the IHE De-Identifcation Handbook [61], or the HITRUST  De-Identifcation Framework [77].  4.1. Determine the Privacy, Data Usability, and Access Objectives  Agencies intent on de-identifying data for release should understand the nature of the  data that they intend to de-identify and determine the policies and standards that will be  used to determine acceptable levels of data accuracy, de-identifcation, and the risk of re- identifcation. For example:  • Where did the data come from?  • What promises were made when the data were collected?  • What are the legal and regulatory requirements regarding data privacy and release?  • What is the purpose of the data release?  • What is the intended use of the data?  • What data-sharing model (Section 3.4) will be used?  • Which standards for privacy protection or de-identifcation will be used?  • What is the level of risk that the project is willing to accept?  • What are the goals for limiting re-identifcation? For example:  – No one can be re-identifed.  – Only a few people can be re-identifed.  – Only a few people can be re-identifed in theory, but no one will actually be  re-identifed in practice.  – Only outliers can be re-identifed.  – Only people who are not outliers can be re-identifed.  40  NIST SP 800-188 3pd  November 2022  1578  1579  1580  1581  1582  1583  1584  1585  1586  1587  1588  1589  1590  1591  1592  1593  1594  1595  1596  1597  1598  1599  1600  1601  1602  1603  1604  1605  1606  1607  1608  1609  1610  1611  1612  1613  – There is a small percentage chance of re-identifcation that is shared by every- one in the dataset.  – There is a small percentage chance of re-identifcation, but some people in the  dataset are signifcantly more likely to be re-identifed, and the re-identifcation  probability is somehow bounded.  • What harm might result from re-identifcation, and what techniques will be used to  mitigate those harms?  • How should compliance with that level of risk be determined?  Some goals and objectives are synergistic, while others are in opposition.  4.2. Conducting a Data Survey  Different kinds of data require different kinds of de-identifcation techniques. As a result,  an important early step in the de-identifcation of government data is to identify the data  modalities that are present in the dataset and formulate a plan for de-identifcation that takes  into account goals for data release, data accuracy, privacy protection, and the best available  science.  For example:  • Tabular numeric and categorical data is the subject of the majority of de-identifcation  research and practice. These datasets are most frequently de-identifed by using tech- niques based on the designation and removal of direct identifers and the manipula- tion of quasi-identifers. The chief criticism of de-identifcation based on direct and  quasi-identifers is that administrative determinations of quasi-identifers may miss  variables that can be uniquely identifying when combined and linked with external  data, including data that are not available at the time the de-identifcation is per- formed but become available in the future.  K-anonymity [122] is a common framework for performing and evaluating the de- identifcation of tabular numeric and categorical data. However, risk determinations  based on this kind of de-identifcation will be incorrect if direct and quasi-identifers  are not properly classifed. For example, if there exist quasi-identifers that are not  identifed as such and not subjected to k-anonymity, then it may be easy to re-identify  records in the de-identifed dataset.  Tabular data may also be used to create a synthetic dataset that preserves some infer- ence validity but does not have a one-to-one correspondence to the original dataset.  • Dates and times require special attention when de-identifying because temporal in- formation is inherently linked to an external dataset: the natural progression of time.  Some dates and times (e.g., February 22, 1732) are highly identifying, while others  are not. Dates that refer to matters of public record (e.g., date of birth, death, or home  41  NIST SP 800-188 3pd  November 2022  1614  1615  1616  1617  1618  1619  1620  1621  1622  1623  1624  1625  1626  1627  1628  1629  1630  1631  1632  1633  1634  1635  1636  1637  1638  1639  1640  1641  1642  1643  1644  1645  1646  1647  1648  1649  purchase) should be routinely taken as having high re-identifcation potential. Dates  may also form the basis of linkages between dataset records or even within a record.  For example, a record may contain the date of admission, the date of discharge, and  the number of days in residence. Thus, care should be taken when de-identifying  dates to locate and properly handle potential linkages and relationships. Applying  different techniques to different felds may result in information being left in a dataset  that can be used for re-identifcation. Specifc issues regarding date de-identifcation  are discussed in Section 4.3.4, “De-Identifying Dates.”  • Geographic and map data also require special attention when de-identifying, as  some locations can be highly identifying, other locations are not identifying at all,  and some locations are only identifying at specifc times. As with dates and times,  de-identifying geographic locations is challenging because locations inherently link  to an external reality, and some locations during specifc time periods are highly  correlated with specifc individuals (e.g., 38.8977° N, 77.0365° W). Identifying lo- cations can be de-identifed through the use of perturbation or generalization. The  effectiveness of such de-identifcation techniques for protecting privacy in the pres- ence of external information has not been well-characterized [51, p.37][115]. Spe- cifc issues regarding geographical de-identifcation are discussed in Section 4.3.5,  “De-Identifying Geographical Locations.”  • Unstructured text may contain direct identifers, such as a person’s name, or may  contain additional information that can serve as a quasi-identifer. Finding such iden- tifers invariably requires domain-specifc knowledge [51, p. 30]. Note that unstruc- tured text may be present in tabular datasets and require special attention.19  • Photos and video may contain identifying information, such as printed names (e.g.,  name tags), as well as metadata in the fle format. A range of biometric techniques  also exists for matching photos of individuals against a dataset of photos and identi- fers [51, p. 32].  • Medical imagery poses additional problems over photographs and video due to the  presence of technical, medically specifc information. For example, identifying in- formation may be present in the image itself (e.g., a photo may show an identifying  scar or tattoo), an identifer may be “burned in” to the image area (e.g., an identif- cation plate containing a patient name that is included in an X-Ray), or an identifer  may be present in the fle metadata. The body part in the image itself may also be  recognized using a biometric algorithm and dataset [51, p.35].  • Genetic sequences and other kinds of sequence information can be identifed by  using existing databanks that match sequences and identities. There is also evi-  19For an example of how unstructured text felds can damage the policy objectives and privacy assurances of  a larger structured dataset, see Andrew Peterson’s article, “Why the names of six people who complained  of sexual assault were published online by Dallas police” [98].  42  NIST SP 800-188 3pd  November 2022  1650  1651  1652  1653  1654  1655  1656  1657  1658  1659  1660  1661  1662  1663  1664  1665  1666  1667  1668  1669  1670  1671  1672  1673  1674  1675  1676  1677  1678  1679  dence that genetic sequences from individuals who are not in datasets can be matched  through genealogical triangulation – a process that uses genetic information and other  information as quasi-identifers to single out a specifc identity [51, p.36]. At present,  there is no known method to reliably de-identify genetic sequences. Specifc issues  regarding the de-identifcation of genetic information is discussed in Section 4.3.6,  “De-Identifying Genomic Information.”  In many cases, data are complex and contain multiple modalities. Such mixtures may  complicate risk determinations.  4.3. De-Identifcation by Removing Identifers and Transforming Quasi-Identifers  De-identifcation based on the removal of identifers and the transformation of quasi-identifers  is one of the most common approaches currently in use. It has the advantage of being con- ceptually straightforward, and there is a long institutional history of using this approach  within both federal statistical agencies and the healthcare industry. This approach has the  disadvantage of not being based on formal methods for assuring privacy protection. The  lack of formal methods does not mean that this approach cannot protect privacy, but it does  mean that privacy protection is not assured.  Below is a sample process for de-identifying data by removing identifers and transforming  quasi-identifers:20  1. Determine the re-identifcation risk threshold. The organization determines accept- able risk for working with the dataset and possibly mitigating controls based on  strong precedents and standards.21  2. Determine the information in the dataset that could be used to identify the data sub- jects. Identifying information can include:  Direct identifers including names, phone numbers, and other information that un- ambiguously identifes an individual.  Quasi-identifers that could be used in a linkage attack. Typically, quasi-identifers  identify multiple individuals and can be used to triangulate a specifc individual.  High-dimensional data [10] that can be used to single out data records and thus  constitute a unique pattern that could be identifying if the values exist in a  secondary source to link against.22  20This protocol is based on a protocol developed by Professors Khaled El Emam and Bradley Malin [44].  21See the Federal Committee on Statistical Methodology’s Data Protection Toolkit at  https://nces.ed.gov/fcsm/dpt.  22For example, Narayanan and Shmatikov demonstrated that the set of movies that a person had watched  could be used as an identifer given the existence of a second dataset of movies that had been publicly  rated [84].  43  https://nces.ed.gov/fcsm/dpt  NIST SP 800-188 3pd  November 2022  1680  1681  1682  1683  1684  1685  1686  1687  1688  1689  1690  1691  1692  1693  1694  1695  1696  1697  1698  1699  1700  1701  1702  1703  1704  1705  1706  1707  1708  1709  1710  1711  1712  1713  3. Determine the direct identifers in the dataset. An expert determines the elements in  the dataset that only serve to identify the data subjects.  4. Mask (transform) direct identifers. The direct identifers are either removed or re- placed with pseudonyms. Options for performing this operation are discussed in  Section 4.3.1.  5. Perform threat modeling. The organization determines the additional information  they might be able to use for re-identifcation, including both quasi-identifers and  non-identifying values that a data intruder might use for re-identifcation.  6. Determine minimal acceptable data accuracy. The organization determines what uses  can or will be made with the de-identifed data.  7. Determine the transformation process that will be used to manipulate the quasi- identifers. Pay special attention to the data felds that contain dates and geographical  information, removing or recoding as necessary.  8. Import (sample) data from the source dataset. Because the effort to acquire data from  the source (identifed) dataset may be substantial, some researchers recommend a test  data import run to assist in planning [44].  9. Review the results of the trial de-identifcation. Correct any coding or algorithmic  errors that are detected.  10. Transform the quasi-identifers for the entire dataset.  11. Evaluate the actual re-identifcation risk, which is calculated. As part of this evalua- tion, every aspect of the released dataset should be considered in light of the question,  “Can this information be used to identify someone?”  12. Compare the actual re-identifcation risk with the threshold specifed by the policy- makers.  13. If the data do not pass the actual risk threshold, adjust the procedure and repeat Steps  11 and 12. For example, additional transformations may be required. Alternatively,  it may be necessary to remove outliers. Removing data will of course impact data  quality, but it will also protect the privacy of the individuals whose data has been  removed.  4.3.1. Removing or Transforming of Direct Identifers  There are many possible processes for removing direct identifers from a dataset, including:  • Removal and replacement. Replace identifers with the value used by the database  to indicate a missing value, such as NULL or NA.  • Masking. Replace identifers with a repeating character, such as XXXXXX or 999999.  44  NIST SP 800-188 3pd  November 2022  1714  1715  1716  1717  1718  1719  1720  1721  1722  1723  1724  1725  1726  1727  1728  1729  1730  1731  1732  1733  1734  1735  1736  1737  1738  1739  1740  1741  1742  1743  1744  1745  1746  1747  1748  • Encryption. Encrypt the identifers with a strong encryption algorithm. After en- cryption, the key can be discarded the cryptographic key to prevent decryption. How- ever, if there is a desire to employ the same transformation at a later point in time,  the key should not be discarded but rather stored in a secure location separate from  the de-identifed dataset. Encryption used for this purpose carries special risks that  need to be addressed with specifc controls (see Section 4.3.2 below for further in- formation).  • Hashing with a keyed hash. A keyed hash is a special kind of hash function that  produces different hash values for different keys. The hash key should have suffcient  randomness to defeat a brute force attack aimed at recovering the hash key (e.g.,  SHA-256 HMAC [20] with a 256-bit randomly generated key). As with encryption,  the key should be discarded unless there is a desire for repeatability. Hashing used  for this purpose carries special risks that need to be addressed with specifc controls  (see Section 4.3.2 below for further information).  • Replacement with keywords. This approach transforms identifers such as George  Washington to PATIENT. Note that some keywords may be equally identifying, such  as transforming George Washington to PRESIDENT.  • Replacement with realistic surrogate values. This approach transforms identifers  such as George Washington to surrogates that blend in, such as 23 Abraham Polk.  Encryption, hashing with a keyed hash, and replacement with realistic surrogate values are  pseudonymization techniques. The technique used to remove direct identifers should be  clearly documented for users of the dataset – especially if the technique of replacement by  realistic surrogate names is used – so that future data users have documentation that the  dataset has been de-identifed.  If the agency plans to make data available for longitudinal research and contemplates mul- tiple data releases, then the transformation process should be repeatable, and the resulting  transformed identities should be pseudonyms. The mapping between the direct identifer  and the pseudonym is performed using a lookup table or a repeatable transformation. In  either case, the release of the lookup table or the information used for the repeatable trans- formation will result in compromised identities. Thus, the lookup table or the information  for the transformation must be highly protected. When using a lookup table, the pseudonym  must be randomly assigned.  A signifcant risk of using a repeatable transformation is that a data intruder may be able  to determine the transformation and – in so doing – gain the capability to re-identify all of  the records in the dataset.  23A study by Carrell et. al found that using realistic surrogate names in de-identifed text like John Walker  and 3900 Pennsylvania Ave instead of generic labels like PATIENT and ADDRESS could decrease or  mitigate the risk of re-identifying the few names that remained in the text because “the reviewers were  unable to distinguish the residual (leaked) identifers from the...surrogates” [21].  45  NIST SP 800-188 3pd  November 2022  1749  1750  1751  1752  1753  1754  1755  1756  1757  1758  1759  1760  1761  1762  1763  1764  1765  1766  1767  1768  1769  1770  1771  1772  1773  1774  1775  1776  1777  1778  1779  1780  1781  1782  1783  1784  When multiple organizations use the same pseudonymization scheme, they can trade data  and perform matching on the pseudonyms. However, this practice also allows the orga- nizations to re-identify each other’s shared datasets. As an alternative, organizations can  participate in a private set intersection protocol, of which there are many in the crypto- graphic literature [78, 34, 69].  4.3.2. Special Security Note Regarding the Encryption or Hashing of Direct  Identifers  The transformation of direct identifers through encryption or hashing carries special risks,  as errors in procedure or the release of the encryption key can compromise identities for  the entire dataset.  When information is protected with encryption, the security of the encrypted data depends  entirely on the security of the encryption key. If a key is improperly chosen, it may be  possible for a data intruder to discover the key using a brute force search. Because there  is no visual difference between data that are encrypted with a strong encryption key and  data that are encrypted with a weak key, organizations must utilize administrative controls  to ensure that keys are both unpredictable and suitably protected. The use of encryption or  hashing to protect direct identifers is, therefore, not recommended.  4.3.3. De-Identifying Numeric Quasi-Identifers  Once a determination is made regarding quasi-identifers, they should be transformed. A  variety of techniques are available to transform quasi-identifers:  • Top and bottom coding. Outlier values that are above or below certain values are  coded appropriately. For example, the HIPAA Privacy Rules calls for ages over 89  to be “aggregated into a single category of age 90 or older” [132, § 164.514 (b)].  • Micro aggregation. Individual microdata are combined into small groups that pre- serve some data analysis capability while providing for some disclosure protec- tion [110].  • Generalize categories with small counts. When preparing contingency tables, sev- eral categories with small values may be combined. For example, rather than re- porting that there is one person with blue eyes, two people with green eyes, and one  person with hazel eyes, it may be reported that there are four people with blue, green,  or hazel eyes.  • Data suppression. Cells in contingency tables with counts lower than a predefned  threshold can be suppressed to prevent the identifcation of attribute combinations  with small numbers [141].  • Blanking and imputing. Specifc values that are highly identifying can be removed  and replaced with imputed values.  46  NIST SP 800-188 3pd  November 2022  1785  1786  1787  1788  1789  1790  1791  1792  1793  1794  1795  1796  1797  1798  1799  1800  1801  1802  1803  1804  1805  1806  1807  1808  1809  1810  1811  1812  1813  1814  1815  1816  1817  1818  1819  1820  1821  1822  • Attribute or record swapping. Attributes or data values are swapped within a set  of similar records. For example, data that represent families in two similar towns  within a county might be swapped with each other. “Swapping has the additional  quality of removing any 100-percent assurance that a given record belongs to a given  household” [130, p.31] while preserving the accuracy of regional statistics, such as  sums and averages. In this case, the average number of children per family in the  county would be unaffected by data swapping. However, swapping may damage  or destroy important relationships within the data and introduce systematic biases,  depending on how the swapping candidates are selected.  • Noise infusion. Also called “partially synthetic data,” this approach adds small ran- dom values to attributes. For example, instead of reporting that a person is 84 years  old, the person may be reported as being 79 years old. Noise infusion increases  variance in reported statistics and leads to attenuation bias in estimated regression  coeffcients and correlations among attributes [36, 7]. When combined with a re- quirement for non-negative reporting of attributes, such as age or population, noise  infusion also introduces systematic bias since more values are increased in value than  decreased.  These techniques (and others) are described in detail in several publications, including:  • Statistical Policy Working Paper #22. (Second version, 2005) by the Federal Com- mittee on Statistical Methodology [47]. This 137-page paper includes worked exam- ples of disclosure limitation, specifc recommended practices for federal agencies,  profles of federal statistical agencies conducting disclosure limitation, and an ex- tensive bibliography. This document has been superseded by the Data Protection  Toolkit.  • The Data Protection Toolkit (BETA). A website maintained by the Federal Com- mittee on Statistical Methodology for the purpose of promoting data access while  protecting confdentiality throughout the federal statistical system [48]. https://nces.  ed.gov/fcsm/dpt  • The Anonymisation Decision-Making Framework. By Mark Elliot, Elaine MacKey,  Kieron O’Hara and Caroline Tudor, UKAN, University of Manchester, Manchester,  UK, 2016. This 156-page book provides tutorials and worked examples for de- identifying data and calculating risk.  • IHE IT Infrastructure Handbook: De-Identifcation. (Integrating the Health- care Enterprise, June 6, 2014) IHE offers a variety of guides, including one on de- identifcation at http://www.ihe.net/User Handbooks/.  Swapping and noise infusion both introduce noise into the dataset, such that records lit- erally contain incorrect data. Certain kinds of noise infusion have been mathematically  proven to provide formal privacy guarantees. Swapping has no such guarantees.  47  https://nces.ed.gov/fcsm/dpt https://nces.ed.gov/fcsm/dpt https://nces.ed.gov/fcsm/dpt http://www.ihe.net/User_Handbooks/  NIST SP 800-188 3pd  November 2022  1823  1824  1825  1826  1827  1828  1829  1830  1831  1832  1833  1834  1835  1836  1837  1838  1839  1840  1841  1842  1843  1844  1845  1846  1847  1848  1849  1850  1851  1852  1853  1854  1855  1856  1857  1858  1859  All of these techniques impact data accuracy, but whether they impact data utility depends  on the downstream uses of the data. For example, top-coding household incomes will not  impact a measurement of the 90-10 quantile ratio, but it will impact a measurement of the  top 1% of household incomes [99].  Prior to the adoption of differential privacy by the U.S. Census Bureau, federal statistical  agencies largely did not document the specifc statistical disclosure techniques they used  when performing statistical disclosure limitation. Likewise, statistical agencies did not  document the parameters used in the transformations nor the amount of data that have been  transformed, as documenting these techniques can allow a data intruder to reverse-engineer  the specifc values, eliminating privacy protection [7]. This lack of transparency sometimes  resulted in erroneous conclusions on the part of data users. This is another example of why  it is important for documentation of the de-identifcation process to accompany the release  of de-identifed data and is one of the motivations for the U.S. Census Bureau to to adopt  data privacy techniques that do not rely on secrecy for their effectiveness [56, 121, 58, 6].  4.3.4. De-Identifying Dates  Dates can exist in many ways in a dataset. Dates may be in particular kinds of typed  columns, such as a date of birth or the date of an encounter. Dates may be present as  a number, such as the number of days since an epoch like January 1, 1900. Dates may  be present in the free text narratives or in photographs (e.g., a photograph that shows a  calendar or a picture of a computer screen with date information).  Several strategies have been developed for de-identifying dates:  • Under the HIPAA Privacy Rule, dates must be generalized to no greater specifcity  than the year (e.g., July 4, 1776, becomes 1776).  • Dates within a single person’s record can be systematically adjusted by a random  amount. For example, dates of a hospital admission and discharge might be system- atically moved the same number of days – a date of admission and discharge of July  4, 1776, and July 9, 1776, become Sept. 10, 1777, and Sept. 15, 1777 [89]. However,  this does not eliminate the risk that a data intruder will make inferences based on the  interval between dates.  • In addition to a systematic shift, the intervals between dates can be perturbed to  protect against re-identifcation attacks that involve identifable intervals while still  maintaining the order of events.  • Some dates cannot be arbitrarily changed without compromising data accuracy. For  example, it may be necessary to preserve the day of the week, whether a day is a  workday or a holiday, or a relationship to a holiday or event.  • Some ages can be randomly adjusted without impacting data accuracy while others  cannot. For example, in many cases, the age of an individual can be randomly ad-  48  NIST SP 800-188 3pd  November 2022  1860  1861  1862  1863  1864  1865  1866  1867  1868  1869  1870  1871  1872  1873  1874  1875  1876  1877  1878  1879  1880  1881  1882  1883  1884  1885  1886  1887  1888  1889  1890  1891  1892  1893  1894  1895  justed ±2 years if the person is over the age of 25 but not if their age is between  one and three. However, individuals become eligible for specifc benefts at specifc  ages, such as Social Security retirement at age 62, so changes to ages around these  milestones may also result in data accuracy problems.  4.3.5. De-Identifying Geographical Locations and Geolocation Data  Geographical data can exist in many ways in a dataset. Geographical locations may be  indicated by map coordinates (e.g., 39.1351966, -77.2164013), a street address (e.g., 100  Bureau Drive), or a postal code (e.g., 20899). Geographical locations can also be embedded  in textual narratives.  Some geographical locations are not identifying (e.g., a crowded train station), while others  may be highly identifying (e.g., a house in which a single person lives). Other locations  may be identifying at some times of day and not others or during some months or some  years. The amount of noise required to de-identify geographical locations signifcantly  depends on the availability of external data, including geographical surveys. Identity may  be shielded in an urban environment by adding ±100 m, whereas a rural environment may  only require ±5 km or more to introduce suffcient ambiguity.  A prescriptive de-identifcation rule – even one that accounts for varying population densi- ties – may still be insuffcient for de-identifcation if the rule fails to consider the interaction  between geographic locations and other quasi-identifers in the dataset. Noise should be  added with caution to avoid the creation of inconsistencies in underlying data (e.g., mov- ing the location of a residence along a coast into a body of water or across geopolitical  boundaries).  Single locations may become identifying if they represent locations linked to a single in- dividual that are recorded over time (e.g., a work/home commuting pair). Such behavioral  time-location patterns can be quite distinct and allow for re-identifcation even with a small  number of recorded locations per individual [82, 81]. Research in 2021 concluded that  “[t]he risk of re-identifcation remains high even in country-scale location datasets” [46].  Data that are of higher resolution are typically more identifying. For example, in July  2021, the Catholic publication The Pillar published a report in which it had purchased  the de-identifed geolocation information for users of a homosexual dating platform. With  this data, the journalists identifed a prominent Catholic offcial as a user of the platform  by simply matching the geolocation data to the offcial’s offcial residence. The offcial  promptly resigned [100].  4.3.6. De-Identifying Genomic Information  Deoxyribonucleic acid (DNA) is the molecule inside human cells that carries genetic in- structions used for the proper functioning of living organisms. DNA present in the cell  49  NIST SP 800-188 3pd  November 2022  1896  1897  1898  1899  1900  1901  1902  1903  1904  1905  1906  1907  1908  1909  1910  1911  1912  1913  1914  1915  1916  1917  1918  1919  1920  1921  1922  1923  1924  1925  1926  1927  1928  1929  1930  1931  1932  1933  1934  nucleus is inherited from both parents, while DNA present in the mitochondria is only  inherited from an organism’s mother.  DNA is a repeating polymer that is made from four chemical bases: adenine (A), guanine  (G), cytosine (C), and thymine (T). Human DNA consists of roughly 3 billion bases, of  which 99% are the same in all people [54]. Modern technology allows for the complete  specifc sequence of an individual’s DNA to be chemically determined, although this is  rarely done in practice. With current technology, it is far more common to use a DNA  microarray to probe for the presence or absence of specifc DNA sequences at predeter- mined points in the genome. This approach is typically used to determine the presence  or absence of specifc single nucleotide polymorphisms (SNPs) [53]. DNA sequences and  SNPs are the same for monozygotic (identical) twins, individuals resulting from divided  embryos, and clones. With these exceptions, it is believed that no two humans have the  same complete DNA sequence.  Individual SNPs may be shared by many individuals, but a suffciently large number of  SNPs that show suffcient variability is generally believed to produce a combination that  is unique to an individual. Thus, there are some sections of the DNA sequence and some  combinations of SNPs that have high variability within the human population and oth- ers that have signifcant conservation between individuals within a specifc population or  group. When there is high variability, DNA sequences and SNPs can be used to match an  individual with a historical sample that has been analyzed and entered into a dataset. The  inheritability of genetic information has also allowed researchers to determine the surnames  and even the complete identities of some individuals [57].  As the number of individuals who have their DNA and SNPs measured increases, scientists  are realizing that the characteristics of DNA and SNPs in individuals may be more com- plicated than the preceding paragraphs imply. DNA changes as individuals age because of  senescence, transcription errors, and mutation. DNA methylation, which can impact the  functioning of DNA, also changes over time [17]. Individuals who are made up of DNA  from multiple individuals – typically the result of the fusion of twins in early pregnancy  – are known as chimera or mosaic. In 2015, a man in the United States failed a paternity  test because the genes in his saliva were different from those in his sperm [68]. A hu- man chimera was identifed in 1953 because the person’s blood contained a mixture of two  blood types: A and O [37]. The incidence of human chimeras is unknown.  Because of the high variability inherent in DNA, complete DNA sequences may be iden- tifable by linking with an external dataset. Likewise, biological samples for which DNA  can be extracted may be identifable. Subsections of an individual’s DNA sequence and  collections of highly variable SNPs may be identifable unless it is known that there are  many individuals who share the region of DNA or those SNPs. Furthermore, genetic infor- mation may not only identify an individual but could also identify an individual’s ancestors,  siblings, and descendants.  50  NIST SP 800-188 3pd  November 2022  Reading Level at Start of School Year # of Students  Below grade level 30-39  At grade level 50-59  Above grade level 20-29  Table 1. Reading levels at a hypothetical school, as measured by entrance examinations,  reported at the start of the school year on October 1.  Reading Level at Start of School Year # of Students  Below grade level 30-39  At grade level 50-59  Above grade level 30-39  Table 2. Reading levels at a hypothetical school, as measured by entrance examinations,  reported one month into the school year on November 1 after a new student has transferred to  the school.  1935  1936  1937  1938  1939  1940  1941  1942  1943  1944  1945  1946  1947  1948  1949  1950  1951  1952  1953  1954  1955  4.3.7. De-Identifying Text Narratives and Qualitative Information  Researchers must devote specifc attention when they de-identify text narratives and other  kinds of qualitative information. Many approaches developed in the 1980s and 1990s that  provided reasonable privacy assurances at the time may no longer provide adequate protec- tion in an era with high-quality internet search and social media [96, 97]. This is an area of  active research.  4.3.8. Challenges Posed by Aggregation Techniques  Aggregation does not necessarily provide privacy protection, especially when data are pre- sented in multiple data releases. Consider a hypothetical example of a school that reports  on its website the number of students performing below, at, and above grade level at the  start of the school year (table 1). Then consider that a new student enrolls at the school on  October 15, and the school updates the table on its website (table 2).  By comparing the two tables, it is possible to infer that the student who joined the school  is likely performing above grade level. This reveals protected information. Moreover, if  a person who views both tables knows the specifc student who enrolled in October, they  have learned a private fact about that student.  Aggregation does not inherently protect privacy, and thus aggregation alone is not suffcient  to provide formal privacy guarantees. However, the differential privacy literature does pro- vide methods for performing aggregation that are both formally private and highly accurate  when applied to large datasets. These methods work through the addition of carefully cali- brated noise.  51  NIST SP 800-188 3pd  November 2022  1956  1957  1958  1959  1960  1961  1962  1963  1964  1965  1966  1967  1968  1969  1970  1971  1972  1973  1974  1975  1976  1977  1978  1979  1980  1981  1982  1983  1984  1985  1986  1987  4.3.9. Challenges Posed by High-Dimensional Data  Even after removing all of the unique identifers and manipulating the quasi-identifers,  data can still be identifying if it is of suffciently high dimensionality and if there exists a  way to link the supposedly non-identifying values to an identity.24  4.3.10. Challenges Posed by Linked Data  Data can be linked in many ways. Pseudonyms allow data records from the same individual  to be linked together over time. Family identifers allow data from parents to be linked with  their children. Device identifers allow data to be linked to physical devices and potentially  link together all data coming from the same device. Data can also be linked to geographical  locations.  Data linkage increases the risk of re-identifcation by providing more attributes that can be  used to distinguish the true identity of a data record from others in the population. For ex- ample, survey responses that are linked together by household are more readily re-identifed  than survey responses that are not linked. Heart rate measurements may not be considered  identifying, but given a long sequence of tests, each individual in a dataset would have a  unique constellation of heart rate measurements, and the dataset could be susceptible to  being linked with another dataset that contains the same values.25 Geographical location  data can – when linked over time – create individual behavioral time-location patterns that  can be used to classify and identify unlabeled data, even with a small number of recorded  locations per individual [82, 81].  Dependencies between records may result in record linkages even when there is no explicit  linkage identifer. For example, it may be that an organization has new employees take a  profciency test within seven days of being hired. This information would allow links to be  drawn between an employee dataset that accurately reported an employee’s start date and a  training dataset that accurately reported the date that the test was administered, even if the  sponsoring organization did not intend for the two datasets to be linkable.  4.3.11. Challenges Posed by Composition  In computer science, the term composition refers to combining multiple functions to create  more complicated ones. One of the defning characteristics of complex systems is that they  have unpredictable behavior, even when they are composed of very simple components. A  challenge of composition is to develop approaches for limiting or eliminating such unpre- dictable behavior. Typically, this is done by proactively limiting the primitives that can be  24For example, consider a dataset of an anonymous survey that links together responses from parents and their  children. In such a dataset, a child might be able to fnd their parents’ confdential responses by searching  for their own responses and then following the link [84].  25This is a different approach than characterizing an individual’s heartbeat pattern so that it can be used as a  biometric. In this case, it is a specifc sequence of heartbeats that is recognized.  52  NIST SP 800-188 3pd  November 2022  1988  1989  1990  1991  1992  1993  1994  1995  1996  1997  1998  1999  2000  2001  2002  2003  2004  2005  2006  2007  2008  2009  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019  2020  2021  2022  composed. De-identifcation is such a primitive that statisticians and data scientists must  carefully control to ensure that the results of de-identifcation efforts can be composed.  Without such controls, the results of composition can become unpredictable.  Specifcally, it is important to understand whether the techniques used for de-identifying  will retain their privacy guarantees when they are subject to composition. For example, if  the same dataset is made available through two different de-identifcation regimes, what  will happen to the privacy guarantees if the two downstream datasets are recombined? One  of the primary advantages of differential privacy is that its operators are composable. This  is not true of most other de-identifcation techniques.  Composition concerns can arise when:  • The same dataset is provided to multiple downstream users.  • Snapshots of a dataset are published on a periodic basis.  • Changes in computer technology result in new aspects of a dataset being made avail- able.  • Legal proceedings require that aspects of the dataset (attributes or a subset of records)  are made available without de-identifcation.  Privacy risk can result from unanticipated composition, which is one of the reasons that  released datasets should be subjected to periodic review and reconsideration.  4.3.12. Potential Failures of De-Identifcation  The de-identifcation process outlined in this section can fail to prevent a disclosure for a  number of different reasons. In addition, failures of data utility can also occur, in which  the de-identifcation process removes too much information, and the de-identifed dataset  is not useful for its intended purpose.  • If an inappropriate risk threshold is selected, then the risk of re-identifcation may  be higher than intended. Agencies should select risk thresholds conservatively to  address this issue.  • If direct or quasi-identifers are missed, then identifying information may remain in  the de-identifed dataset, leading to increased re-identifcation risk. Agencies should  be mindful of the ways in which personal information can be used to identify indi- viduals and – in ambiguous situations – assume that such information is identifying.  • If threats are missed during threat modeling, then the re-identifcation risk could  be higher than intended. In particular, if other datasets that could be linked with  the de-identifed dataset are not considered, then the risk could be much higher than  anticipated. Agencies should carefully consider existing and future data releases  during threat modeling.  53  NIST SP 800-188 3pd  November 2022  2023  2024  2025  2026  2027  2028  2029  2030  2031  2032  2033  2034  2035  2036  2037  2038  2039  2040  2041  2042  2043  2044  2045  2046  2047  2048  2049  2050  2051  2052  2053  2054  2055  2056  2057  2058  • If the selected transformations fail to remove identifying information, then the  risk of de-identifcation could be higher than intended. Agencies should select trans- formations with well-understood properties and a history of successful use.  • If the de-identifed dataset does not produce accurate results for its intended use,  then it may not satisfy the goals of the data release. Future data custodians may be  forced to oversee additional data releases, and those future releases might be com- bined with the already released datasets in ways that are unforeseen. Agencies should  understand how the de-identifed data will be used and make sure to carefully evalu- ate its utility for those purposes before releasing it.  4.3.13. Post-Release Monitoring  Following the release of a de-identifed dataset, the releasing agency should monitor it to  ensure that the assumptions made during the de-identifcation remain valid. This is because  the identifability of a dataset can only increase over time. For example, the de-identifed  dataset may contain information that can be linked to an internal dataset that is later the  subject of a data breach. In such a situation, the data breach could also result in the re- identifcation of the de-identifed dataset. The de-identifed dataset might also be linked  to an external dataset released by a completely separate organization. Agencies have no  control over the release of such datasets, and even monitoring may be challenging in this  situation. In some cases, the de-identifed dataset might be linked with privately held data,  making monitoring impossible.  Agencies may wish to make releasing units responsible for post-release monitoring or to  centralize the post-release monitoring in a single location. However, proper post-release  monitoring requires knowledge of the datasets that have been released and the kinds of data  that would allow for a re-identifcation attack. These requirements are likely to increase  costs to organizations that wish to delegate post-release monitoring to other organizations  or third parties. One way to decrease the requirement for post-release monitoring is to  perform the de-identifcation using a formal privacy model (e.g., differential privacy) that  provides for privacy without making assumptions about background information available  to the data intruder.  4.4. Synthetic Data  An alternative to de-identifying using the technique presented in the previous section is to  use the original dataset to create a synthetic dataset [35, p.8].  Synthetic data can be created by two approaches:  1. Sampling an existing dataset and either adding noise to specifc cells likely to have a  high risk of disclosure or replacing those cells with imputed values. This is known  as a “partially synthetic” dataset (see Table 3).  54  NIST SP 800-188 3pd  November 2022  Data adjective Description  Datasets without formal guarantees:  Partially synthetic Data for which there may be one-to-one mappings between records  in the original dataset and the synthetic dataset but for which some  attributes may have been altered or swapped between records. This  approach is sometimes called blank-and-impute.  Datasets with formal guarantees if the original dataset is not used to create the data:  Test Data that resemble the original dataset in terms of structure and  the range of values but for which there is no attempt to ensure that  inferences drawn on the test data will be like those drawn on the  original data. Test data may also include extreme values that are  not in the original data but are present for testing software.  Realistic Data that have a characteristic that is like the original data but that  is not developed by modifying original data and which contains no  information that is privacy-sensitive.  Datasets with formal guarantees when formal techniques are used:  Fully synthetic Data for which there is no one-to-one mapping between any record  in the original dataset and the synthetic dataset.  Table 3. Adjectives used for describing data in data releases.  2059  2060  2061  2062  2063  2064  2065  2066  2067  2068  2069  2070  2. Using the existing dataset to create a model and then using that model to create a  synthetic dataset. This is known as a “fully synthetic” dataset (see Table 3).  In both cases, formal privacy techniques can be used to quantify the privacy protection  offered by the synthetic dataset.  4.4.1. Partially Synthetic Data  A partially synthetic dataset is one in which some of the data have been altered from the  original dataset using probabilistic models. For example, data that belong to two families  in adjoining towns may be swapped to protect the identity of the families. Alternatively, the  data for an outlier variable may be removed and replaced with a range value that is incorrect  (e.g., replacing the value “60” with the range “30-35”). It is considered best practice for  the data publisher to indicate that some values have been modifed or otherwise imputed  but not to reveal the specifc values that have been modifed.  55  NIST SP 800-188 3pd  November 2022  2071  2072  2073  2074  2075  2076  2077  2078  2079  2080  2081  2082  2083  2084  2085  2086  2087  2088  2089  2090  2091  2092  2093  2094  2095  2096  2097  2098  2099  2100  2101  2102  2103  2104  2105  4.4.2. Test Data  It is also possible to create test data that is syntactically valid but does not convey accu- rate information when analyzed. Such data can be used for software development. When  creating test data, it is useful for the names, addresses, and other information in the data to  be conspicuously non-natural so that the test data are not inadvertently confused with true  confdential data. For example, use the name “FIRSTNAME1 LASTNAME2” rather than  “JOHN SMITH.”  4.4.3. Fully Synthetic Data  A fully synthetic dataset is a dataset for which there is no one-to-one mapping between  data in the original dataset and data in the de-identifed dataset. One approach to creating a  fully synthetic dataset is to use the original dataset to create a high-fdelity model and then  to use a simulation to produce individual data elements that are consistent with the model.  Special efforts must be taken to maintain marginal and joint probabilities when creating  partially or fully synthetic data.  Fully synthetic datasets cannot provide more information to the downstream user than was  contained in the original model. Nevertheless, some users may prefer to work with the fully  synthetic dataset instead of the model for a variety of reasons:  • Synthetic data provides users with the ability to develop queries and other techniques  that can be applied to the real data without exposing real data to users during the  development process. The queries and techniques can then be provided to the data  owner, who can run the queries or techniques on the real data and provide the results  to the users.  • Many hypotheses not represented exactly in the original model may be informed by  the synthetic data because they are correlated with hypotheses (effects) that are in the  model.  • Some users may place more trust in a synthetic dataset than in a model.  • When researchers form their hypotheses from synthetic data and then verify their  fndings on actual data, they can be protected from pretest estimation and false- discovery bias [7, p.257].  Because of the possibility of false discovery, analysts should be able to validate their dis- coveries against the original data to ensure that the things they discover are in the original  data and not artifacts of the data generation process.  Both high-fdelity models and synthetic data generated from models may leak personal  information that is potentially re-identifable. The amount of leakage can be controlled us- ing formal privacy models (e.g., differential privacy) that typically involve the introduction  56  NIST SP 800-188 3pd  November 2022  2106  2107  2108  2109  2110  2111  2112  2113  2114  2115  2116  2117  2118  2119  2120  2121  2122  2123  2124  2125  2126  2127  2128  2129  2130  2131  2132  2133  2134  2135  2136  2137  2138  2139  2140  2141  of noise. Section 4.4.6 describes the construction of fully synthetic data with differential  privacy.  There are several advantages for agencies that choose to release de-identifed data as a fully  synthetic dataset:  • It can be very diffcult or even impossible to map records to actual people.  • The privacy guarantees can potentially be mathematically established and proven (cf.  the section below on “Creating a synthetic dataset with differential privacy”).  • The privacy guarantees can remain in force even if there are future data releases.  Fully synthetic data also have these disadvantages and limitations:  • It is not possible to create pseudonyms that map back to actual people because the  records are fully fabricated.  • The data release may be less useful for accountability or transparency. For example,  investigators equipped with a synthetic data release would be unable to fnd the actual  “people” who make up the release because they would not actually exist.  • It is diffcult to fnd meaningful correlations or abnormalities in synthetic data that  are not represented in the model. For example, if a model contains only main effects  and frst-order interactions, then all second-order interactions can only be estimated  from the synthetic data to the extent that their design is correlated with the main or  frst-order interactions.  • Users of the data may not realize that the data are synthetic. Simply providing doc- umentation that the data are fully synthetic may not be suffcient public notifcation  since the dataset may be separated from the documentation. Instead, it is best to  indicate in the data itself that the values are synthetic. For example, names like  “SYNTHETIC PERSON” or “FIRSTNAME1 LASTNAME1” may be placed in the  data.  • Releasing a synthetic dataset may not be regarded by the public as a legitimate act of  transparency, or the public may question the validity of the data based on its perceived  lack of relationship to the original dataset. These concerns can be addressed with  public education and by documenting the accuracy of the synthetic dataset.  In addition, it can be extremely challenging to construct the high-fdelity models that enable  good synthetic datasets. The best known techniques for constructing these models are  designed around ensuring that specifc properties of the data (e.g., correlations between  certain data attributes) are preserved when the model is constructed. Models constructed  this way may not necessarily refect other properties that were present in the original data.  It is often possible to construct very high-fdelity models when the desirable properties  of the synthetic data are known in advance (e.g., when it is known what questions future  57  NIST SP 800-188 3pd  November 2022  2142  2143  2144  2145  2146  2147  2148  2149  2150  2151  2152  2153  2154  2155  2156  2157  2158  2159  2160  2161  2162  2163  2164  2165  2166  2167  2168  2169  2170  2171  2172  2173  2174  2175  2176  analysts will want to answer using the synthetic data). Constructing synthetic data that  faithfully represents all properties of the original data is much more challenging.  4.4.4. Synthetic Data with Validation  Agencies that share or publish synthetic data can optionally provide a validation service  that takes queries or algorithms developed with synthetic data and applies them to actual  data. The results of these queries or algorithms can then be compared with the results  of running the same queries on the synthetic data, and the researchers can be warned if  the results are different. Alternatively, results can be provided to the researchers after the  application of additional statistical disclosure limitation.  4.4.5. Synthetic Data and Open Data Policy  Releases of synthetic data can be confusing to the lay public.  • It may not be clear to data users that the synthetic data release is actually synthetic.  Members of the public may assume instead that the synthetic data are simply an  operational dataset that has had identifying columns suppressed.  • Synthetic data may contain synthetic individuals who appear similar to actual indi- viduals in the population.  • Fully synthetic datasets do not have a zero-disclosure risk because they still contain  information derived from non-public information about individuals. The disclosure  risk may be greater when synthetic data are created with traditional statistical mod- eling or data imputing techniques rather than those based on formal privacy models,  such as differential privacy, as the formal models have provisions for tracking the  accumulated privacy loss that results from multiple data operations, as discussed in  Section 4.4.6.  4.4.6. Creating a Synthetic Dataset with Diferential Privacy  A growing number of mathematical algorithms have been developed for creating syn- thetic datasets that meet the mathematical defnition of privacy provided by differential  privacy [40]. Most of these algorithms will transform a dataset containing private data into  a new dataset that contains synthetic data that nevertheless provides reasonably accurate  results in response to a variety of queries. However, there is no algorithm or implemen- tation currently in existence that can be used by a person who is unskilled in the area of  differential privacy.  The idea of differential privacy is that the result of a data analysis function κ applied to a  dataset should not change very much if an arbitrary person p’s data is added to or removed  from a dataset D. That is, κ(D) ≈ κ(D − p). The degree to which the two values are  approximately equal is determined by the privacy loss parameter ε .  58  NIST SP 800-188 3pd  November 2022  2177  2178  2179  2180  2181  2182  2183  2184  2185  2186  2187  2188  2189  2190  2191  2192  2193  2194  2195  2196  2197  2198  2199  2200  2201  2202  2203  2204  2205  2206  2207  2208  2209  2210  2211  2212  2213  2214  In the mathematical formulation of differential privacy, ε can range from 0 to ∞. When  ε = 0, the output of κ does not depend on the input dataset. When ε = ∞, the output of κ  is entirely dependent upon the input dataset, such that changing a single record results in  an unambigous measurable change in κ’s output. Thus, larger values of ε provide for more  accuracy but result in increased privacy loss.  When ε is set appropriately, differential privacy limits the privacy loss that a data subject  experiences from the use of their private data to the maximum privacy loss necessary for  a given statistical purpose. Note that this particular notion of privacy does not protect all  secrets about a person. It only protects the secrets that an observer would not have been  able to learn if the person’s data was not present in the dataset. Stated another way, differ- ential privacy protects individuals from additional harm resulting from their participation  in the data analysis but does not protect them from harm that would have occurred even  if their data were not present. For example, if a study concludes that residents of Ver- mont overwhelmingly drive 4-wheel-drive vehicles, one might conclude that a particular  Vermonter drives a 4-wheel-drive vehicle even if that individual did not participate in the  study. Differential privacy does not attempt to prevent inferences of this type.  Many academic papers on differential privacy assume a value of 1.0 for ε but do not explain  the rationale of the choice. Some researchers working in the feld of differential privacy  have tried mapping existing privacy regulations to the choice of ε , but these efforts invari- ably result in values of ε = 1. Principled approaches for setting ε is a subject of current  academic research [72].  There are relatively few scholarly publications regarding the deployment of differential pri- vacy in real-world situations, and there are few papers that provide guidance in choosing  appropriate values of ε . Thus, agencies that are interested in using differential privacy al- gorithms to allow for querying of sensitive datasets or the creation of synthetic data should  ensure that the techniques are appropriately implemented and that the privacy protections  are appropriate to the desired application.  4.5. De-Identifying with an Interactive Query Interface  Another model for granting public access to de-identifed agency information is to construct  an interactive query interface that allows members of the public or qualifed investigators to  run queries over the agency’s dataset. This option has been developed by several agencies,  and there are many ways that it can be implemented. For example:  • If the queries are run on actual data, the results can be altered through the injection  of noise to protect privacy, potentially satisfying a formal privacy model such as  differential privacy. Alternatively, individual queries can be reviewed by agency staff  to verify that privacy thresholds are maintained.  • Queries can be run on synthetic data. In this case, the agency can also run queries  on the actual data and warn the external researchers if the queries run on synthetic  ̸  59  NIST SP 800-188 3pd  November 2022  2215  2216  2217  2218  2219  2220  2221  2222  2223  2224  2225  2226  2227  2228  2229  2230  2231  2232  2233  2234  2235  2236  2237  2238  2239  2240  2241  2242  2243  2244  2245  data deviate signifcantly from the queries run on the actual data (ensuring that the  warning itself does not compromise the privacy of some individual).  • Query interfaces can be made freely available on the public internet, or they can be  made available in a restricted manner to qualifed researchers operating in secure  locations.  A signifcant privacy risk with interactive queries is that each query results in additional  privacy loss [33].26 For this reason, query interfaces should also log both queries and  query results in order to deter and detect malicious use.  One of the advantages of synthetic data is that the privacy loss budget can be spent on  creating the synthetic dataset rather than on responding to interactive queries.  4.6. Validating a De-Identifed Dataset  Agencies should validate datasets after they are de-identifed to ensure that the resulting  dataset meets the agency’s goals in terms of both data usefulness and privacy protection.  4.6.1. Validating Data Usefulness  De-identifcation decreases data accuracy and the usefulness of the resulting dataset. It is  therefore important to ensure that the de-identifed dataset is still useful for the intended  application. Otherwise, there is no reason to go through the expense and added risk of  de-identifcation.  Several approaches exist for validating data usefulness. For example, insiders can perform  statistical calculations on both the original dataset and the de-identifed dataset and compare  the results to see if the de-identifcation resulted in unacceptable changes. Agencies can  engage trusted outsiders to examine the de-identifed dataset and determine whether the  data could be used for the intended purpose.  Recognizing that there is an inherent trade-off between data accuracy and privacy protec- tion, agencies can adopt accuracy goals for the data that they make available to a broad  audience. An accuracy goal specifes how accurate data must be in order to be ft for an  intended use. Limiting data accuracy to this goal is an important technique for protecting  the privacy of data subjects.  4.6.2. Validating Privacy Protection  Several approaches exist for validating the privacy protection provided by de-identifcation,  including:  26If a fnite privacy loss budget is allocated, the data controller needs to respond by increasing the amount of  noise added to each response, accepting a higher level of privacy risk, or ceasing to answer questions as the  budget nears exhaustion. This can result in equity issues if the frst users to query the dataset obtain better  answers than later users.  60  NIST SP 800-188 3pd  November 2022  2246  2247  2248  2249  2250  2251  2252  2253  2254  2255  2256  2257  2258  2259  2260  2261  2262  2263  2264  2265  2266  2267  2268  2269  2270  2271  2272  2273  2274  • Examining the resulting data fles to make sure that no identifying information is  unintentionally included in fle data or metadata.  • Examining the resulting data fles to make sure that the data meet stated goals for  ambiguity under a k-anonymity model, if such a standard is desired.  • Critically evaluating all default assumptions used by software that performs data  modifcation or modeling.  • Conducting a motivated intruder test to see if reasonably competent outside indi- viduals can perform re-identifcation using publicly available datasets, commercially  available datasets, or even private datasets that might be available to certain data  intruders. Motivations for an intruder can include prurient interest, causing embar- rassment or harm, revealing private facts about public fgures, or engaging in a rep- utation attack. Details for how to conduct a motivated intruder test can be found in  Anonymisation: Managing data protection risk code of practice, published by the  United Kingdom’s Information Commissioner’s Offce [63].  • Providing the team conducting the motivated intruder test with some confdential  agency data to understand how a data intruder might be able to take advantage of  data leaked as a result of a breach or a hostile insider.  These approaches do not provide provable guarantees on the protection offered by de- identifcation, but they may be useful as part of an overall agency risk assessment.27 Ap- plications that require provable privacy guarantees should rely on formal privacy methods,  such as differential privacy, when planning their data releases.  Validating the privacy protection of de-identifed data is greatly simplifed by using vali- dated de-identifcation software, as discussed in Section 5, “Evaluation.”  4.6.3. Re-Identifcation Studies  Re-identifcation studies are motivated intruder tests. These studies can identify issues that  would allow external actors to successfully re-identify de-identifed data. Re-identifcation  studies look for vulnerabilities in a dataset that could be used for re-identifying data sub- jects. They do not determine whether someone with intimate knowledge of a specifc re- spondent can fnd that respondent in the database. The only way to protect a single specifc  27Although other documents that discuss de-identifcation use the term risk assessment to refer to a specifc  calculation of ambiguity using the k-anonymity de-identifcation model, this document uses the term risk  assessment to refer to a much broader process. Specifcally, risk assessment is defned as, “The process  of identifying, estimating, and prioritizing risks to organizational operations (including mission, functions,  image, reputation), organizational assets, individuals, other organizations, and the Nation, resulting from  the operation of an information system. Part of risk management incorporates threat and vulnerability  analyses and considers mitigations provided by security controls planned or in place. Synonymous with  risk analysis” [23].  61  NIST SP 800-188 3pd  November 2022  2275  2276  2277  2278  2279  2280  2281  2282  2283  2284  2285  2286  2287  2288  2289  2290  2291  2292  2293  2294  2295  2296  2297  2298  2299  2300  2301  2302  2303  2304  2305  2306  2307  2308  2309  2310  2311  2312  individual perceived to be at high risk of re-identifcation is through data perturbation (e.g.,  noise injection) or information reduction (e.g., removing the observation altogether).  The key statistic calculated in re-identifcation studies is the conditional re-identifcation  rate. This statistic is a proxy for disclosure risk. The rate is the number of confrmed links  between the dataset and another dataset divided by the number of putative (suspected)  links, unduplicated by “defender” ID, expressed as a percentage. If the conditional re- identifcation rate falls above an agreed upon threshold for any publication strata, it suggests  that the data should not be released outside of a controlled environment.  Re-identifcation studies are often an iterative process. If a re-identifcation study uncovers  problems with the de-identifed data, the data curator can engage with subject matter ex- perts, make changes to the dataset, and perform another re-identifcation study. Changes to  the dataset might involve coarsening linking variables, eliminating highly disclosive link- ing variables from the microdata to be released, or coarsening strata. This continues until  the study concludes that the de-identifed data can be disseminated.  There are two very different types of re-identifcation studies:  1. Micro (or targeted) re-identifcation studies, where one is looking for a specifc  person. A well-known example is that of former Governor William Weld of Mas- sachusetts, whose medical records in a hospital discharge summary were record  linked to voter records [16]. As noted earlier, individual targets are supremely hard  to protect as there is often extensive publicly available information about them.  2. Macro (or wholesale) re-identifcation studies, where one seeks to embarrass or  discredit the organization releasing the data. This is accomplished by linking easily  procurable external intruder data to the protected microdata that are being released.  Several metrics can be calculated to uncover putative links, and several methods can  be used to confrm putative links. Python has record linkage objects that probabilis- tically link fles using a wide variety of metrics.  Formal privacy parameters often appear opaque and elusive to non-theoreticians. Subject  matter experts and decision-makers more clearly understand disclosure risk after reviewing  the results of re-identifcation studies.  External intruders may calculate low or high suspected re-identifcation rates, given the  information they have available to them. They may even purport to have successfully linked  their external data to a de-identifed dataset. By conducting a re-identifcation study a  priori, those seeking to disseminate the de-identifed data know how successful the external  intruder’s re-identifcation attempt was if both parties have access to the same external  internal data.  The conditional re-identifcation rate is identical to the metric of precision in the record  linkage and health science literature. It represents the ratio of true positives to the sum  of true positives and false positives. Data owners should not be alarmed if an external  62  NIST SP 800-188 3pd  November 2022  2313  2314  2315  2316  2317  2318  2319  2320  2321  2322  2323  2324  2325  2326  2327  2328  2329  2330  2331  2332  2333  2334  2335  2336  2337  2338  2339  2340  2341  2342  2343  2344  2345  2346  2347  2348  organization reports a relatively high suspected re-identifcation rate as long as they know  that the conditional re-identifcation rate is low [45, 59, 111].  Confrmed re-identifcation rates are defned in Section 3.2.1 as re-identifcation probabil- ities. On its own, a low confrmed re-identifcation probability does not indicate that an or- ganization should disseminate a de-identifed dataset. Even when a confrmed rate is low,  a high conditional rate should direct an organization to not disseminate the de-identifed  microdata.  Re-identifcation studies may identify problems that can direct improvements to any or- ganization’s disclosure avoidance methods. Re-identifcation studies are not designed to  replace legacy or modern provable privacy methods but to act as a quality control to vali- date that the methods – old and new – protect as they were designed.  5. Software Requirements, Evaluation, and Validation  Agencies should clearly defne the requirements for de-identifcation algorithms and the  software that implements those algorithms. They should be sure that the algorithms that  they intend to use are validated, that the software implements the algorithms as expected,  and that the data that result from the operation of the software are correct.  Today, there a growing number of algorithms and tools for performing de-identifcation,  data masking, and performing other privacy-preserving operations. NIST maintains a list of  some of these tools at https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/  collaboration-space/focus-areas/de-id/tools. Such tools are also increasingly being eval- uated in academic literature [116] and by NIST [107, 1], although there are no widely  accepted performance standards or certifcation procedures at present.  5.1. Evaluating Privacy-Preserving Techniques  There have been decades of research in the feld of statistical disclosure limitation and  de-identifcation, and understanding in the feld has evolved over time. Agencies should  not base their technical evaluation of a technique solely on the fact that the technique has  been published in peer-reviewed literature or that the agency has a long history of using  the technique and has not experienced any problems. Instead, it is necessary to evaluate  proposed techniques through the totality of scientifc experience and with regard to current  threats.  Traditional statistical disclosure limitation and de-identifcation techniques base their risk  assessments – in part – on an expectation of what kinds of data are available to a data  intruder to conduct a linkage attack. Where possible, these assumptions should be docu- mented and published along with a description of the privacy-preserving techniques that  were used to transform the datasets prior to release so that they can be reviewed by external  experts and the scientifc community.  63  https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/tools https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/tools https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/tools  NIST SP 800-188 3pd  November 2022  2349  2350  2351  2352  2353  2354  2355  2356  2357  2358  2359  2360  2361  2362  2363  2364  2365  2366  2367  2368  2369  2370  2371  2372  2373  2374  2375  2376  2377  2378  2379  2380  2381  Because our understanding of privacy technology and the capabilities of privacy attacks  are rapidly evolving, techniques that have been previously established should be periodi- cally reviewed. New vulnerabilities may be discovered in techniques that have been pre- viously accepted. Alternatively, new techniques may be developed that allow agencies to  re-evaluate the trade-offs they have made with respect to privacy risk and data usability.  5.2. De-Identifcation Tools  A de-identifcation tool is a program that is involved in the creation of de-identifed datasets.  5.2.1. De-Identifcation Tool Features  De-identifcation tools may perform many functions, including:  • Detecting identifying information  • Calculating re-identifcation risk  • Performing de-identifcation  • Mapping identifers to pseudonyms  • Providing for the selective revelation of pseudonyms  De-identifcation tools may handle a variety of data modalities. For example, tools may  be designed for tabular data or for multimedia. Tools may attempt to de-identify all data  types or be developed for specifc modalities. A potential risk of using de-identifcation  tools is that a tool could be equipped to handle some but not all of the different modalities  in a dataset. For example, a tool could de-identify the categorical information in a table  according to a de-identifcation standard but might not detect or attempt to address the  presence of identifying information in a text feld. For this reason, de-identifcation tools  should be validated for the specifc kinds of data that the agency intends to use.  5.2.2. Data Provenance and File Formats  Output fles created by de-identifcation tools and data masking tools can record provenance  information, such as metadata regarding input datasets, the de-identifcation methods used,  and the resulting decrease in data accuracy. Output fles can also be explicitly marked to in- dicate that they have been de-identifed. For example, de-identifcation profles that are part  of the Digital Imaging and Communications in Medicine (DICOM) specifcation indicate  which elements are direct versus quasi-identifers and which de-identifcation algorithms  have been employed [32, Appendix E, “Attribute Confdentiality Profles”].  5.2.3. Data Masking Tools  Data masking tools are programs that can remove or replace designated felds in a dataset  while maintaining relationships between tables. These tools can be used to remove direct  64  NIST SP 800-188 3pd  November 2022  2382  2383  2384  2385  2386  2387  2388  2389  2390  2391  2392  2393  2394  2395  2396  2397  2398  2399  2400  2401  2402  2403  2404  2405  2406  2407  2408  2409  2410  2411  2412  2413  2414  2415  identifers but generally cannot identify or modify quasi-identifers in a manner consistent  with a privacy policy or risk analysis.  Data masking tools were developed to allow software developers and testers access to  datasets that contain realistic data while providing minimal privacy protection. Absent  additional controls or data manipulations, data masking tools should not be used for the  de-identifcation of datasets that are intended for public release nor as the sole mechanism  to ensure confdentiality in non-public data sharing.  5.3. Evaluating De-Identifcation Software  Once techniques are evaluated and approved, agencies should ensure that the techniques are  faithfully executed by their chosen software. Privacy software evaluation should consider  the trade-off between data usability and privacy protection. Privacy software evaluation  should also seek to detect and minimize the chances of tool error and user error.  For example, agencies should verify:  • Correctness. The software properly implements the chosen algorithms.  • Containment. The software does not leak identifying information in expected or  unexpected ways, such as through the inaccuracies of foating-point arithmetic or the  differences in execution time (if observable to a data intruder).  • Usability. The software can be operated effciently and with minimal error, and users  can detect and correct errors when they happen.  Agencies should also evaluate the performance of the de-identifcation software, such as:  • Effciency. How long does it take to run on a dataset of a typical size?  • Scalability. How much does it slow down when moving from a dataset of N to 100N?  • Repeatability. If the tool is run twice on the same dataset, are the results similar? If  two different people run the tool, do they get similar results?  Ideally, software should be able to track the accumulated privacy leakage from multiple  data releases.  5.4. Evaluating Data Accuracy  Finally, agencies should evaluate the accuracy of the de-identifed data to verify that it is  suffcient for the intended use. For example, researchers at MIT and Harvard applied k- anonymity de-identifcation to educational data collected by a massive open online course  operated by MITx and HarvardX on the edX platform and found that de-identifcation  resulted in meaningful biases that changed the meaning of some statistics. For example, in  one case, de-identifcation decreased the number of enrolled female students from 29% to  26% because of the need to suppress attributes for specifc microdata [30].  65  NIST SP 800-188 3pd  November 2022  2416  2417  2418  2419  2420  2421  2422  2423  2424  2425  2426  2427  2428  2429  2430  2431  2432  2433  2434  2435  2436  2437  2438  2439  2440  2441  2442  2443  2444  2445  2446  2447  2448  2449  2450  The feld of statistical disclosure control has developed approaches for gauging the impact  of SDC techniques on microdata [142]. The literature examines the mathematical impact  of SDC procedures (e.g., sampling, recoding, suppression, rounding, and noise infusion)  and computes the possible impact on various statistical measurements.  Approaches for evaluating data accuracy include [71]:  • Demonstrating that machine learning algorithms trained on the de-identifed data can  accurately predict the original data and vice versa  • Verifying that statistical distributions do not incur undue bias because of the de- identifcation procedure  • Publishing suffcient information about the statistical properties of the disclosure lim- itation methods to permit the correction of inferences using these properties  Agencies can create or adopt standards regarding the accuracy of de-identifed data. If  data accuracy cannot be well-maintained along with data privacy goals, then the release of  data that is inaccurate for statistical analyses could potentially result in incorrect scientifc  conclusions and incorrect policy decisions.  6. Conclusion  Government agencies can use de-identifcation technology to make datasets available to  researchers and the public without compromising the privacy of the people contained within  the data.  There are currently three primary models available for de-identifcation:  1. agencies can make data available with traditional de-identifcation techniques that  rely on the suppression of identifying information (direct identifers) and the manip- ulation of information that partially identifes (quasi-identifers);  2. agencies can create synthetic datasets; and  3. agencies can make data available through a query interface.  These models can be mixed within a single dataset to provide different kinds of access for  different users or intended uses.  Privacy protection can be strengthened when agencies employ formal models for privacy  protection, such as differential privacy, because the mathematical models that these sys- tems use are designed to ensure privacy protection irrespective of future data releases or  developments in re-identifcation technology. However, the mathematics underlying these  systems is very new, and there is little experience within the Government in using these  systems. Thus, agencies should understand the implications of these systems before de- ploying them in place of traditional de-identifcation approaches that do not offer formal  privacy guarantees.  66  NIST SP 800-188 3pd  November 2022  2451  2452  2453  2454  2455  2456  2457  2458  2459  Agencies that use de-identifcation should establish appropriate governance structures to  support de-identifcation, data release, and post-release monitoring. Such structures will  typically include a Disclosure Review Board as well as appropriate education, training,  and research efforts.  A summary of this document’s advice for practitioners appears in Figure 5.  In closing, it is important to remember that different jurisdictions may have different stan- dards and policies regarding the defnition and use of de-identifed data. Information that  is considered de-identifed in one jurisdiction may be regarded as being identifable in an- other.  67  NIST SP 800-188 3pd  November 2022  Governance and Management (Section 3) The management of de-identifcation in- cludes identifying the goals of the de-identifcation process and considering risks  to participants in the data release. To guide this process, this document describes  several tools:  • Consider all phases of the Data Life Cycle (Section 3.3).  • Consider different Data Sharing Models (Section 3.4), including complemen-  tary protections like Data Use Agreements, Synthetic Data, and Enclaves.  • Leverage the Five Safes (Section 3.5), a methodology for evaluating risk.  • Form a Disclosure Review Board (Section 3.6) to oversee the implementation  of de-identifcation policies.  • Follow existing de-identifcation standards when possible (Section 3.7).  Technical Steps (Section 4) The technical process of de-identifcation should leverage the  best practices developed over the past several decades. In particular, NIST recom- mends that agencies:  • Conduct a Data Survey (Section 4.2) to identify de-identifcation requirements  specifc to the data.  • Determine identifers and quasi-identifers in the data, and select a method for  de-identifying each one (Section 4.3).  • Consider the existing auxiliary data (Section 4.3) that could be used to enable  a re-identifcation attack.  • Practice defense in depth by combining security measures with de- identifcation when possible, and consider using Synthetic Data (Section 4.4)  or an Interactive Query Interface (Section 4.5).  • When possible, use formal privacy techniques to quantify privacy loss asso- ciated with the release of de-identifed data (Section 4.4.6).  • Validate the utility and privacy of the de-identifed data (Section 4.6). In par- ticular, establish accuracy goals for de-identifcation data so that the data is not  more accurate than required for the intended purpose.  Software (Section 5) In general, agencies should:  • Utilize automated, repeatable, software-based approaches for performing de-  identifcation.  • Carefully consider the software used to implement de-identifcation to ensure  that the algorithms used have been validated and that the software correctly  implements those algorithms.  • Consider the effciency, scalability, and repeatability properties of software  tools, and evaluate the accuracy of the tool’s output.  Fig. 5. Advice for Practitioners: A Summary  68  NIST SP 800-188 3pd  November 2022  2460  2461  2462  2463  2464  2465  2466  2467  2468  2469  2470  2471  2472  2473  2474  2475  2476  2477  2478  2479  2480  2481  2482  2483  2484  2485  2486  2487  2488  2489  2490  2491  2492  2493  2494  2495  References  [1] July 2020. URL: https://www.nist.gov/blogs/cybersecurity- insights/differential- privacy-privacy-preserving-data-analysis-introduction-our.  [2] 115th Congress (2017–2018). Public Law 115-435: The Foundations for Evidence- Based Policymaking Act of 2018. 2018. URL: https://www.congress.gov/bill/115th- congress/house-bill/4174.  [3] 45 CFR 164 Health Insurance Portability and Accountability Act of 1996 (HIPAA)  Privacy Rule Safe Harbor method Standard: De-identifcationof protected health  information.  [4] 81 FR 49689: Revision of OMB Circular No. A-130, “Managing Information as a  Strategic Resource. July 2016. URL: https://www.cio.gov/policies-and-priorities/  circular-a-130/.  [5] 95th Congress. Public Law 95-416. Oct. 1978. URL: https : / / www. census . gov /  history/pdf/NARA Legislation.pdf.  [6] John Abowd et al. “The 2020 Census Disclosure Avoidance System TopDown Al- gorithm”. In: Harvard Data Science Review Special Issue 2 (June 2022). URL:  https://hdsr.mitpress.mit.edu/pub/7evz361i.  [7] John M. Abowd and Ian M. Schmutte. Economic Analysis and Statistical Dis- closure Limitation. Mar. 2015. URL: https: / /www.brookings.edu/bpea- articles /  economic-analysis-and-statistical-disclosure-limitation/.  [8] John M. Abowd and Lars Vilhuber. “How Protective are Synthetic Data?” In: Lec- ture Notes in Computer Science: Privacy in Statistical Databases 5262 (2008),  pp. 239–246.  [9] “Accuracy”. In: Glossary of Statistical Terms (Sept. 2001). Last accessed June 23,  2022. URL: https://stats.oecd.org/glossary/detail.asp?ID=21.  [10] Charu C. Aggarwal. “On K-Anonymity and the Curse of Dimensionality”. In: Pro- ceedings of the 31st International Conference on Very Large Data Bases. VLDB  ’05. Trondheim, Norway: VLDB Endowment, 2005, pp. 901–909. ISBN: 1595931546.  [11] J. Trent Alexander, Michael Davern, and Betsey Stevenson. “Inaccurate age and  sex data in the census PUMS fles: Evidence and implications”. In: Public Opinion  Quarterly 74 (3 2010), pp. 551–569. URL: https://doi.org/10.1093/poq/nfq033.  [12] Micah Altman et al. “Towards a Modern Approach to Privacy-Aware Government  Data Releases”. In: Berkeley Technology Law Journal 30 (3), pp. 1967–2072. URL:  http://papers.ssrn.com/sol3/papers.cfm?abstract id=2779266.  [13] AMD. AMD Secure Encrypted Virtualization (SEV). Last accessed July 13, 2022.  2022. URL: https://developer.amd.com/sev/.  69  https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our https://www.congress.gov/bill/115th-congress/house-bill/4174 https://www.congress.gov/bill/115th-congress/house-bill/4174 https://www.congress.gov/bill/115th-congress/house-bill/4174 https://www.cio.gov/policies-and-priorities/circular-a-130/ https://www.cio.gov/policies-and-priorities/circular-a-130/ https://www.cio.gov/policies-and-priorities/circular-a-130/ https://www.census.gov/history/pdf/NARA_Legislation.pdf https://www.census.gov/history/pdf/NARA_Legislation.pdf https://www.census.gov/history/pdf/NARA_Legislation.pdf https://hdsr.mitpress.mit.edu/pub/7evz361i https://www.brookings.edu/bpea-articles/economic-analysis-and-statistical-disclosure-limitation/ https://www.brookings.edu/bpea-articles/economic-analysis-and-statistical-disclosure-limitation/ https://www.brookings.edu/bpea-articles/economic-analysis-and-statistical-disclosure-limitation/ https://stats.oecd.org/glossary/detail.asp?ID=21 https://doi.org/10.1093/poq/nfq033 http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2779266 https://developer.amd.com/sev/  NIST SP 800-188 3pd  November 2022  2496  2497  2498  2499  2500  2501  2502  2503  2504  2505  2506  2507  2508  2509  2510  2511  2512  2513  2514  2515  2516  2517  2518  2519  2520  2521  2522  2523  2524  2525  2526  2527  2528  2529  2530  2531  2532  2533  [14] Olivia Angiuli, Joe Blitzstein, and Jim Waldo. “How to De-Identify Your Data”.  In: Commun. ACM 58.12 (Nov. 2015), pp. 48–55. ISSN: 0001-0782. DOI: 10.1145/  2814340. URL: https://doi.org/10.1145/2814340.  [15] ASTM International. ASTM E1869-04 (Reapproved 2014) Standard Guide for Con- fdentiality, Privacy, Access, and Data Security Principles for Health Information  Including Electronic Health Records. 2014.  [16] Daniel Barth-Jones. The ‘Re-Identifcation’ of Governor William Weld’s Medical  Information: A Critical Re-Examination of Health Data Identifcation Risks and  Privacy Protections, Then and Now. July 2012. URL: https://ssrn.com/abstract=  2076397%20or%20http://dx.doi.org/10.2139/ssrn.2076397.  [17] Hans T Bjornsson et al. “Intra-individual Change Over Time in DNA Methylation  with Familial Clustering”. In: JAMA 299 (24 June 2008), pp. 2877–2833. URL:  https://pubmed.ncbi.nlm.nih.gov/18577732/.  [18] Sylvia M. Burwell. Open Data Policy-Managing Information as an Asset. May  2013. URL: https : / / obamawhitehouse . archives . gov / sites / default / fles / omb /  memoranda/2013/m-13-13.pdf.  [19] George Bush. Executive Order 13402:Strengthening Federal Efforts to Protect Against  Identity Theft. May 2006. URL: https:/ /www.gpo.gov/fdsys/pkg/FR- 2006- 05- 15/pdf/06-4552.pdf.  [20] G. Camarillo, C. Holmberg, and Y. Gao. Re-INVITE and Target-Refresh Request  Handling in the Session Initiation Protocol (SIP). RFC 6141 (Proposed Standard).  Internet Engineering Task Force, Mar. 2011. URL: www.ietf.org/rfc/rfc6141.txt.  [21] David Carrell et al. “Hiding in plain sight: Use of realistic surrogates to reduce  exposure of protected health information in clinical text”. In: Journal of the Ameri- can Medical Informatics Association : JAMIA 20 (2 July 2012), pp. 342–348. DOI:  10.1136/amiajnl-2012-001034.  [22] Ann Cavoukian. Privacy by Design: The 7 Foundational Principles. Ontario, CA,  Jan. 2011. URL: https://www.ipc.on.ca/wp-content/uploads/Resources/7foundationalprinciples.  pdf.  [23] Jennifer Cawthra et al. Securing Telehealth Remote Patient Monitoring Ecosystem.  2022. DOI: 10.6028/NIST.SP.1800-30. URL: https://nvlpubs.nist.gov/nistpubs/  SpecialPublications/NIST.SP.1800-30.pdf.  [24] Census Bureau Data Stewardship Program. DS025: Organization of the Disclosure  Review Board. Dec. 2019. URL: https://www2.census.gov/foia/ds policies/ds025.  pdf.  [25] Malcolm Chisholm. “7 Phases of a Data Life Cycle”. In: Information Manage- ment (July 2015). URL: http: / /www.information- management .com/news/data- management/Data-Life-Cycle-Defned-10027232-1.html.  70  https://doi.org/10.1145/2814340 https://doi.org/10.1145/2814340 https://doi.org/10.1145/2814340 https://doi.org/10.1145/2814340 https://ssrn.com/abstract=2076397%20or%20http://dx.doi.org/10.2139/ssrn.2076397 https://ssrn.com/abstract=2076397%20or%20http://dx.doi.org/10.2139/ssrn.2076397 https://ssrn.com/abstract=2076397%20or%20http://dx.doi.org/10.2139/ssrn.2076397 https://pubmed.ncbi.nlm.nih.gov/18577732/ https://obamawhitehouse.archives.gov/sites/default/files/omb/memoranda/2013/m-13-13.pdf https://obamawhitehouse.archives.gov/sites/default/files/omb/memoranda/2013/m-13-13.pdf https://obamawhitehouse.archives.gov/sites/default/files/omb/memoranda/2013/m-13-13.pdf https://www.gpo.gov/fdsys/pkg/FR-2006-05-15/pdf/06-4552.pdf https://www.gpo.gov/fdsys/pkg/FR-2006-05-15/pdf/06-4552.pdf https://www.gpo.gov/fdsys/pkg/FR-2006-05-15/pdf/06-4552.pdf www.ietf.org/rfc/rfc6141.txt https://doi.org/10.1136/amiajnl-2012-001034 https://www.ipc.on.ca/wp-content/uploads/Resources/7foundationalprinciples.pdf https://www.ipc.on.ca/wp-content/uploads/Resources/7foundationalprinciples.pdf https://www.ipc.on.ca/wp-content/uploads/Resources/7foundationalprinciples.pdf https://doi.org/10.6028/NIST.SP.1800-30 https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1800-30.pdf https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1800-30.pdf https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1800-30.pdf https://www2.census.gov/foia/ds_policies/ds025.pdf https://www2.census.gov/foia/ds_policies/ds025.pdf https://www2.census.gov/foia/ds_policies/ds025.pdf http://www.information-management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html http://www.information-management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html http://www.information-management.com/news/data-management/Data-Life-Cycle-Defined-10027232-1.html  NIST SP 800-188 3pd  November 2022  2534  2535  [26] A. Church. “A Note on the ‘Entscheidungsproblem’”. In: Journal of Symbolic  Logic 1 (1936), pp. 40–41.  2536  2537  2538  [27] Commission on Evidence-Based Policymaking. The Promise of Evidence-Based  Policymaking. Sept. 2017. URL: https://www.acf.hhs.gov/opre/project/commission- evidence-based-policymaking-cep.  2539  2540  [28] Tore Dalenius. “Finding a Needle in a Haystack, or Identifying Anonymous Census  Records”. In: Journal of Offcial Statistics 2 (3 1986), pp. 329–336.  2541  2542  [29] Tore Dalenius. “Towards a methodology for statistical disclosure control”. In: Statis- tik Tidskrift 15 (1977), pp. 429–444.  2543  2544  [30] Jon P. Daries et al. “Privacy, Anonymity, and Big Data in the Social Sciences”. In:  Communications of the ACM 57 (6 Sept. 2014), pp. 56–63.  2545  2546  [31] Tanvi Desai, Felix Ritchie, and Richard Welpton. Economics Working Paper Series  1601. 2016. URL: http://dx.doi.org/10.13140/RG.2.1.3661.1604.  2547  2548  2549  [32] DICOM Standards Committee. DICOM PS3.15 2016e — Security and System Man- agement Profles. 2016. URL: http : / / dicom . nema . org / medical / dicom / current /  output/html/part15.html#chapter E.  2550  2551  2552  2553  2554  [33] Irit Dinur and Kobbi Nissim. “Revealing Information While Preserving Privacy”.  In: Proceedings of the Twenty-second ACM SIGMOD-SIGACT-SIGART Sympo- sium on Principles of Database Systems. PODS ’03. San Diego, California: ACM,  2003, pp. 202–210. ISBN: 1-58113-670-6. DOI: 10 .1145 /773153 .773173. URL:  doi.acm.org/10.1145/773153.773173.  2555  2556  2557  2558  2559  [34] Changyu Dong, Liqun Chen, and Zikai Wen. “When Private Set Intersection Meets  Big Data: An Effcient and Scalable Protocol”. In: Proceedings of the 2013 ACM  SIGSAC Conference on Computer and Communications Security. CCS ’13. Berlin,  Germany: Association for Computing Machinery, 2013, pp. 789–800. ISBN: 9781450324779.  DOI: 10.1145/2508859.2516701. URL: https://doi.org/10.1145/2508859.2516701.  2560  2561  2562  2563  [35] Jörg Drechsler, Stefan Bender, and Susanne Rässler. Comparing fully and partially  synthetic datasets for statistical disclosure control in the German IAB Establish- ment Panel (Working paper 11). New York, 2007. URL: http : / / fdz . iab.de /342 /  section.aspx/Publikation/k080530j05.  2564  2565  [36] George T. Duncan, Mark Elliot, and Juan-José Salazar-Gonzalez. Statistical Conf- dentiality: Principles and Practice. Springer, 2011, p. 113.  2566  2567  [37] I. Dunsford et al. “A human blood-group chimera”. In: British Medical Journal 81  (July 1953). URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2028470/.  2568  2569  2570  [38] Cynthia Dwork and Aaron Roth. “The Algorithmic Foundations of Differential Pri- vacy”. In: Foundations and Trends in Theoretical Computer Science. Vol. 9. 3–4.  NOW, 2014, pp. 211–407.  71  https://www.acf.hhs.gov/opre/project/commission-evidence-based-policymaking-cep https://www.acf.hhs.gov/opre/project/commission-evidence-based-policymaking-cep https://www.acf.hhs.gov/opre/project/commission-evidence-based-policymaking-cep http://dx.doi.org/10.13140/RG.2.1.3661.1604 http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E https://doi.org/10.1145/773153.773173 doi.acm.org/10.1145/773153.773173 https://doi.org/10.1145/2508859.2516701 https://doi.org/10.1145/2508859.2516701 http://fdz.iab.de/342/section.aspx/Publikation/k080530j05 http://fdz.iab.de/342/section.aspx/Publikation/k080530j05 http://fdz.iab.de/342/section.aspx/Publikation/k080530j05 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2028470/  NIST SP 800-188 3pd  November 2022  2571  2572  2573  [39] Cynthia Dwork et al. “Calibrating Noise to Sensitivity in Private Data Analysis”.  In: Theory of Cryptography. Ed. by Shai Halevi and Tal Rabin. Berlin, Heidelberg:  Springer Berlin Heidelberg, 2006, pp. 265–284. ISBN: 978-3-540-32732-5.  2574  2575  2576  2577  [40] Cynthia Dwork et al. “Calibrating Noise to Sensitivity in Private Data Analysis”.  In: Proceedings of the Third Conference on Theory of Cryptography. TCC’06. New  York, NY: Springer-Verlag, 2006, pp. 265–284. ISBN: 3540327312. DOI: 10.1007/  11681878 14. URL: doi.org/10.1007/11681878 14.  2578  2579  2580  2581  [41] Department of Education (ED) Disclosure Review Board (DRB). The Data Disclo- sure Decision. Version 1.0. 2015. URL: https://s3.amazonaws.com/sitesusa/wp- content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department- of-Education-Case-Study Mar-2015.pdf.  2582  2583  [42] Mark Elliot and Angela Dale. Scenarios of attack: the data intruder’s perspective  on statistical disclosure risk. Spring 1999.  2584  2585  2586  [43] El Emam. Methods for the de-identifcation of electronic health records for genomic  research. 2011. URL: https : / / genomemedicine . biomedcentral . com / articles / 10 .  1186/gm239.  2587  2588  2589  2590  [44] K. El Emam and B. Malin. “Appendix B: Concepts and Methods for De-Identifying  Clinical Trial Data”. In: Sharing Clinical Trial Data: Maximizing Benefts, Mini- mizing Risk. Washington, DC: Institute of Medicine of the National Academies,  The National Academies Press, 2015.  2591  2592  [45] Khaled El Emam and Luk Arbuckle. Anonymizing Health Data: Case Studies and  Methods to Get you Started. Sebastopol, CA: O’Reilly Media, 2013.  2593  2594  2595  2596  2597  [46] Ali Farzanehfar, Florimond Houssiau, and Yves-Alexandre de Montjoye. “The risk  of re-identifcation remains high even in country-scale location datasets”. In: Pat- terns 2.3 (2021), p. 100204. ISSN: 2666-3899. DOI: https : / /doi .org /10 .1016/ j .  patter. 2021 .100204. URL: https : / /www.sciencedirect . com/science / article /pii /  S2666389921000143.  2598  2599  2600  2601  [47] Confdentiality and Data Access Committee. Statistical Policy Working Paper 22:  Report on Statistical Disclosure Limitation Methodology. Tech. rep. Federal Com- mittee on Statistical Methodology, 2005. URL: https://www.hhs.gov/sites/default/  fles/spwp22.pdf.  2602  2603  [48] Federal Committee on Statistical Methodology. Data Protection Toolkit. Sept. 2020.  URL: https://nces.ed.gov/fcsm/dpt.  2604  2605  2606  2607  [49] Matthew Fredrikson et al. “Privacy in Pharmacogenetics: An End-to-End Case  Study of Personalized Warfarin Dosing”. In: 23rd USENIX Security Symposium.  San Diego, CA. URL: https://www.usenix.org/system/fles/conference/usenixsecurity14/  sec14-paper-fredrikson-privacy.pdf.  72  https://doi.org/10.1007/11681878_14 https://doi.org/10.1007/11681878_14 https://doi.org/10.1007/11681878_14 doi.org/10.1007/11681878_14 https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1151/2016/10/The-Data-Disclosure-Decision-Department-of-Education-Case-Study_Mar-2015.pdf https://genomemedicine.biomedcentral.com/articles/10.1186/gm239 https://genomemedicine.biomedcentral.com/articles/10.1186/gm239 https://genomemedicine.biomedcentral.com/articles/10.1186/gm239 https://doi.org/https://doi.org/10.1016/j.patter.2021.100204 https://doi.org/https://doi.org/10.1016/j.patter.2021.100204 https://doi.org/https://doi.org/10.1016/j.patter.2021.100204 https://www.sciencedirect.com/science/article/pii/S2666389921000143 https://www.sciencedirect.com/science/article/pii/S2666389921000143 https://www.sciencedirect.com/science/article/pii/S2666389921000143 https://www.hhs.gov/sites/default/files/spwp22.pdf https://www.hhs.gov/sites/default/files/spwp22.pdf https://www.hhs.gov/sites/default/files/spwp22.pdf https://nces.ed.gov/fcsm/dpt https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf  NIST SP 800-188 3pd  November 2022  2608  2609  2610  [50] Simson Garfnkel. De-Identifcation of Personally Identifable Information. Tech.  rep. NIST IR 8053. National Institute of Science and Technology, Nov. 2015. URL:  http://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf.  2611  2612  2613  [51] Simson L. Garfnkel. De-identifcation of personal information. 2015. DOI: 10 .  6028/NIST.IR.8053. URL: https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.  pdf.  2614  2615  2616  [52] Simson L. Garfnkel. Government Data De-Identifcation Stakeholder’s Meeting  June 29, 2016 Meeting Report. 2016. DOI: 10.6028/NIST.IR.8150. URL: https:  //nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf.  2617  2618  [53] Genetics Home Reference. What are single nucleotide polymorphisms (SNPs)?  Last access June 16, 2022. 2022. URL: https://ghr.nlm.nih.gov/primer/genomicresearch/  2619 snp.  2620  2621  [54] Genetics Home Reference. What is DNA. Last access June 16, 2022. 2022. URL:  https://ghr.nlm.nih.gov/primer/basics/dna.  2622  2623  [55] Craig Gentry. “A Fully Homomorphic Encryption Scheme”. AAI3382729. PhD  thesis. Stanford, CA, USA, 2009. ISBN: 9781109444506.  2624  2625  2626  [56] Ruobin Gong, Erica L. Groshen, and Salil Vadhan. “Harnessing the Known Un- knowns: Differential Privacy and the 2020 Census”. In: Harvard Data Science Re- view Special Issue 2 (June 2022). URL: https://hdsr.mitpress.mit.edu/pub/fgyf5cne.  2627  2628  [57] Melissa Gymrek et al. “Identifying Personal Genomes by Surname Inference”. In:  Science 339 (6117 Jan. 2013), pp. 321–329.  2629  2630  2631  [58] Michael B. Hawes. “Implementing Differential Privacy: Seven Lessons From the  2020 United States Census”. In: Harvard Data Science Review 2.2 (Apr. 2020).  URL: https://hdsr.mitpress.mit.edu/pub/dgg03vo6.  2632  2633  [59] TN Herzog, FJ Scheuren, and WE Winkler. Data Quality and Record Linkage Tech- niques. New York/London: Springer, 2007.  2634  2635  2636  [60] Vagelis Hristidis, ed. Information Discovery on Electronic Health Records. 1st.  Chapman and Hall/CRC, 2009. ISBN: 1420090380. URL: https://doi.org/10.1201/  9781420090413.  2637  2638  2639  2640  [61] IHE IT Infrastructure Technical Committee. IHE IT Infrastructure Handbook: De- Identifcation. Integrating the Healthcare Enterprise, Mar. 2014. URL: https://ihe.  net / uploadedFiles / Documents / ITI / IHE ITI Handbook De - Identifcation Rev1 .  0 2014-03-14.pdf.  2641  2642  2643  2644  [62] Clay Johnson III. OMB Memorandum M-07-16: Safeguarding Against and Re- sponding to the Breach of Personally Identifable Information. May 2007. URL:  https : / /georgewbush- whitehouse .archives .gov/omb/memoranda / fy2007/m07- 16.pdf.  73  http://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf https://doi.org/10.6028/NIST.IR.8053 https://doi.org/10.6028/NIST.IR.8053 https://doi.org/10.6028/NIST.IR.8053 https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf https://doi.org/10.6028/NIST.IR.8150 https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://ghr.nlm.nih.gov/primer/genomicresearch/snp https://ghr.nlm.nih.gov/primer/genomicresearch/snp https://ghr.nlm.nih.gov/primer/genomicresearch/snp https://ghr.nlm.nih.gov/primer/basics/dna https://hdsr.mitpress.mit.edu/pub/fgyf5cne https://hdsr.mitpress.mit.edu/pub/dgg03vo6 https://doi.org/10.1201/9781420090413 https://doi.org/10.1201/9781420090413 https://doi.org/10.1201/9781420090413 https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://ihe.net/uploadedFiles/Documents/ITI/IHE_ITI_Handbook_De-Identification_Rev1.0_2014-03-14.pdf https://georgewbush-whitehouse.archives.gov/omb/memoranda/fy2007/m07-16.pdf https://georgewbush-whitehouse.archives.gov/omb/memoranda/fy2007/m07-16.pdf https://georgewbush-whitehouse.archives.gov/omb/memoranda/fy2007/m07-16.pdf  NIST SP 800-188 3pd  November 2022  2645 [63] Information Commissioner’s Offce. Anonymisation: code of practice, managing  2646 data protection risk. 2012. URL: https://ico.org.uk/media/1061/anonymisation- 2647 code.pdf.  2648 [64] ISO 26324:2012, Information and documentation – Digital object identifer system.  2649 Geneva, Switzerland, 2012. URL: https://www.iso.org/standard/43506.html.  2650 [65] ISO/IEC 24760-1:2011, Information technology – Security techniques – A frame- 2651 work for identity management – Part 1: Terminology and concepts. 2011.  2652 [66] ISO/TS 25237:2008(E) Health Informatics — Pseudonymization. Geneva, Switzer- 2653 land, 2008.  2654 [67] Auguste Kerckhoffs. “II. Desiderata De La Cryptographie Militaire”. In: Journal  2655 des sciences militaires IX (Jan. 1883), pp. 5–38.  2656 [68] Shehab Khan. ““Human chimera”: Man fails paternity test because genes in his  2657 saliva are different to those in sperm”. In: The Independent (Oct. 2015).  2658 [69] Vladimir Kolesnikov et al. “Effcient Batched Oblivious PRF with Applications to  2659 Private Set Intersection”. In: Proceedings of the 2016 ACM SIGSAC Conference on  2660 Computer and Communications Security. CCS ’16. Vienna, Austria: Association  2661 for Computing Machinery, 2016, pp. 818–829. ISBN: 9781450341394. DOI: 10 .  2662 1145/2976749.2978381. URL: https://doi.org/10.1145/2976749.2978381.  2663 [70] Leah Krehling. De-Identifcation Guideline. Tech. rep. WL-2020-01. Department  2664 of Electrical and Computer Engineering, Western University, 2020, p. 45.  2665 [71] Sandra Lechner and Winfried Pohlmeier. “To Blank or Not to Blank? A Compari- 2666 son of the Effects of Disclosure Limitation Methods on Nonlinear Regression Es- 2667 timates”. In: Privacy in Statistical Databases, Lecture Notes in Computer Science  2668 3050 (2004), pp. 187–200.  2669 [72] Jaewoo Lee and Chris Clifton. “How Much Is Enough? Choosing ε for Differential  2670 Privacy”. In: Information Security. Ed. by Xuejia Lai, Jianying Zhou, and Hui Li.  2671 Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 325–340. ISBN: 978-3- 2672 642-24861-0.  2673 [73] Ninghui Li, Tiancheng Li, and Suresh Venkatasubramanian. “t-Closeness: Privacy  2674 Beyond k-Anonymity and l-Diversity”. In: 2007 IEEE 23rd International Confer- 2675 ence on Data Engineering. 2007, pp. 106–115. DOI: 10.1109/ICDE.2007.367856.  2676 [74] Yehuda Lindell. “Secure Multiparty Computation”. In: Commun. ACM 64.1 (Dec.  2677 2020), pp. 86–96. ISSN: 0001-0782. DOI: 10.1145/3387108. URL: https://doi.org/  2678 10.1145/3387108.  2679 [75] M. Altman M et al. “Towards a Modern Approach to Privacy-Aware Government  2680 Data Release”. In: Berkeley Journal of Technology Law Internet (2016). URL: https:  2681 //lawcat.berkeley.edu/record/1127405?ln=en.  74  https://ico.org.uk/media/1061/anonymisation-code.pdf https://ico.org.uk/media/1061/anonymisation-code.pdf https://ico.org.uk/media/1061/anonymisation-code.pdf https://www.iso.org/standard/43506.html https://doi.org/10.1145/2976749.2978381 https://doi.org/10.1145/2976749.2978381 https://doi.org/10.1145/2976749.2978381 https://doi.org/10.1145/2976749.2978381 https://doi.org/10.1109/ICDE.2007.367856 https://doi.org/10.1145/3387108 https://doi.org/10.1145/3387108 https://doi.org/10.1145/3387108 https://doi.org/10.1145/3387108 https://lawcat.berkeley.edu/record/1127405?ln=en https://lawcat.berkeley.edu/record/1127405?ln=en https://lawcat.berkeley.edu/record/1127405?ln=en  NIST SP 800-188 3pd  November 2022  2682 [76] Ashwin Machanavajjhala et al. “l-diversity: Privacy beyond k-anonymity”. In: Proc.  2683 22nd Intnl. Conf. Data Engg. (ICDE). 2006.  2684 [77] Sean Martin. When De-identifying Patient Information, Follow the HITRUST Frame- 2685 work. Sept. 2016. URL: https://hitrustalliance.net/de-identifying-patient-information- 2686 follow-hitrust-framework/.  2687 [78] Daniel A. Mayer et al. “Implementation and Performance Evaluation of Privacy- 2688 Preserving Fair Reconciliation Protocols on Ordered Sets”. In: Proceedings of the  2689 First ACM Conference on Data and Application Security and Privacy. CODASPY  2690 ’11. San Antonio, TX, USA: Association for Computing Machinery, 2011, pp. 109–  2691 120. ISBN: 9781450304665. DOI: 10.1145/1943513.1943529. URL: https://doi.org/  2692 10.1145/1943513.1943529.  2693 [79] E McCallister, T Grance, and K A Scarfone. Guide to protecting the confden- 2694 tiality of Personally Identifable Information (PII). Gaithersburg, MD, 2010. DOI:  2695 10.6028/NIST.SP.800-122. URL: https://nvlpubs.nist.gov/nistpubs/Legacy/SP/  2696 nistspecialpublication800-122.pdf.  2697 [80] William K. Michener et al. “Participatory design of DataONE—Enabling cyber- 2698 infrastructure for the biological and environmental sciences”. In: Ecological In- 2699 formatics 11 (2012). Data platforms in integrative biodiversity research, pp. 5–15.  2700 ISSN: 1574-9541. DOI: https : / / doi . org / 10 . 1016 / j . ecoinf . 2011 . 08 . 007. URL:  2701 https://www.sciencedirect.com/science/article/pii/S1574954111000768.  2702 [81] Yves-Alexandre de Montjoye et al. “Unique in the Crowd: The Privacy Bounds of  2703 Human Mobility”. In: Nature Scientifc Reports 3 (1376 2013).  2704 [82] Yves-Alexandre de Montjoye et al. “Unique in the Shopping Mall: On the Reiden- 2705 tifability of Credit Card Metadata”. In: Science 347 (536 2015).  2706 [83] Arvind Narayanan and Ed Felten. No silver bullet: De-identifcation still doesn’t  2707 work. Working Paper. July 2014. URL: http://randomwalker.info/publications/no- 2708 silver-bullet-de-identifcation.pdf.  2709 [84] Arvind Narayanan and Vitaly Shmatikov. “Robust De-anonymization of Large Sparse  2710 Datasets”. In: 2008 IEEE Symposium on Security and Privacy (sp 2008). 2008,  2711 pp. 111–125. DOI: 10.1109/SP.2008.33.  2712 [85] NIST Big Data Interoperability Framework: volume 1, defnitions, version 2. Gaithers- 2713 burg, MD, 2018. DOI: 10.6028/NIST.SP.1500-1r1. URL: https://nvlpubs.nist.gov/  2714 nistpubs/SpecialPublications/NIST.SP.1500-1r1.pdf.  2715 [86] NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk  2716 Management, Version 1.0. Gaithersburg, MD, 2022. DOI: 10.6028/NIST.CSWP.10.  2717 URL: https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.01162020.pdf.  75  https://hitrustalliance.net/de-identifying-patient-information-follow-hitrust-framework/ https://hitrustalliance.net/de-identifying-patient-information-follow-hitrust-framework/ https://hitrustalliance.net/de-identifying-patient-information-follow-hitrust-framework/ https://doi.org/10.1145/1943513.1943529 https://doi.org/10.1145/1943513.1943529 https://doi.org/10.1145/1943513.1943529 https://doi.org/10.1145/1943513.1943529 https://doi.org/10.6028/NIST.SP.800-122 https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf https://doi.org/https://doi.org/10.1016/j.ecoinf.2011.08.007 https://www.sciencedirect.com/science/article/pii/S1574954111000768 http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf https://doi.org/10.1109/SP.2008.33 https://doi.org/10.6028/NIST.SP.1500-1r1 https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-1r1.pdf https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-1r1.pdf https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-1r1.pdf https://doi.org/10.6028/NIST.CSWP.10 https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.01162020.pdf  NIST SP 800-188 3pd  November 2022  2718  2719  2720  2721  2722  [87] Christine M. O’Keefe and James O. Chipperfeld. “A Summary of Attack Meth- ods and Confdentiality Protection Measures for Fully Automated Remote Analysis  Systems”. In: International Statistical Review / Revue Internationale de Statistique  81.3 (2013), pp. 426–455. ISSN: 03067734, 17515823. URL: http://www.jstor.org/  stable/43299645 (visited on 06/28/2022).  2723  2724  2725  2726  [88] Barack Obama. Executive Order 13642—Making Open and Machine Readable the  New Default for Government Information. May 2013. URL: https://obamawhitehouse.  archives.gov/the- press- offce/2013/05/09/executive- order- making- open- and- machine-readable-new-default-government-.  2727  2728  2729  2730  2731  [89] Offce of Civil Rights, US Department of Health and Human Services. Guidance  Regarding Methods for De-identifcation of Protected Health Information in Ac- cordance with the Health Insurance Portability and Accountability Act (HIPAA)  Privacy Rule. Nov. 2012. URL: http : / / www. hhs . gov / hipaa / for - professionals /  privacy/special-topics/de-identifcation/.  2732  2733  2734  2735  [90] Offce of Civil Rights, US Department of Health and Human Services. Individuals’  Right under HIPAA to Access their Health Information 45 CFR § 164.524. Last  accessed June 17, 2022. 2022. URL: https://www.hhs.gov/hipaa/for-professionals/  privacy/guidance/access/index.html.  2736  2737  2738  [91] Offce of Management and Budget. Circular A110 Revised 11/19/93, as further  amended 9/30/99. URL: https : / / obamawhitehouse . archives . gov / omb / circulars  a110/.  2739  2740  2741  [92] Offce of Management and Budget. Statistical Programs and Standards. Last ac- cessed July 15, 2022. 2022. URL: https://www.whitehouse.gov/omb/information- regulatory-affairs/statistical-programs-standards/.  2742  2743  2744  [93] Offce of Safeguards, US Internal Revenue Service. Publication 1075: Tax Infor- mation Security Guidelines For Federal, State and Local Agencies. 2021. URL:  https://www.irs.gov/pub/irs-pdf/p1075.pdf.  2745  2746  [94] Paul Ohm. “Broken Promises of Privacy: Responding to the Surprising Failure of  Anonymization”. In: UCLA Law Review 57 (July 2012), pp. 1701–1778.  2747  2748  [95] OHRP-Guidance on Research Involving Private Information or Biological Speci- mens. Aug. 2008. URL: http://www.hhs.gov/ohrp/policy/cdebiol.html.  2749  2750  2751  [96] Joanne Pascale et al. Issue Paper on Disclosure Review for Information Products  with Qualitative Research Findings. Mar. 2020. URL: https : / /www.census .gov/  library/working-papers/2020/adrm/rsm2020-01.html.  2752  2753  2754  2755  2756  [97] Joanne Pascale et al. “Protecting the Identity of Participants in Qualitative Re- search”. In: Journal of Survey Statistics and Methodology 10.3 (Jan. 2022), pp. 549–  567. ISSN: 2325-0984. DOI: 10.1093/jssam/smab048. eprint: https://academic.oup.  com/jssam/article-pdf/10/3/549/44275508/smab048.pdf. URL: https://doi.org/10.  1093/jssam/smab048.  76  http://www.jstor.org/stable/43299645 http://www.jstor.org/stable/43299645 http://www.jstor.org/stable/43299645 https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- http://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/ http://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/ http://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/ https://www.hhs.gov/hipaa/for-professionals/privacy/guidance/access/index.html https://www.hhs.gov/hipaa/for-professionals/privacy/guidance/access/index.html https://www.hhs.gov/hipaa/for-professionals/privacy/guidance/access/index.html https://obamawhitehouse.archives.gov/omb/circulars_a110/ https://obamawhitehouse.archives.gov/omb/circulars_a110/ https://obamawhitehouse.archives.gov/omb/circulars_a110/ https://www.whitehouse.gov/omb/information-regulatory-affairs/statistical-programs-standards/ https://www.whitehouse.gov/omb/information-regulatory-affairs/statistical-programs-standards/ https://www.whitehouse.gov/omb/information-regulatory-affairs/statistical-programs-standards/ https://www.irs.gov/pub/irs-pdf/p1075.pdf http://www.hhs.gov/ohrp/policy/cdebiol.html https://www.census.gov/library/working-papers/2020/adrm/rsm2020-01.html https://www.census.gov/library/working-papers/2020/adrm/rsm2020-01.html https://www.census.gov/library/working-papers/2020/adrm/rsm2020-01.html https://doi.org/10.1093/jssam/smab048 https://academic.oup.com/jssam/article-pdf/10/3/549/44275508/smab048.pdf https://academic.oup.com/jssam/article-pdf/10/3/549/44275508/smab048.pdf https://academic.oup.com/jssam/article-pdf/10/3/549/44275508/smab048.pdf https://doi.org/10.1093/jssam/smab048 https://doi.org/10.1093/jssam/smab048 https://doi.org/10.1093/jssam/smab048  NIST SP 800-188 3pd  November 2022  2757  2758  2759  2760  2761  [98] Andrew Peterson. “Why the names of six people who complained of sexual assault  were published online by Dallas police”. In: The Washington Post (Apr. 2016).  URL: https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the- names-of-six-people-who-complained-of-sexual-assault-were-published-online- by-dallas-police/.  2762  2763  [99] Thomas Piketty and Emmanuel Saez. “Income Inequality in the United States 1913- 1998”. In: Quarterly Journal of Economics 118 (1 2003), pp. 1–41.  2764  2765  2766  [100] “Pillar Investigates: USCCB gen sec Burrill resigns after sexual misconduct alle- gations”. In: The Pillar (July 2021). URL: https://www.pillarcatholic.com/p/pillar- investigates-usccb-gen-sec.  2767  2768  2769  [101] Sandro Pinto and Nuno Santos. “Demystifying Arm TrustZone: A Comprehensive  Survey”. In: ACM Comput. Surv. 51.6 (Jan. 2019). ISSN: 0360-0300. DOI: 10.1145/  3291047. URL: https://doi.org/10.1145/3291047.  2770  2771  2772  [102] Private Lives and Public Policies: Confdentiality and Accessibility of Government  Statistics. Panel on Confdentiality and Data Access, National Research Council,  p. 288. ISBN: 0-309-57611-3. URL: http://www.nap.edu/catalog/2122/.  2773 [103] Public Law 93-579: The Privacy Act. 88 Stat. 1896, 5 U.S.C. § 552a.  2774  2775  [104] Balaji Raghunathan. The Complete Book of Data Anonymization: From Planning  to Implementation. USA: Auerbach Publications, 2013. ISBN: 1439877300.  2776  2777  [105] William H. Rehnquist. Department of State v. Washington Post Co., 456 U.S. 595  (1982). 1982. URL: https://www.loc.gov/item/usrep456595/.  2778  2779  2780  [106] Report 08-536, Privacy: Alternatives Exist for Enhancing Protection of Personally  Identifable Information. May 2008. URL: http://www.gao.gov/new.items/d08536.  pdf.  2781  2782  2783  [107] Diane Ridgeway et al. Challenge Design and Lessons Learned from the 2018 Dif- ferential Privacy Challenges. 2021. DOI: 10 . 6028 / NIST. TN . 2151. URL: https :  //nvlpubs.nist.gov/nistpubs/TechnicalNotes/NIST.TN.2151.pdf.  2784  2785  2786  2787  [108] Pierangela Samarati and Latanya Sweeney. “Protecting privacy when disclosing  information: k-anonymity and its enforcement through generalization and suppres- sion”. In: Proceedings of the IEEE Symposium on Research in Security and Privacy  (May 1998).  2788  2789  2790  [109] Pierangela Samarti. “Protecting Respondents’ Identities in Microdata Release”.  In: IEEE Transactions on Knowledge and Data Engineering 13 (6 Nov. 2001),  pp. 1010–1027.  2791  2792  2793  [110] Josep Sanz and Josep Domingo-Ferrer. “A Comparative Study of Microaggrega- tion Methods”. In: Questiio: Quaderns d’Estadistica, Sistemes, Informatica i In- vestigació Operativa 22 (3 Aug. 2000), pp. 511–526.  77  https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.washingtonpost.com/news/the-switch/wp/2016/04/29/why-the-names-of-six-people-who-complained-of-sexual-assault-were-published-online-by-dallas-police/ https://www.pillarcatholic.com/p/pillar-investigates-usccb-gen-sec https://www.pillarcatholic.com/p/pillar-investigates-usccb-gen-sec https://www.pillarcatholic.com/p/pillar-investigates-usccb-gen-sec https://doi.org/10.1145/3291047 https://doi.org/10.1145/3291047 https://doi.org/10.1145/3291047 https://doi.org/10.1145/3291047 http://www.nap.edu/catalog/2122/ https://www.loc.gov/item/usrep456595/ http://www.gao.gov/new.items/d08536.pdf http://www.gao.gov/new.items/d08536.pdf http://www.gao.gov/new.items/d08536.pdf https://doi.org/10.6028/NIST.TN.2151 https://nvlpubs.nist.gov/nistpubs/TechnicalNotes/NIST.TN.2151.pdf https://nvlpubs.nist.gov/nistpubs/TechnicalNotes/NIST.TN.2151.pdf https://nvlpubs.nist.gov/nistpubs/TechnicalNotes/NIST.TN.2151.pdf  NIST SP 800-188 3pd  November 2022  2794  2795  2796  [111] M Scaiano et al. “unifed framework for evaluating the risk of re-identifcation of  text de-identifcation tools”. In: Journal of Biomedical Informatics 63 (Oct. 2016),  pp. 174–183.  2797  2798  2799  2800  2801  [112] Matthias Schunter. “Intel Software Guard Extensions: Introduction and Open Re- search Challenges”. In: Proceedings of the 2016 ACM Workshop on Software PRO- tection. SPRO ’16. Vienna, Austria: Association for Computing Machinery, 2016,  p. 1. ISBN: 9781450345767. DOI: 10.1145/2995306.2995307. URL: https://doi.org/  10.1145/2995306.2995307.  2802  2803  2804  [113] M Seastrom. “Licensing”. In: Confdentiality, Disclosure and Data Access: Theory  and Practical Application for Statistical Agencies. Ed. by P. Doyle et al. Elsevier  Science, 2001.  2805  2806  2807  2808  2809  [114] Jordi Soria-Comas and Josep Domingo-Ferrer. “Connecting privacy models: syn- ergies between k-anonymity, t-closeness, and differential privacy”. In: Working  Paper (English Only). Ottawa, Canada, Oct. 2013. URL: https : / / www . unece .  org / fleadmin / DAM / stats / documents / ece / ces / ge . 46 / 2013 / Topic 2 soria - comas domingo-ferrer.pdf.  2810  2811  [115] Philip Steel and Jon Sperling. The Impact of Multiple Geographies and Geographic  Detail on Disclosure Risk: Interactions between Census Tract and ZIP Code Tab-  2812  2813  ulation Geography. 2001. URL: https : / /www.census .gov/content /dam/Census/  library/working-papers/2001/adrm/steel-sperling-2001.pdf.  2814  2815  2816  [116] Jackson M. Steinkamp et al. “Evaluation of Automated Public De-Identifcation  Tools on a Corpus of Radiology Reports”. In: Radiol Artif Intell 2 (6 Oct. 2020).  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8082401/.  2817  2818  [117] John Paul Stevens. U.S. Dept. of Justice v. Reporters Committee For Freedom of  Press, 489 U.S. 749 (1989). 1988. URL: https://www.loc.gov/item/usrep489749/.  2819  2820  [118] John Paul Stevens. United States Department of State v. Ray et al., 502 U.S. 164  (1991). 1991. URL: https://www.loc.gov/item/usrep502164/.  2821  2822  2823  2824  [119] Kevin Stine et al. Volume I: guide for mapping types of information and infor- mation systems to security categories. Gaithersburg, MD, 2008. DOI: 10 . 6028 /  NIST. SP. 800 - 60v1r1. URL: https : / / nvlpubs . nist . gov / nistpubs / Legacy / SP /  nistspecialpublication800-60v1r1.pdf.  2825  2826  [120] Tim Stobierski. In: Business Insights (Feb. 2021). URL: https : / /online .hbs .edu/  blog/post/data-life-cycle.  2827  2828  2829  [121] Teresa A. Sullivan. “Coming to Our Census: How Social Statistics Underpin Our  Democracy (and Republic)”. In: Harvard Data Science Review 2.1 (Jan. 2020).  URL: https://hdsr.mitpress.mit.edu/pub/1g1cbvkv.  2830  2831  2832  [122] Latanya Sweeney. “k-anonymity: a model for protecting privacy”. In: International  Journal on Uncertainty, Fuzziness and Knowledge-based Systems 10 (5 2002),  pp. 557–570.  78  https://doi.org/10.1145/2995306.2995307 https://doi.org/10.1145/2995306.2995307 https://doi.org/10.1145/2995306.2995307 https://doi.org/10.1145/2995306.2995307 https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_2_soria-comas_domingo-ferrer.pdf https://www.census.gov/content/dam/Census/library/working-papers/2001/adrm/steel-sperling-2001.pdf https://www.census.gov/content/dam/Census/library/working-papers/2001/adrm/steel-sperling-2001.pdf https://www.census.gov/content/dam/Census/library/working-papers/2001/adrm/steel-sperling-2001.pdf https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8082401/ https://www.loc.gov/item/usrep489749/ https://www.loc.gov/item/usrep502164/ https://doi.org/10.6028/NIST.SP.800-60v1r1 https://doi.org/10.6028/NIST.SP.800-60v1r1 https://doi.org/10.6028/NIST.SP.800-60v1r1 https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://online.hbs.edu/blog/post/data-life-cycle https://online.hbs.edu/blog/post/data-life-cycle https://online.hbs.edu/blog/post/data-life-cycle https://hdsr.mitpress.mit.edu/pub/1g1cbvkv  NIST SP 800-188 3pd  November 2022  2833  2834  2835  [123] Latanya Sweeney. “k-anonymity: a model for protecting privacy”. In: Int. J. Un- certain. Fuzziness Knowl.-Based Syst. 10 (5 Oct. 2002), pp. 557–570. URL: http:  //dx.doi.org/10.1142/S0218488502001648.  2836  2837  [124] Latanya Sweeney. “Weaving Technology and Policy Together to Maintain Conf- dentiality”. In: Journal of Law, Medicine and Ethics 25 (1997), pp. 98–110.  2838  2839  2840  [125] “The Debate Over ‘Re-Identifcation’ Of Health Information: What Do We Risk?”  In: Health Affairs Blog (Aug. 2012). DOI: 10.1377/hblog20120810.021952. URL:  https://www.healthaffairs.org/do/10.1377/forefront.20120810.021952.  2841  2842  2843  [126] Title V of the E-Government Act of 2002: Confdential Information Protection and  Statistical Effciency Act (CIPSEA) PL 107–347,116 Stat. 2899, 44 USC § 101  Section 502(8).  2844  2845  [127] TransCelerate Biopharma, Inc. Data De-identifcation and Anonymization of Indi- vidual Patient Data in Clinical Studies—A Model Approach. 2013.  2846  2847  2848  [128] Michael Carl Tschantz and Jeannette M. Wing. Formal Methods for Privacy. Tech.  rep. CMU-CS-09-154. Pittsburg, PA: Carnegie Mellon University, Aug. 2009. URL:  http://reports-archive.adm.cs.cmu.edu/anon/2009/CMU-CS-09-154.pdf.  2849  2850  2851  [129] A. M. Turing. “On Computable Numbers, with an Application to the Entschei- dungsproblem”. In: Proceedings of the London Mathematical Society, Series 2 (42  1936–37), pp. 230–265.  2852  2853  [130] US Census Bureau. Census Confdentiality and Privacy: 1790-2002. 2003. URL:  https://www.census.gov/prod/2003pubs/conmono2.pdf.  2854  2855  [131] US Census Bureau. The “72-Year Rule”. Jan. 2022. URL: https://www.census.gov/  history/www/genealogy/decennial census records/the 72 year rule 1.html.  2856  2857  2858  [132] US Congress. Public Law 104-191: Health Insurance Portability and Accountabil- ity Act of 1996 (HIPAA). Aug. 1996. URL: https://www.congress.gov/bill/104th- congress/house-bill/3103.  2859  2860  [133] US Congress. Public Law 114-185: FOIA Improvement Act of 2016. 2016. URL:  https://www.congress.gov/114/plaws/publ185/PLAW-114publ185.pdf.  2861  2862  [134] US Congress. The Freedom Of Information Act, 5 U.S.C. § 552. 2022. URL: https:  //www.justice.gov/oip/freedom-information-act-5-usc-552.  2863  2864  2865  [135] US Department of Health and Human Services. 45 CFR Part 46: Federal Policy  for the Protection of Human Subjects. Jan. 2017. URL: https://www.govinfo.gov/  content/pkg/FR-2017-01-19/pdf/2017-01058.pdf.  2866  2867  [136] US Department of Health and Human Services. Guidance Regarding Methods for  De-identifcation of Protected Health Information in Accordance with the Health  2868  2869  2870  Insurance Portability and Accountability Act (HIPAA) Privacy Rule. 2012. URL:  https : / / www . hhs . gov / hipaa / for - professionals / privacy / special - topics / de - identifcation/index.html.  79  http://dx.doi.org/10.1142/S0218488502001648 http://dx.doi.org/10.1142/S0218488502001648 http://dx.doi.org/10.1142/S0218488502001648 https://doi.org/10.1377/hblog20120810.021952 https://www.healthaffairs.org/do/10.1377/forefront.20120810.021952 http://reports-archive.adm.cs.cmu.edu/anon/2009/CMU-CS-09-154.pdf https://www.census.gov/prod/2003pubs/conmono2.pdf https://www.census.gov/history/www/genealogy/decennial_census_records/the_72_year_rule_1.html https://www.census.gov/history/www/genealogy/decennial_census_records/the_72_year_rule_1.html https://www.census.gov/history/www/genealogy/decennial_census_records/the_72_year_rule_1.html https://www.congress.gov/bill/104th-congress/house-bill/3103 https://www.congress.gov/bill/104th-congress/house-bill/3103 https://www.congress.gov/bill/104th-congress/house-bill/3103 https://www.congress.gov/114/plaws/publ185/PLAW-114publ185.pdf https://www.justice.gov/oip/freedom-information-act-5-usc-552 https://www.justice.gov/oip/freedom-information-act-5-usc-552 https://www.justice.gov/oip/freedom-information-act-5-usc-552 https://www.govinfo.gov/content/pkg/FR-2017-01-19/pdf/2017-01058.pdf https://www.govinfo.gov/content/pkg/FR-2017-01-19/pdf/2017-01058.pdf https://www.govinfo.gov/content/pkg/FR-2017-01-19/pdf/2017-01058.pdf https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html  NIST SP 800-188 3pd  November 2022  2871  2872  2873  2874  2875  [137] Joel Havermann, plaintiff—Appellant v. Carolyn W. Colvin, Acting Commissioner  of the Social Security Administration, Defendant— Appellee, No. 12-2453, US Court  of Appeals for the Fourth Circuit, 537 Fed. Appx. 142; 2013 US App. Aug 1,  2013. Joel Havemann v. Carolyn W. Colvin, Civil No. JFM-12-1325. 2015 US Dist.  LEXIS 27560. Mar. 2015.  2876  2877  [138] “Utility”. In: Glossary of Statistical Terms (Aug. 2002). Last accessed June 23,  2022. URL: https://stats.oecd.org/glossary/detail.asp?ID=4884.  2878  2879  2880  [139] Russell T. Vought. Phase 1 Implementation of the Foundations for Evidence-Based  Policymaking Act of 2018: Learning Agendas, Personnel, and Planning Guidance.  URL: https://www.whitehouse.gov/wp-content/uploads/2019/07/M-19-23.pdf.  2881  2882  2883  [140] Charlie Warzel and Stuart A. Thompson. “How Your Phone Betrays Democracy”.  In: The New York Times (Dec. 2019). URL: https://www.nytimes.com/interactive/  2019/12/21/opinion/location-data-democracy-protests.html.  2884  2885  2886  [141] Cathy Wasserman and Eric Ossiander. Department of Health Agency Standards for  Reporting Data with Small Numbers. May 2018. URL: https://doh.wa.gov/sites/  default/fles/legacy/Documents/1500//SmallNumbers.pdf.  2887  2888  2889  [142] Leon Willenborg and Ton de Waal. “Chapter 3 Data Analytic Impact of SDC  Techniques on Microdata”. In: Elements of Statistical Disclosure Control (2012),  pp. 72–92.  2890  2891  2892  2893  [143] Li Xiong et al. “Privacy-Preserving Information Discovery on EHRs”. In: Informa- tion Discovery on Electronic Health Records. Ed. by Vagelis Hristidis. 1st. Chap- man and Hall/CRC, 2009. ISBN: 1420090380. URL: https : / / doi . org / 10 . 1201 /  9781420090413.  80  https://stats.oecd.org/glossary/detail.asp?ID=4884 https://www.whitehouse.gov/wp-content/uploads/2019/07/M-19-23.pdf https://www.nytimes.com/interactive/2019/12/21/opinion/location-data-democracy-protests.html https://www.nytimes.com/interactive/2019/12/21/opinion/location-data-democracy-protests.html https://www.nytimes.com/interactive/2019/12/21/opinion/location-data-democracy-protests.html https://doh.wa.gov/sites/default/files/legacy/Documents/1500//SmallNumbers.pdf https://doh.wa.gov/sites/default/files/legacy/Documents/1500//SmallNumbers.pdf https://doh.wa.gov/sites/default/files/legacy/Documents/1500//SmallNumbers.pdf https://doi.org/10.1201/9781420090413 https://doi.org/10.1201/9781420090413 https://doi.org/10.1201/9781420090413  NIST SP 800-188 3pd  November 2022  2894  2895  2896  2897  2898  2899  2900  2901  2902  2903  2904  2905  2906  2907  2908  2909  2910  2911  2912  2913  2914  2915  2916  2917  2918  2919  2920  2921  2922  2923  2924  2925  2926  2927  2928  Appendix A. Standards  • ASTM E1869-04(2014) Standard Guide for Confdentiality, Privacy, Access, and  Data Security Principles for Health Information Including Electronic Health Records.  • DICOM PS3.15 2016d – Security and System Management Profles Chapter E At- tribute Confdentiality Profles, DICOM Standards Committee, NEMA 2016. http:  //dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter E  • HITRUST De-Identifcation Working Group (2015, March). De-Identifcation Frame- work: A Consistent, Managed Methodology for the De-Identifcation of Personal  Data and the Sharing of Compliance and Risk Information. Frisco, TX: HITRUST.  Retrieved from https://hitrustalliance.net/de-identifcation-license-agreement/  • ISO 8000-2:2012(E) Data quality – Part 2: Vocabulary, 2012. ISO, Geneva, Switzer- land. 2012.  • ISO/IEC 27000:2014 Information technology -- Security techniques -- Information  security management systems -- Overview and vocabulary. ISO, Geneva, Switzer- land. 2012.  • ISO/IEC 24760-1:2011 Information technology -- Security techniques -- A frame- work for identity management -- Part 1: Terminology and concepts. ISO, Geneva,  Switzerland. 2011.  • ISO/TS 25237:2008(E) Health Informatics – Pseudonymization. ISO, Geneva, Switzer- land. 2008.  • ISO/IEC 20889 WORKING DRAFT 2016-05-30, Information technology – Secu- rity techniques – Privacy enhancing data de-identifcation techniques. ISO, Geneva,  Switzerland. 2016.  • IHE IT Infrastructure Handbook, De-Identifcation, Integrating the Healthcare Enter- prise, June 6, 2014. http://www.ihe.net/User Handbooks/  Appendix A.1. NIST Publications  • NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk  Management, Version 1.0. Gaithersburg, MD, 2022. DOI: 10.6028/NIST.CSWP.10.  URL: https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.01162020.pdf  • Kevin Stine et al. Volume I: guide for mapping types of information and information  systems to security categories. Gaithersburg, MD, 2008. DOI: 10.6028/NIST.SP.800- 60v1r1. URL: https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800- 60v1r1.pdf  • Simson L. Garfnkel. De-identifcation of personal information. 2015. DOI: 10.6028/  NIST.IR.8053. URL: https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf  81  http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E http://dicom.nema.org/medical/dicom/current/output/html/part15.html#chapter_E https://doi.org/10.6028/NIST.CSWP.10 https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.01162020.pdf https://doi.org/10.6028/NIST.SP.800-60v1r1 https://doi.org/10.6028/NIST.SP.800-60v1r1 https://doi.org/10.6028/NIST.SP.800-60v1r1 https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-60v1r1.pdf https://doi.org/10.6028/NIST.IR.8053 https://doi.org/10.6028/NIST.IR.8053 https://doi.org/10.6028/NIST.IR.8053 https://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8053.pdf http://www.ihe.net/User https://hitrustalliance.net/de-identification-license-agreement  NIST SP 800-188 3pd  November 2022  2929  2930  2931  2932  2933  2934  2935  2936  2937  2938  2939  2940  2941  2942  2943  2944  2945  2946  2947  2948  2949  2950  2951  2952  2953  2954  2955  2956  2957  2958  2959  2960  2961  2962  • Simson L. Garfnkel. Government Data De-Identifcation Stakeholder’s Meeting  June 29, 2016 Meeting Report. 2016. DOI: 10.6028/NIST.IR.8150. URL: https:  //nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf  Appendix A.2. Other U.S. Government Publications  • Census Confdentiality and Privacy: 1790-2002, US Census Bureau, 2003. https:  //www.census.gov/prod/2003pubs/conmono2.pdf  • Data De-identifcation: An Overview of Basic Terms, Privacy Technical Assistance  Center, US Department of Education. May 2013. http://ptac.ed.gov/sites/default/  fles/data deidentifcation terms.pdf  • Data Disclosure Decision, Department of Education (ED) Disclosure Review Board  (DRB), A Product of the Federal CIO Council Innovation Committee. Version 1.0,  2015.  • Disclosure Avoidance Techniques at the US Census Bureau: Current Practices and  Research, Research Report Series (Disclosure Avoidance #2014-02), Amy Lauger,  Billy Wisniewski, and Laura McKenna, Center for Disclosure Avoidance Research,  US Census. Bureau, September 26, 2014. https://www.census.gov/srd/CDAR/cdar2014-02  Discl Avoid Techniques.pdf  • Frequently Asked Questions – Disclosure Avoidance, Privacy Technical Assistance  Center, U.S. Department of Education. October 2012 (revised July 2015). http:  //ptac.ed.gov/sites/default/fles/FAQ Disclosure Avoidance.pdf  • Guidance Regarding Methods for De-identifcation of Protected Health Information  in Accordance with the Health Insurance Portability and Accountability Act (HIPAA)  Privacy Rule, U.S. Department of Health & Human Services, Offce for Civil Rights,  November 26, 2012. http://www.hhs.gov/ocr/privacy/hipaa/understanding/coveredentities/  De-identifcation/hhs deid guidance.pdf  • http://www.hhs.gov/ohrp/policy/cdebiol.html  • http://ptac.ed.gov/sites/default/fles/data deidentifcation terms.pdf  • The Data Disclosure Decision, Department of Education (ED) Disclosure Review  Board (DRB), A Product of the Federal CIO Council Innovation Committee.  • https://www.cdc.gov/nchs/data/nchs microdata release policy 4-02a.pdf  • http://www.cdc.gov/nchs/nvss/dvs data release.htm  • Linking Data for Health Services Research: A Framework and Instructional Guide.,  Dusetzina SB, Tyree S, Meyer AM, Meyer A, Green L, Carpenter WR. (Prepared  by the University of North Carolina at Chapel Hill under Contract No. 290-2010-  82  https://doi.org/10.6028/NIST.IR.8150 https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8150.pdf https://www.census.gov/prod/2003pubs/conmono2.pdf https://www.census.gov/prod/2003pubs/conmono2.pdf https://www.census.gov/prod/2003pubs/conmono2.pdf http://ptac.ed.gov/sites/default/files/data_deidentification_terms.pdf http://ptac.ed.gov/sites/default/files/data_deidentification_terms.pdf http://ptac.ed.gov/sites/default/files/data_deidentification_terms.pdf https://www.census.gov/srd/CDAR/cdar2014-02_Discl_Avoid_Techniques.pdf https://www.census.gov/srd/CDAR/cdar2014-02_Discl_Avoid_Techniques.pdf https://www.census.gov/srd/CDAR/cdar2014-02_Discl_Avoid_Techniques.pdf http://ptac.ed.gov/sites/default/files/FAQ_Disclosure_Avoidance.pdf http://ptac.ed.gov/sites/default/files/FAQ_Disclosure_Avoidance.pdf http://ptac.ed.gov/sites/default/files/FAQ_Disclosure_Avoidance.pdf http://www.hhs.gov/ocr/privacy/hipaa/understanding/coveredentities/De-identification/hhs_deid_guidance.pdf http://www.hhs.gov/ocr/privacy/hipaa/understanding/coveredentities/De-identification/hhs_deid_guidance.pdf http://www.hhs.gov/ocr/privacy/hipaa/understanding/coveredentities/De-identification/hhs_deid_guidance.pdf http://www.hhs.gov/ohrp/policy/cdebiol.html http://ptac.ed.gov/sites/default/files/data_deidentification_terms.pdf https://www.cdc.gov/nchs/data/nchs_microdata_release_policy_4-02a.pdf http://www.cdc.gov/nchs/nvss/dvs_data_release.htm  privacy/privacy-  NIST SP 800-188 3pd  November 2022  2963  2964  2965  2966  2967  2968  2969  2970  2971  2972  2973  2974  2975  2976  2977  2978  2979  2980  2981  2982  2983  2984  2985  2986  2987  2988  2989  2990  2991  2992  2993  2994  2995  2996  000141.) AHRQ Publication No. 14-EHC033-EF. Rockville, MD: Agency for Health- care Research and Quality; September 2014.  • National Center for Health Statistics Data Release and Access Policy for Micro-data  and Compressed Vital Statistics File, Centers for Disease Control, April 26, 2011.  http://www.cdc.gov/nchs/nvss/dvs data release.htm  • National Center for Health Statistics Policy on Micro-Data Dissemination, Centers  for Disease Control, July 2002. https://www.cdc.gov/nchs/data/nchs microdata release policy 4- 02a.pdf  • OHRP-Guidance on Research Involving Private Information or Biological Speci- mens (2008), Department of Health & Human Services, Offce of Human Research  Protections (OHRP), August 16, 2008. http://www.hhs.gov/ohrp/policy/cdebiol.html  • OMB Circular A-130, Managing Information as a Strategic Resource, July 2016.  • Privacy and Confdentiality Research and the U.S. Census Bureau, Recommenda- tions Based on a Review of the Literature, Thomas S. Mayer, Statistical Research Di- vision, US Bureau of the Census. February 7, 2002. https://www.census.gov/srd/papers/pdf/rsm2002- 01.pdf  • Statistical Policy Working Paper 22 (Second version, 2005), Report on Statistical  Disclosure Limitation Methodology, Federal Committee on Statistical Methodology,  December 2005.  Selected Publications by Other Governments  • Privacy business resource 4: De-identifcation of data and information, Offce of the  Australian Information Commissioner, Australian Government, April 2014. http://www.oaic.gov.au/images/documents/  resources/privacy-business-resources/privacy business resource 4.pdf  • Opinion 05/2014 on Anonymisation Techniques, Article 29 Data Protection Working  Party, 0829/14/EN WP216, Adopted on 10 April 2014.  • Anonymisation: Managing data protection risk, Code of Practice 2012, Information  Commissioner’s Offce. https://ico.org.uk/media/for-organisations/documents/1061/anonymisation- code.pdf. 108 pages.  • The Anonymisation Decision-Making Framework, Mark Elliot, Elaine Mackey, Kieron  O’Hara and Caroline Tudor, UKAN, University of Manchester, July 2016. http:  //ukanon.net/ukan-resources/ukan-decision-making-framework/  Reports and Books  • Private Lives and Public Policies: Confdentiality and Accessibility of Government  Statistics (1993), George T. Duncan, Thomas B. Jabine, and Virginia A. de Wolf,  83  http://www.cdc.gov/nchs/nvss/dvs_data_release.htm https://www.cdc.gov/nchs/data/nchs_microdata_release_policy_4-02a.pdf https://www.cdc.gov/nchs/data/nchs_microdata_release_policy_4-02a.pdf https://www.cdc.gov/nchs/data/nchs_microdata_release_policy_4-02a.pdf http://www.hhs.gov/ohrp/policy/cdebiol.html http://www.oaic.gov.au/images/documents/privacy/privacy-resources/privacy-business-resources/privacy_business_resource_4.pdf http://www.oaic.gov.au/images/documents/privacy/privacy-resources/privacy-business-resources/privacy_business_resource_4.pdf http://www.oaic.gov.au/images/documents/privacy/privacy-resources/privacy-business-resources/privacy_business_resource_4.pdf https://ico.org.uk/media/for-organisations/documents/1061/anonymisation-code.pdf https://ico.org.uk/media/for-organisations/documents/1061/anonymisation-code.pdf https://ico.org.uk/media/for-organisations/documents/1061/anonymisation-code.pdf http://ukanon.net/ukan-resources/ukan-decision-making-framework/ http://ukanon.net/ukan-resources/ukan-decision-making-framework/ http://ukanon.net/ukan-resources/ukan-decision-making-framework/ https://www.census.gov/srd/papers/pdf/rsm2002  NIST SP 800-188 3pd  November 2022  2997  2998  2999  3000  3001  3002  3003  3004  3005  3006  3007  3008  3009  3010  3011  3012  3013  3014  3015  3016  3017  3018  3019  3020  3021  3022  3023  3024  3025  3026  3027  3028  3029  3030  3031  Editors; Panel on Confdentiality and Data Access; Commission on Behavioral and  Social Sciences and Education; Division of Behavioral and Social Sciences and Ed- ucation; National Research Council, 1993. http://dx.doi.org/10.17226/2122  • Sharing Clinical Trial Data: Maximizing Benefts, Minimizing Risk, Committee on  Strategies for Responsible Sharing of Clinical Trial Data, Board on Health Sciences  Policy, Institute of Medicine of the National Academies, The National Academies  Press, Washington, DC. 2015.  • P. Doyle and J. Lane, Confdentiality, Disclosure and Data Access: Theory and Prac- tical Applications for Statistical Agencies, North-Holland Publishing, Dec 31, 2001.  • George T. Duncan, Mark Elliot, Juan-José Salazar-Gonzalez, Statistical Confden- tiality: Principles and Practice, Springer, 2011.  • Cynthia Dwork and Aaron Roth, The Algorithmic Foundations of Differential Pri- vacy (Foundations and Trends in Theoretical Computer Science). Now Publishers,  August 11, 2014. http://www.cis.upenn.edu/˜aaroth/privacybook.html  • Khaled El Emam, Guide to the De-Identifcation of Personal Health Information,  CRC Press, 2013.  • Khaled El Emam and Luk Arbuckle, Anonymizing Health Data, O’Reilly, Cam- bridge, MA. 2013.  • K El Emam and B Malin, “Appendix B: Concepts and Methods for De-Identifying  Clinical Trial Data,” in Sharing Clinical Trial Data: Maximizing Benefts, Minimiz- ing Risk, Institute of Medicine of the National Academies, The National Academies  Press, Washington, DC. 2015.  • Anco Hundepool, Josep Domingo-Ferrer, Luisa Franconi, Sarah Giessing, Eric Schulte  Nordholt, Keith Spicer, Peter-Paul de Wolf, Statistical Disclosure Control, Wiley,  September 2012.  How-To Articles  • Leah Krehling, De-Identifcation Guideline, WHISPERLAB, Technical Report WL- 2020-01, Department of Electrical and Computer Engineering, Western University,  2020.  • Olivia Angiuli, Joe Blitstein, and Jim Waldo, How to De-Identify Your Data, Com- munications of the ACM, December 2015.  • Jörg Drechsler, Stefan Bender, Susanne Rässler, Comparing fully and partially syn- thetic datasets for statistical disclosure control in the German IAB Establishment  Panel. 2007, United Nations, Economic Commission for Europe. Working paper,  11, New York, 8 p. http://fdz.iab.de/342/section.aspx/Publikation/k080530j05  84  http://www.nap.edu/author/CBASSE http://www.nap.edu/author/CBASSE http://www.nap.edu/author/CBASSE http://www.nap.edu/author/DBASSE http://www.nap.edu/author/DBASSE http://www.nap.edu/author/DBASSE http://fdz.iab.de/342/section.aspx/Publikation/k080530j05 http://www.cis.upenn.edu/�aaroth/privacybook.html http://dx.doi.org/10.17226/2122  NIST SP 800-188 3pd  November 2022  3032  3033  3034  3035  3036  3037  3038  3039  3040  3041  3042  3043  3044  3045  • Ebaa Fayyoumi and B. John Oommen, A survey on statistical disclosure control and  micro-aggregation techniques for secure statistical databases. 2010, Software Prac- tice and Experience. 40, 12 (November 2010), 1161-1188. DOI=10.1002/spe.v40:12  http://dx.doi.org/10.1002/spe.v40:12http://dx.doi.org/10.1002/spe.v40:12  • Jingchen Hu, Jerome P. Reiter, and Quanli Wang, Disclosure Risk Evaluation for  Fully Synthetic Categorical Data, Privacy in Statistical Databases, pp. 185-199,  2014. https://link.springer.com/chapter/10.1007/978-3-319-11257-2 15  • Matthias Templ, Bernhard Meindl, Alexander Kowarik and Shuang Chen, Introduc- tion to Statistical Disclosure Control (SDC), IHSN Working Paper No. 007, Inter- national Household Survey Network, August 2014. http://www.ihsn.org/home/sites/  default/fles/resources/ihsn-working-paper-007-Oct27.pdf  • Natalie Shlomo, Statistical Disclosure Control Methods for Census Frequency Ta- bles, International Statistical Review (2007), 75, 2, 199-217. https://www.jstor.org/  stable/41508461  85  http://dx.doi.org/10.1002/spe.v40:12 https://link.springer.com/chapter/10.1007/978-3-319-11257-2_15 http://www.ihsn.org/home/sites/default/files/resources/ihsn-working-paper-007-Oct27.pdf http://www.ihsn.org/home/sites/default/files/resources/ihsn-working-paper-007-Oct27.pdf http://www.ihsn.org/home/sites/default/files/resources/ihsn-working-paper-007-Oct27.pdf https://www.jstor.org/stable/41508461 https://www.jstor.org/stable/41508461 https://www.jstor.org/stable/41508461  NIST SP 800-188 3pd  November 2022  3046  3047  3048  3049  3050  3051  3052  3053  3054  3055  3056  3057  3058  3059  3060  3061  3062  3063  3064  3065  3066  3067  3068  3069  3070  3071  3072  3073  Appendix B. List of Symbols, Abbreviations, and Acronyms  Selected acronyms and abbreviations used in this paper are defned below.  ACM Association for Computing Machinery  AHRQ Agency for Healthcare Research and Quality  AMD Advanced Micro Devices  ARM Advanced RISC Machines (formerly Acron RISC Machine)  ARMP average record matching probability  ASTM ASTM (formerly the American Society for Testing and Materials)  CED-DA Center for Enterprise Dissemination-Disclosure Avoidance  CFR Code of Federal Regulations  CIO chief information offcer  CIPSEA The Confdential Information Protection and Statistical Effciency Act of 2002  CNSS Committee on National Security Systems  CNSSI Committee on National Security Systems instruction  CPU central processing unit  CRC (formerly the Chemical Rubber Company)  DC District of Columbia  DCMA Defense Contract Management Agency  DICOM Digital Imaging and Communications in Medicine  DNA deoxyribonucleic acid  DOI digital object identifer  DRB disclosure review board  DUA data use agreement  EDDRB Department of Education disclosure review board  FCSM Federal Committee on Statistical Methodology  FHE Fully-homomorphic encryption  FISMA Federal Information Security Modernization Act  FOIA Freedom of Information Act  86  NIST SP 800-188 3pd  November 2022  3074  3075  3076  3077  3078  3079  3080  3081  3082  3083  3084  3085  3086  3087  3088  3089  3090  3091  3092  3093  3094  3095  3096  3097  3098  3099  3100  3101  3102  HHS Health and Human Services  HIPAA Health Insurance Portability and Accountability Act  HITRUST (formerly the Health Industry Trust Alliance)  IAB Institut fur¨ Arbeitsmarkt-und Berufsforschung (Germany’s Institute for Employment  and Research)  ICSP Interagency Council on Statistical Policy  ID Identifcation number  IEC International Electrotechnical Commission  IHE Integrating the Healthcare Enterprise  IHSN International Household Survey Network  IP internet protocol  IR inter-agency report  IRB institutional review board  IRS Internal Revenue Service  ISO (formerly International Organization for Standardization)  ISO/TS ISO Technical Standard  IT information technology  ITL Information Technology Laboratory  KIRP Known inclusion re-identifcation probability  MA Massachusetts  MCC Millennium Challenge Corporation  MD Maryland  MIT Massachusetts Institute of Technology  MPC multi-party computation  NEMA National Electrical Manufacturers Association  NIST National Institute of Standards and Technology  NISTIR National Institute of Standards and Technology interagency report  OECD Organisation for Economic Co-operation and Development  OHRP Offce for Human Research Protections  87  NIST SP 800-188 3pd  November 2022  3103  3104  3105  3106  3107  3108  3109  3110  3111  3112  3113  3114  3115  3116  3117  3118  3119  3120  3121  3122  3123  3124  3125  OMB Offce of Management and Budget  OPRE Offce of Planning, Research and Evaluation  PDF portable document fle  PEC privacy enhancing cryptography  PHI protected health information  PII personally identifable information  PL public law  PUF public use fle  RMP record matching probability  SDC statistical disclosure control  SDL statistical disclosure limitation  SHA secure hash algorithm  SLA service-level agreement  SP special publication  TEE trusted execution environments  TX Texas  UIRP Unknown inclusion re-identifcation probability  UK United kingdom  UKAN United Kingdom Advocacy Network  US United States  USC United States Code  WHISPERLAB Western Information Security and Privacy Research Laboratory  WP working paper  88  NIST SP 800-188 3pd  November 2022  3126  3127  3128  3129  3130  3131  3132  3133  3134  3135  3136  3137  3138  3139  3140  3141  3142  3143  3144  3145  3146  3147  3148  3149  3150  3151  3152  3153  3154  3155  3156  3157  3158  Appendix C. Glossary  Selected terms used in the publication are defned below. Where noted, the defnition is  sourced from another publication.  anonymization A process that removes the association between the identifying dataset  and the data subject. (ISO 25237-2008)  attribute An inherent characteristic. (ISO 9241-302:2008)  attribute disclosure Re-identifcation event in which an entity learns confdential infor- mation about a data principal, without necessarily identifying the data principal.  (ISO/IEC 20889 WORKING DRAFT 2 2016-05-27)  anonymity Condition in identifcation whereby an entity can be recognized as distinct,  without suffcient identity information to establish a link to a known identity. (ISO/IEC  24760-1:2011)  anticipated re-identifcation rate When an organization contemplates performing re-identifcation,  the re-identifcation rate that the resulting de-identifed data are likely to have.  attacker A person who seeks to exploit potential vulnerabilities of a system.  attribute Characteristic or property of an entity that can be used to describe its state, ap- pearance, or other aspect. (ISO/IEC 24760-1:2011)[65]  brute force attack In cryptography, an attack that involves trying all possible combina- tions to fnd a match.  characteristic Distinguishing feature. (ISO 8000-2:2012(E))  coded 1. Identifying information (such as name or social security number) that would  enable the investigator to readily ascertain the identity of the individual to whom  the private information or specimens pertain has been replaced with a number, let- ter, symbol, or combination thereof (i.e., the code); 2. A key to decipher the code  exists, enabling linkage of the identifying information to the private information or  specimens. [95]  control Measure that is modifying risk. Note: controls include any process, policy, device,  practice, or other actions which modify risk. (ISO/IEC 27000:2014)  covered entity Under HIPAA, a health plan, a health care clearinghouse, or a health care  provider that conducts certain health care transactions electronically (e.g., billing).  (HIPAA Privacy Rule)  data Re-interpretable representation of information in a formalized manner suitable for  communication, interpretation, or processing. (ISO 8000-2:2012(E))  89  NIST SP 800-188 3pd  November 2022  3159  3160  3161  3162  3163  3164  3165  3166  3167  3168  3169  3170  3171  3172  3173  3174  3175  3176  3177  3178  3179  3180  3181  3182  3183  3184  3185  3186  3187  3188  3189  data accuracy Closeness of agreement between a property value and the true value. (ISO  8000-2:2012(E)  data dictionary collection of data dictionary entries that allows lookup by entity identifer.  (ISO 8000-2:2012(E))  data dictionary entry Description of an entity type containing, at a minimum, an unam- biguous identifer, a term, and a defnition. (ISO 8000-2:2012(E))  data intruder A data user who attempts to disclose information about a population through  identifcation or attribution. (OECD Glossary of Statistical Terms)  data life cycle The set of processes in an application that transform raw data into action- able knowledge. (NIST SP 1500-1)  data subjects Persons to whom data refer. (ISO/TS 25237:2008)  data use agreement Executed agreement between a data provider and a data recipient that  specifes the terms under which the data can be used.  data universe All possible data within a specifed domain.  dataset A collection of data.  dataset with identifers A dataset that contains information that directly identifes indi- viduals.  dataset without identifers A dataset that does not contain direct identifers.  de-identifcation A process that is applied to a dataset with the goal of preventing or lim- iting informational risks to individuals, protected groups, and establishments, while  still allowing for the production of aggregate statistics.28  de-identifcation model An approach to the application of data de-identifcation tech- niques that enables the calculation of re-identifcation risk. (ISO/IEC 20889 WORK- ING DRAFT 2 2016-05-27)  de-identifcation process A general term for any process of removing the association be- tween a set of identifying data and the data principal. (ISO/TS 25237:2008)  de-identifed information Records that have had enough PII removed or obscured such  that the remaining information does not identify an individual, and there is no rea- sonable basis to believe that the information can be used to identify an individual.  (SP800-122)  direct identifying data Data that directly identify a single individual. (ISO/TS 25237:2008)  28ISO/TS 25237:2008 defnes de-identifcation as the “general term for any process of removing the associ- ation between a set of identifying data and the data subject” [66, p.3]. This document intentionally adopts  a broader defnition for de-identifcation that allows for noise-introducing techniques, such as differential  privacy and the creation of synthetic datasets that are based on privacy-preserving models.  90  NIST SP 800-188 3pd  November 2022  3190  3191  3192  3193  3194  3195  3196  3197  3198  3199  3200  3201  3202  3203  3204  3205  3206  3207  3208  3209  3210  3211  3212  3213  3214  3215  3216  3217  3218  3219  3220  3221  3222  3223  3224  3225  disclosure Divulging of, or provision of access to, data. (ISO/TS 25237:2008)  disclosure limitation Statistical methods used to hinder anyone from identifying an indi- vidual respondent or establishment by analyzing published data, especially by ma- nipulating mathematical and arithmetical relationships among the data. [p.21][130]  effectiveness The extent to which planned activities are realized and planned results achieved.  (ISO/IEC 27000:2014)  entity An item inside or outside an information and communication technology system,  such as a person, an organization, a device, a subsystem, or a group of such items  that has recognizably distinct existence. (ISO/IEC 24760-1:2011)  expert determination Within the context of de-identifcation, refers to the Expert Deter- mination method for de-identifying protected health information in accordance with  the HIPAA Privacy Rule de-identifcation standard.  Federal Committee on Statistical Methodology (FCSM) An interagency committee ded- icated to improving the quality of Federal statistics. The FCSM was created by the  Offce of Management and Budget (OMB) to inform and advise OMB and the Inter- agency Council on Statistical Policy (ICSP) on methodological and statistical issues  that affect the quality of Federal data. (fscm.sites.usa.gov)  genomic information Information based on an individual’s genome, such as a sequence  of DNA or the results of genetic testing.  harm Any adverse effects that would be experienced by an individual (i.e., that may be  socially, physically, or fnancially damaging) or an organization if the confdentiality  of PII were breached. (SP 800-122)  Health Insurance Portability and Accountability Act of 1996 (HIPAA) A federal statute  that called on the federal Department of Health and Human Services to establish reg- ulatory standards to protect the privacy and security of individually identifable health  information. See https://www.hhs.gov/hipaa/for-professionals/index.html.  HIPAA See Health Insurance Portability and Accountability Act of 1996.  HIPAA Privacy Rule Establishes national standards to protect individuals’ medical records  and other personal health information and applies to health plans, health care clear- inghouses, and those health care providers that conduct certain health care transac- tions electronically. (HIPAA Privacy Rule, 45 CFR 160, 162, 164). See https://www.hhs.gov/hipaa/for- professionals/privacy/index.html.  identifcation The process of using claimed or observed attributes of an entity to single  out the entity among other entities in a set of identities. (ISO/TS 25237:2008)  identifying information Information that can be used to distinguish or trace an individ- ual’s identity (e.g., their name, social security number, biometric records, etc.) alone  91  https://www.hhs.gov/hipaa/for https://www.hhs.gov/hipaa/for-professionals/index.html https://fscm.sites.usa.gov  NIST SP 800-188 3pd  November 2022  3226  3227  3228  3229  3230  3231  3232  3233  3234  3235  3236  3237  3238  3239  3240  3241  3242  3243  3244  3245  3246  3247  3248  3249  3250  3251  3252  3253  3254  3255  3256  3257  3258  3259  3260  3261  3262  or when combined with other personal or identifying information that is linked or  linkable to a specifc individual (e.g., date and place of birth, mother’s maiden name,  etc.). (OMB M-07-16)  identifer Information used to claim an identity, before a potential corroboration by a cor- responding authenticator. (ISO/TS 25237:2008)  imputation A procedure for entering a value for a specifc data item where the response is  missing or unusable. (OECD Glossary of Statistical Terms)  inference Refers to the ability to deduce the identity of a person associated with a set of  data through “clues” contained in that information. This analysis permits determi- nation of the individual’s identity based on a combination of facts associated with  that person even though specifc identifers have been removed, like name and social  security number. (ASTM E1869-04)[15]  information Knowledge concerning objects, such as facts, events, things, processes, or  ideas, including concepts, that within a certain context has a particular meaning.  (ISO 8000-2:2012(E))  k-anonymity A technique “to release person-specifc data such that the ability to link to  other information using the quasi-identifer is limited” [122]. k-anonymity achieves  this through suppression of identifers and output perturbation.  l-diversity A refnement to the k-anonymity approach that assures that groups of records  specifed by the same identifers have suffcient diversity to prevent inferential dis- closure. [76]  masking The process of systematically removing a feld or replacing it with a value in a  way that does not preserve the analytic utility of the value, such as replacing a phone  number with asterisks or a randomly generated pseudonym. [45]  motivated intruder test The ‘motivated intruder’ is taken to be a person who starts with- out any prior knowledge but who wishes to identify the individual from whose per- sonal data the anonymised data has been derived. This test is meant to assess whether  the motivated intruder would be successful. [63]  noise A convenient term for a series of random disturbances borrowed through communi- cation engineering, from the theory of sound. In communication theory, noise results  in the possibility of a signal sent, x, being different from the signal received, y, and  the latter has a probability distribution conditional upon x. If the disturbances con- sist of impulses at random intervals, it is sometimes known as “shot noise.” (OECD  Glossary of Statistical Terms)  non-deterministic noise A random value that cannot be predicted.  non-ignorable bias A bias introduced into data or an analytics procedure that results in a  change that cannot be ignored.  92  NIST SP 800-188 3pd  November 2022  3263  3264  3265  3266  3267  3268  3269  3270  3271  3272  3273  3274  3275  3276  3277  3278  3279  3280  3281  3282  3283  3284  3285  3286  3287  3288  3289  3290  3291  3292  3293  3294  3295  3296  3297  3298  3299  non-public personal information Information about a person that is not publicly known;  called “private information” in some other publications.  personal identifer Information with the purpose of uniquely identifying a person within  a given context. (ISO/TS 25237:2008)  personal data Any information relating to an identifed or identifable natural person (data  subject). (ISO/TS 25237:2008)  personal information See personal data.  personally identifable information (PII) Any information about an individual maintained  by an agency, including (1) any information that can be used to distinguish or trace  an individual’s identity, such as name, social security number, date and place of birth,  mother’s maiden name, or biometric records; and (2) any other information that is  linked or linkable to an individual, such as medical, educational, fnancial, and em- ployment information. [106](SP 800-122)  perturbation-based methods Perturbation-based methods falsify the data before publica- tion by introducing an element of error purposely for confdentiality reasons. This  error can be inserted in the cell values after the table is created, which means the  error is introduced to the output of the data and will therefore be referred to as output  perturbation, or the error can be inserted in the original data on the microdata level,  which is the input of the tables one wants to create; the method with then be referred  to as data perturbation—input perturbation being the better but uncommonly used  expression. Possible methods are: rounding; random perturbation; [and] disclosure  control methods for microstatistics applied to macrostatistics. (OECD Glossary of  Statistical Terms)  privacy Freedom from intrusion into the private life or affairs of an individual when that  intrusion results from undue or illegal gathering and use of data about that individual.  (ISO/IEC 2382-8:1998, defnition 08-01-23)  privacy risk  privacy loss A measure of the extent to which a data release may reveal information that  is specifc to an individual.  privacy loss budget An upper bound on the cumulative total privacy loss for individuals.  property value Instance of a specifc value together with an identifer for a data dictionary  entry that defnes a property. (ISO 8000-2:2012(E))  protected health information (PHI) Individually identifable health information: (1) Ex- cept as provided in paragraph (2) of this defnition, that is: (i) Transmitted by elec- tronic media; (ii) Maintained in electronic media; or (iii) Transmitted or maintained  in any other form or medium. (2) Protected health information excludes individu- ally identifable health information in: (i) Education records covered by the Fam-  93  NIST SP 800-188 3pd  November 2022  3300  3301  3302  3303  3304  3305  3306  3307  3308  3309  3310  3311  3312  3313  3314  3315  3316  3317  3318  3319  3320  3321  3322  3323  3324  3325  3326  3327  3328  3329  3330  3331  3332  ily Educational Rights and Privacy Act, as amended, 20 USC. 1232g; (ii) Records  described at 20 USC. 1232g(a)(4)(B)(iv); and (iii) Employment records held by a  covered entity in its role as employer. (HIPAA Privacy Rule, 45 CFR 160.103). See  https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html.  pseudonymization A particular type of de-identifcation that both removes the association  with a data subject and adds an association between a particular set of characteristics  related to the data subject and one or more pseudonyms.29 Typically, pseudonymiza- tion is implemented by replacing direct identifers with a pseudonym, such as a ran- domly generated value.  pseudonym Personal identifer that is different from the normally used personal identifer.  (ISO/TS 25237:2008)  quality Degree to which a set of inherent characteristics fulfls requirements. (ISO 8000- 2:2012(E))  quasi-identifer A variable that can be used to identify an individual through association  with another variable.  recipient Natural or legal person, public authority, agency, or any other body to whom  data are disclosed. (ISO/TS25237:2008)  redaction The removal of information from a document or dataset for legal or security  purposes.  re-identifcation A general term for any process that restores the association between a  set of de-identifed data and a data subject.  re-identifcation risk The likelihood that a third party can re-identify data subjects in a  de-identifed dataset.  re-identifcation rate The percentage of records in a dataset that can be re-identifed.  re-identifcaiton probability TBD  requirement A need or expectation that is stated, generally implied or obligatory. (ISO  8000-2:2012(E))  risk A measure of the extent to which an entity is threatened by a potential circumstance  or event, and typically a function of: (i) the adverse impacts that would arise if the  circumstance or event occurs; and (ii) the likelihood of occurrence. (CNSSI No.  4009)  risk assessment The process of identifying, estimating, and prioritizing risks to organi- zational operations (including mission, functions, image, reputation), organizational  29This defnition is the same as the defnition in ISO/TS 25237:2008, except that the word “anonymization”  is replaced with the word “de-identifcation.”  94  https://www.law.cornell.edu/uscode/text/20/1232g https://www.law.cornell.edu/uscode/text/20/1232g#a_4_B_iv https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html  NIST SP 800-188 3pd  November 2022  3333  3334  3335  3336  3337  3338  3339  3340  3341  3342  3343  3344  3345  3346  3347  3348  3349  3350  3351  3352  assets, individuals, other organizations, and the Nation, resulting from the operation  of an information system. Part of risk management, incorporates threat and vulner- ability analyses, and considers mitigations provided by security controls planned or  in place. Synonymous with risk analysis. (NIST SP 800-39)  safe harbor Within the context of de-identifcation, refers to the Safe Harbor method for  de-identifying protected health information in accordance with the Health Insurance  Portability and Accountability Act (HIPAA) Privacy Rule. See https://www.hhs.gov/hipaa/f professionals/privacy/special-topics/de-identifcation/index.html.  statistical disclosure control The set of methods to reduce the risk of disclosing informa- tion on individuals, businesses or other organizations. Such methods are only related  to the dissemination step and are usually based on restricting the amount of or modi- fying the data released. (OECD Glossary of Statistical Terms)  suppression One of the most commonly used ways of protecting sensitive cells in a table is  via suppression. It is obvious that in a row or column with a suppressed sensitive cell,  at least one additional cell must be suppressed, or the value in the sensitive cell could  be calculated exactly by subtraction from the marginal total. For this reason, certain  other cells must also be suppressed. These are referred to as secondary suppressions.  (OECD Glossary of Statistical Terms)  synthetic data generation A process in which seed data are used to create artifcial data  that have some of the statistical characteristics as the seed data.  or-  95  https://www.hhs.gov/hipaa/for  Executive Summary  Introduction  Document Purpose and Scope  Intended Audience  Organization  Introducing De-Identification  Historical Context  Terminology  Governance and Management of Data De-Identification  Identifying Goals and Intended Uses of De-Identification  Evaluating Risks that Arise from De-Identified Data Releases  Probability of Re-Identification  Adverse Impacts of Re-Identification  Impacts Other Than Re-Identification  Remediation  Data Life Cycle  Data-Sharing Models  The Five Safes  Disclosure Review Boards  De-Identification Standards  Benefits of Standards  Prescriptive De-Identification Standards  Performance-Based De-Identification Standards  Education, Training, and Research  Defense in Depth  Encryption and Access Control  Secure Computation  Trusted Execution Environments  Physical Enclaves  Technical Steps for Data De-Identification  Determine the Privacy, Data Usability, and Access Objectives  Conducting a Data Survey  De-Identification by Removing Identifiers and Transforming Quasi-Identifiers  Removing or Transforming of Direct Identifiers  Special Security Note Regarding the Encryption or Hashing of Direct Identifiers  De-Identifying Numeric Quasi-Identifiers  De-Identifying Dates  De-Identifying Geographical Locations and Geolocation Data  De-Identifying Genomic Information  De-Identifying Text Narratives and Qualitative Information  Challenges Posed by Aggregation Techniques  Challenges Posed by High-Dimensional Data  Challenges Posed by Linked Data  Challenges Posed by Composition  Potential Failures of De-Identification  Post-Release Monitoring  Synthetic Data  Partially Synthetic Data  Test Data  Fully Synthetic Data  Synthetic Data with Validation  Synthetic Data and Open Data Policy  Creating a Synthetic Dataset with Differential Privacy  De-Identifying with an Interactive Query Interface  Validating a De-Identified Dataset  Validating Data Usefulness  Validating Privacy Protection  Re-Identification Studies  Software Requirements, Evaluation, and Validation  Evaluating Privacy-Preserving Techniques  De-Identification Tools  De-Identification Tool Features  Data Provenance and File Formats  Data Masking Tools  Evaluating De-Identification Software  Evaluating Data Accuracy  Conclusion  References  Appendix Standards  NIST Publications  Other U.S. Government Publications  Selected Publications by Other Governments  Reports and Books  How-To Articles  Appendix List of Symbols, Abbreviations, and Acronyms  Appendix Glossary",
    "abstract": ""
}